{
    "papers": [
        {
            "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
            "authors": [
                "Hadi Alzayer",
                "Yunzhi Zhang",
                "Chen Geng",
                "Jia-Bin Huang",
                "Jiajun Wu"
            ],
            "arxiv_id": "2510.14981v1",
            "summary": "We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.",
            "headline_zh": "提出耦合扩散采样方法，实现免训练的多视角图像编辑一致性",
            "intro_zh": [
                "核心问题：预训练2D编辑模型在多视角图像中缺乏跨视角一致性",
                "方法要点：通过耦合扩散采样，约束生成图像序列符合多视角分布",
                "实验或效果：在三个多视角编辑任务中验证有效性和通用性"
            ],
            "tags_zh": [
                "多视角图像编辑",
                "扩散采样",
                "免训练方法",
                "图像一致性",
                "隐式3D正则化"
            ],
            "_index": 0
        },
        {
            "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale",
            "authors": [
                "Haiwen Diao",
                "Mingxuan Li",
                "Silei Wu",
                "Linjun Dai",
                "Xiaohua Wang",
                "Hanming Deng",
                "Lewei Lu",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "arxiv_id": "2510.14979v1",
            "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
            "headline_zh": "提出NEO原生视觉语言模型，以解决视觉语言冲突并提升跨模态推理能力",
            "intro_zh": [
                "核心问题：原生视觉语言模型与模块化模型的根本约束及可访问性挑战",
                "方法要点：设计共享语义空间，整合视觉与语言模块，支持统一编码与推理",
                "实验或效果：NEO在390M图像-文本数据上，媲美顶级模块化模型，实现高效视觉感知"
            ],
            "tags_zh": [
                "原生视觉语言模型",
                "跨模态对齐",
                "共享语义空间",
                "统一编码",
                "视觉语言推理",
                "模型可扩展性"
            ],
            "_index": 1
        },
        {
            "title": "Agentic Design of Compositional Machines",
            "authors": [
                "Wenqian Zhang",
                "Weiyang Liu",
                "Zhen Liu"
            ],
            "arxiv_id": "2510.14980v1",
            "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
            "headline_zh": "提出BesiegeField测试床与RL微调方法，以提升LLMs在组合机器设计中的能力。",
            "intro_zh": [
                "核心问题：LLMs能否学习在模拟物理环境中设计组合机器以满足功能需求。",
                "方法要点：引入BesiegeField测试床，评估LLMs的空间推理和战略组装能力。",
                "实验或效果：通过RL微调改进模型，识别语言、设计和物理推理的交叉挑战。"
            ],
            "tags_zh": [
                "组合机器设计",
                "大型语言模型",
                "强化学习",
                "物理模拟",
                "空间推理",
                "测试床"
            ],
            "_index": 2
        },
        {
            "title": "Learning an Image Editing Model without Image Editing Pairs",
            "authors": [
                "Nupur Kumari",
                "Sheng-Yu Wang",
                "Nanxuan Zhao",
                "Yotam Nitzan",
                "Yuheng Li",
                "Krishna Kumar Singh",
                "Richard Zhang",
                "Eli Shechtman",
                "Jun-Yan Zhu",
                "Xun Huang"
            ],
            "arxiv_id": "2510.14978v1",
            "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
            "headline_zh": "提出无配对数据图像编辑训练范式，利用视觉语言模型反馈优化扩散模型",
            "intro_zh": [
                "核心问题：图像编辑模型依赖大规模输入-目标配对数据，难以获取且易传播预训练模型伪影",
                "方法要点：通过展开扩散模型训练，结合视觉语言模型评估编辑指令遵循和内容保留，提供端到端梯度优化",
                "实验或效果：在标准基准测试中，无配对数据下性能与监督训练模型相当，优于基于强化学习的方法"
            ],
            "tags_zh": [
                "图像编辑",
                "扩散模型",
                "视觉语言模型",
                "无配对训练",
                "分布匹配损失",
                "端到端优化"
            ],
            "_index": 3
        },
        {
            "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation",
            "authors": [
                "Shaowei Liu",
                "Chuan Guo",
                "Bing Zhou",
                "Jian Wang"
            ],
            "arxiv_id": "2510.14976v1",
            "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.",
            "headline_zh": "提出Ponimator框架，基于近距交互姿态实现多样化人-人交互动画",
            "intro_zh": [
                "核心问题：如何从近距交互姿态推断和生成动态交互动画",
                "方法要点：使用两个条件扩散模型，分别处理时空先验生成运动和姿态",
                "实验效果：在多个数据集验证框架通用性、有效性和鲁棒性"
            ],
            "tags_zh": [
                "人-人交互动画",
                "条件扩散模型",
                "姿态先验",
                "运动生成",
                "文本到交互合成"
            ],
            "_index": 4
        },
        {
            "title": "Terra: Explorable Native 3D World Model with Point Latents",
            "authors": [
                "Yuanhui Huang",
                "Weiliang Chen",
                "Wenzhao Zheng",
                "Xin Tao",
                "Pengfei Wan",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "arxiv_id": "2510.14977v1",
            "summary": "World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.",
            "headline_zh": "提出Terra原生3D世界模型，以点潜在表示解决3D一致性与建模效率问题",
            "intro_zh": [
                "现有世界模型依赖像素对齐表示，忽略物理世界3D本质，影响一致性与效率",
                "引入P2G-VAE编码3D输入为点潜在，解码为高斯基元；SPFlow网络生成点潜在",
                "在ScanNet v2上实验，实现多视角一致、任意视角渲染，重建与生成性能领先"
            ],
            "tags_zh": [
                "3D世界模型",
                "点潜在表示",
                "高斯基元",
                "变分自编码器",
                "流匹配网络",
                "多视角一致性"
            ],
            "_index": 5
        },
        {
            "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
            "authors": [
                "Hengyuan Xu",
                "Wei Cheng",
                "Peng Xing",
                "Yixiao Fang",
                "Shuhan Wu",
                "Rui Wang",
                "Xianfang Zeng",
                "Daxin Jiang",
                "Gang Yu",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "arxiv_id": "2510.14975v1",
            "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
            "headline_zh": "提出WithAnyone模型以解决身份一致图像生成中的复制粘贴问题",
            "intro_zh": [
                "核心问题：基于重建训练导致复制粘贴，限制姿态和表情的多样性控制",
                "方法要点：构建MultiID-2M数据集，引入对比身份损失平衡保真与多样性",
                "实验或效果：显著减少复制粘贴，提升可控性，保持高身份相似度"
            ],
            "tags_zh": [
                "身份一致生成",
                "扩散模型",
                "复制粘贴缓解",
                "对比学习",
                "多人物数据集"
            ],
            "_index": 6
        },
        {
            "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
            "authors": [
                "Hansheng Chen",
                "Kai Zhang",
                "Hao Tan",
                "Leonidas Guibas",
                "Gordon Wetzstein",
                "Sai Bi"
            ],
            "arxiv_id": "2510.14974v1",
            "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard $\\ell_2$ flow matching loss. By simply\nmimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
            "headline_zh": "提出π-Flow策略模型，通过模仿蒸馏解决少步生成中的质量-多样性权衡问题",
            "intro_zh": [
                "核心问题：少步生成模型蒸馏过程复杂，常面临质量与多样性权衡。",
                "方法要点：修改学生模型输出层为策略，预测动态流速，实现高效ODE积分。",
                "实验效果：在ImageNet等数据集上，以少步数实现高FID和多样性，超越现有方法。"
            ],
            "tags_zh": [
                "少步生成模型",
                "模仿蒸馏",
                "策略预测",
                "ODE积分",
                "质量-多样性权衡",
                "流匹配损失"
            ],
            "_index": 7
        },
        {
            "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
            "authors": [
                "Mingxuan Yan",
                "Yuping Wang",
                "Zechun Liu",
                "Jiachen Li"
            ],
            "arxiv_id": "2510.14968v1",
            "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io.",
            "headline_zh": "提出基于检索的演示分解器以对齐长视野任务中的规划器",
            "intro_zh": [
                "核心问题：VLM规划器微调依赖人工或启发式分解，子任务与低层策略训练数据不匹配，影响性能。",
                "方法要点：RDD通过视觉特征检索自动分解演示，使子任务与低层策略训练数据对齐。",
                "实验或效果：在仿真和真实任务中优于现有分解器，展现跨场景鲁棒性。"
            ],
            "tags_zh": [
                "长视野任务",
                "视觉语言动作框架",
                "演示分解",
                "视觉特征检索",
                "规划器对齐",
                "机器人操作"
            ],
            "_index": 8
        },
        {
            "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
            "authors": [
                "Miao Hu",
                "Zhiwei Huang",
                "Tai Wang",
                "Jiangmiao Pang",
                "Dahua Lin",
                "Nanning Zheng",
                "Runsen Xu"
            ],
            "arxiv_id": "2510.14965v1",
            "summary": "Real-world robots localize objects from natural-language instructions while\nscenes around them keep changing. Yet most of the existing 3D visual grounding\n(3DVG) method still assumes a reconstructed and up-to-date point cloud, an\nassumption that forces costly re-scans and hinders deployment. We argue that\n3DVG should be formulated as an active, memory-driven problem, and we introduce\nChangingGrounding, the first benchmark that explicitly measures how well an\nagent can exploit past observations, explore only where needed, and still\ndeliver precise 3D boxes in changing scenes. To set a strong reference point,\nwe also propose Mem-ChangingGrounder, a zero-shot method for this task that\nmarries cross-modal retrieval with lightweight multi-view fusion: it identifies\nthe object type implied by the query, retrieves relevant memories to guide\nactions, then explores the target efficiently in the scene, falls back when\nprevious operations are invalid, performs multi-view scanning of the target,\nand projects the fused evidence from multi-view scans to get accurate object\nbounding boxes. We evaluate different baselines on ChangingGrounding, and our\nMem-ChangingGrounder achieves the highest localization accuracy while greatly\nreducing exploration cost. We hope this benchmark and method catalyze a shift\ntoward practical, memory-centric 3DVG research for real-world applications.\nProject page: https://hm123450.github.io/CGB/ .",
            "headline_zh": "提出ChangingGrounding基准与Mem-ChangingGrounder方法，以解决动态场景中3D视觉定位问题。",
            "intro_zh": [
                "核心问题：现有3D视觉定位方法依赖更新点云，难以应对场景变化，阻碍实际部署。",
                "方法要点：结合跨模态检索与轻量多视图融合，实现零样本目标定位与高效探索。",
                "实验或效果：在ChangingGrounding基准上，Mem-ChangingGrounder实现最高定位精度并降低探索成本。"
            ],
            "tags_zh": [
                "3D视觉定位",
                "动态场景",
                "跨模态检索",
                "多视图融合",
                "零样本学习",
                "基准数据集"
            ],
            "_index": 9
        },
        {
            "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion",
            "authors": [
                "Thao Nguyen",
                "Jiaqi Ma",
                "Fahad Shahbaz Khan",
                "Souhaib Ben Taieb",
                "Salman Khan"
            ],
            "arxiv_id": "2510.14962v1",
            "summary": "Precipitation nowcasting, predicting future radar echo sequences from current\nobservations, is a critical yet challenging task due to the inherently chaotic\nand tightly coupled spatio-temporal dynamics of the atmosphere. While recent\nadvances in diffusion-based models attempt to capture both large-scale motion\nand fine-grained stochastic variability, they often suffer from scalability\nissues: latent-space approaches require a separately trained autoencoder,\nadding complexity and limiting generalization, while pixel-space approaches are\ncomputationally intensive and often omit attention mechanisms, reducing their\nability to model long-range spatio-temporal dependencies. To address these\nlimitations, we propose a Token-wise Attention integrated into not only the\nU-Net diffusion model but also the spatio-temporal encoder that dynamically\ncaptures multi-scale spatial interactions and temporal evolution. Unlike prior\napproaches, our method natively integrates attention into the architecture\nwithout incurring the high resource cost typical of pixel-space diffusion,\nthereby eliminating the need for separate latent modules. Our extensive\nexperiments and visual evaluations across diverse datasets demonstrate that the\nproposed method significantly outperforms state-of-the-art approaches, yielding\nsuperior local fidelity, generalization, and robustness in complex\nprecipitation forecasting scenarios.",
            "headline_zh": "提出RainDiff通过令牌注意力扩散模型解决降水临近预报的时空依赖建模问题",
            "intro_zh": [
                "降水临近预报面临混沌时空动态的挑战，现有扩散模型存在可扩展性问题",
                "方法将令牌注意力集成到U-Net扩散模型和时空编码器中，动态捕获多尺度时空交互",
                "实验显示在多样数据集上优于现有方法，提升局部保真度、泛化性和鲁棒性"
            ],
            "tags_zh": [
                "降水临近预报",
                "扩散模型",
                "令牌注意力",
                "时空编码",
                "U-Net架构",
                "多尺度建模"
            ],
            "_index": 10
        },
        {
            "title": "C4D: 4D Made from 3D through Dual Correspondences",
            "authors": [
                "Shizun Wang",
                "Zhenxiang Jiang",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "arxiv_id": "2510.14960v1",
            "summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry\nand camera poses, is an inevitably challenging problem. While recent\npointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great\nprogress in reconstructing static scenes, directly applying them to dynamic\nscenes leads to inaccurate results. This discrepancy arises because moving\nobjects violate multi-view geometric constraints, disrupting the\nreconstruction. To address this, we introduce C4D, a framework that leverages\ntemporal Correspondences to extend existing 3D reconstruction formulation to\n4D. Specifically, apart from predicting pointmaps, C4D captures two types of\ncorrespondences: short-term optical flow and long-term point tracking. We train\na dynamic-aware point tracker that provides additional mobility information,\nfacilitating the estimation of motion masks to separate moving elements from\nthe static background, thus offering more reliable guidance for dynamic scenes.\nFurthermore, we introduce a set of dynamic scene optimization objectives to\nrecover per-frame 3D geometry and camera parameters. Simultaneously, the\ncorrespondences lift 2D trajectories into smooth 3D trajectories, enabling\nfully integrated 4D reconstruction. Experiments show that our framework\nachieves complete 4D recovery and demonstrates strong performance across\nmultiple downstream tasks, including depth estimation, camera pose estimation,\nand point tracking. Project Page: https://littlepure2333.github.io/C4D",
            "headline_zh": "提出C4D框架，通过双对应关系从单目视频恢复4D动态场景",
            "intro_zh": [
                "核心问题：单目视频中动态物体违反多视图几何约束，导致4D重建不准确",
                "方法要点：结合短时光流和长时点跟踪，分离动态元素并优化3D几何与相机参数",
                "实验效果：在深度估计、相机姿态估计和点跟踪等任务中表现优异"
            ],
            "tags_zh": [
                "4D重建",
                "单目视频",
                "动态场景",
                "点跟踪",
                "相机姿态估计",
                "深度估计"
            ],
            "_index": 11
        },
        {
            "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning",
            "authors": [
                "Weikang Shi",
                "Aldrich Yu",
                "Rongyao Fang",
                "Houxing Ren",
                "Ke Wang",
                "Aojun Zhou",
                "Changyao Tian",
                "Xinyu Fu",
                "Yuxuan Hu",
                "Zimu Lu",
                "Linjiang Huang",
                "Si Liu",
                "Rui Liu",
                "Hongsheng Li"
            ],
            "arxiv_id": "2510.14958v1",
            "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
            "headline_zh": "提出MathCanvas框架以增强多模态模型在数学推理中的视觉链式思考能力",
            "intro_zh": [
                "核心问题：现有视觉链式思考方法在数学领域受限于外部工具，难以生成高保真、适时图表。",
                "方法要点：通过视觉操作预训练和策略性视觉辅助推理微调，实现内在视觉链式思考。",
                "实验或效果：在MathCanvas-Bench上相对基线提升86%，并泛化至其他数学基准。"
            ],
            "tags_zh": [
                "多模态数学推理",
                "视觉链式思考",
                "图表生成",
                "预训练数据集",
                "基准评估"
            ],
            "_index": 12
        },
        {
            "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions",
            "authors": [
                "Lizhi Yang",
                "Blake Werner",
                "Massimiliano de Sa Aaron D. Ames"
            ],
            "arxiv_id": "2510.14959v1",
            "summary": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter.",
            "headline_zh": "提出CBF-RL框架，在强化学习训练中集成控制屏障函数以提升安全性",
            "intro_zh": [
                "强化学习常牺牲安全性追求性能，导致实际部署风险",
                "CBF-RL在训练中编码安全约束，无需在线安全过滤器",
                "实验验证在导航和人形机器人任务中实现安全探索和鲁棒性能"
            ],
            "tags_zh": [
                "强化学习",
                "控制屏障函数",
                "安全过滤",
                "机器人控制",
                "训练安全",
                "实时部署"
            ],
            "_index": 13
        },
        {
            "title": "RealDPO: Real or Not Real, that is the Preference",
            "authors": [
                "Guo Cheng",
                "Danni Yang",
                "Ziqi Huang",
                "Jianlou Si",
                "Chenyang Si",
                "Ziwei Liu"
            ],
            "arxiv_id": "2510.14955v1",
            "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
            "headline_zh": "提出RealDPO对齐范式，利用真实世界数据优化视频生成中的复杂运动合成。",
            "intro_zh": [
                "核心问题：视频生成模型在复杂运动合成中难以产生自然、流畅且上下文一致的运动。",
                "方法要点：采用Direct Preference Optimization，以真实视频为正样本对比模型错误输出进行迭代自校正。",
                "实验或效果：在视频质量、文本对齐和运动真实感方面显著优于现有先进模型和偏好优化技术。"
            ],
            "tags_zh": [
                "视频生成",
                "偏好优化",
                "运动合成",
                "真实世界数据",
                "对齐学习"
            ],
            "_index": 14
        },
        {
            "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression",
            "authors": [
                "Zhe Li",
                "Weihao Yuan",
                "Weichao Shen",
                "Siyu Zhu",
                "Zilong Dong",
                "Chang Xu"
            ],
            "arxiv_id": "2510.14954v1",
            "summary": "Whole-body multi-modal human motion generation poses two primary challenges:\ncreating an effective motion generation mechanism and integrating various\nmodalities, such as text, speech, and music, into a cohesive framework. Unlike\nprevious methods that usually employ discrete masked modeling or autoregressive\nmodeling, we develop a continuous masked autoregressive motion transformer,\nwhere a causal attention is performed considering the sequential nature within\nthe human motion. Within this transformer, we introduce a gated linear\nattention and an RMSNorm module, which drive the transformer to pay attention\nto the key actions and suppress the instability caused by either the abnormal\nmovements or the heterogeneous distributions within multi-modalities. To\nfurther enhance both the motion generation and the multimodal generalization,\nwe employ the DiT structure to diffuse the conditions from the transformer\ntowards the targets. To fuse different modalities, AdaLN and cross-attention\nare leveraged to inject the text, speech, and music signals. Experimental\nresults demonstrate that our framework outperforms previous methods across all\nmodalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.",
            "headline_zh": "提出连续掩码自回归运动变换器以解决多模态人体运动生成问题",
            "intro_zh": [
                "核心问题：多模态人体运动生成需有效机制与模态融合，如文本、语音和音乐",
                "方法要点：采用连续掩码自回归变换器，结合门控线性注意力和RMSNorm模块",
                "实验或效果：在文本到运动、语音到手势和音乐到舞蹈任务中优于先前方法"
            ],
            "tags_zh": [
                "人体运动生成",
                "多模态融合",
                "自回归变换器",
                "连续掩码建模",
                "门控注意力"
            ],
            "_index": 15
        },
        {
            "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance",
            "authors": [
                "Zhe Li",
                "Cheng Chi",
                "Yangyang Wei",
                "Boan Zhu",
                "Yibo Peng",
                "Tao Huang",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Shanghang Zhang",
                "Chang Xu"
            ],
            "arxiv_id": "2510.14952v1",
            "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.",
            "headline_zh": "提出RoboGhost框架，通过运动潜在引导实现免重定向的人形机器人控制",
            "intro_zh": [
                "现有语言引导人形机器人运动流程繁琐，易产生累积误差和延迟",
                "采用扩散策略直接从噪声生成可执行动作，避免运动解码和重定向",
                "实验显示降低延迟、提高成功率，并支持多模态扩展"
            ],
            "tags_zh": [
                "人形机器人控制",
                "扩散模型",
                "语言到动作",
                "运动潜在表示",
                "免重定向框架"
            ],
            "_index": 16
        },
        {
            "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation",
            "authors": [
                "Yu Zhou",
                "Sohyun An",
                "Haikang Deng",
                "Da Yin",
                "Clark Peng",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang",
                "Nanyun Peng"
            ],
            "arxiv_id": "2510.14949v1",
            "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance.",
            "headline_zh": "提出编码器方法提升多模态生成模型在方言输入下的鲁棒性",
            "intro_zh": [
                "研究多模态生成模型在方言文本输入时的性能下降问题",
                "设计编码器策略，使模型识别方言特征并保持标准英语性能",
                "实验显示方法在多个方言上性能提升34.4%，标准英语性能无损失"
            ],
            "tags_zh": [
                "多模态生成",
                "方言鲁棒性",
                "编码器方法",
                "基准测试",
                "性能优化"
            ],
            "_index": 17
        },
        {
            "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion",
            "authors": [
                "Blake Werner",
                "Lizhi Yang",
                "Aaron D. Ames"
            ],
            "arxiv_id": "2510.14947v1",
            "summary": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion.",
            "headline_zh": "提出分层控制架构以提升人形机器人在非结构化环境中的鲁棒运动能力",
            "intro_zh": [
                "核心问题：人形机器人在非结构化环境中需平衡快速低层稳定与慢速感知决策",
                "方法要点：采用分层控制架构，结合高速本体感知稳定器与低速感知策略",
                "实验或效果：在仿真和硬件上优于单阶段方法，成功应对楼梯和边缘任务"
            ],
            "tags_zh": [
                "人形机器人运动",
                "分层控制架构",
                "鲁棒性",
                "感知策略",
                "训练课程"
            ],
            "_index": 18
        },
        {
            "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices",
            "authors": [
                "Romina Aalishah",
                "Mozhgan Navardi",
                "Tinoosh Mohsenin"
            ],
            "arxiv_id": "2510.14946v1",
            "summary": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity.",
            "headline_zh": "提出EdgeNavMamba框架，结合Mamba目标检测与强化学习，优化边缘设备上的自主导航。",
            "intro_zh": [
                "边缘设备计算资源受限，需高效深度学习模型支持实时自主导航。",
                "使用强化学习框架，集成Mamba目标检测器提取边界框，驱动导航策略。",
                "实验显示模型尺寸减少67%，能耗降低73%，导航成功率超90%。"
            ],
            "tags_zh": [
                "边缘计算",
                "目标检测",
                "强化学习",
                "模型压缩",
                "自主导航",
                "Mamba模型"
            ],
            "_index": 19
        },
        {
            "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation",
            "authors": [
                "JoungBin Lee",
                "Jaewoo Jung",
                "Jisang Han",
                "Takuya Narihira",
                "Kazumi Fukuda",
                "Junyoung Seo",
                "Sunghwan Hong",
                "Yuki Mitsufuji",
                "Seungryong Kim"
            ],
            "arxiv_id": "2510.14945v1",
            "summary": "We present 3DScenePrompt, a framework that generates the next video chunk\nfrom arbitrary-length input while enabling precise camera control and\npreserving scene consistency. Unlike methods conditioned on a single image or a\nshort clip, we employ dual spatio-temporal conditioning that reformulates\ncontext-view referencing across the input video. Our approach conditions on\nboth temporally adjacent frames for motion continuity and spatially adjacent\ncontent for scene consistency. However, when generating beyond temporal\nboundaries, directly using spatially adjacent frames would incorrectly preserve\ndynamic elements from the past. We address this by introducing a 3D scene\nmemory that represents exclusively the static geometry extracted from the\nentire input video. To construct this memory, we leverage dynamic SLAM with our\nnewly introduced dynamic masking strategy that explicitly separates static\nscene geometry from moving elements. The static scene representation can then\nbe projected to any target viewpoint, providing geometrically consistent warped\nviews that serve as strong 3D spatial prompts while allowing dynamic regions to\nevolve naturally from temporal context. This enables our model to maintain\nlong-range spatial coherence and precise camera control without sacrificing\ncomputational efficiency or motion realism. Extensive experiments demonstrate\nthat our framework significantly outperforms existing methods in scene\nconsistency, camera controllability, and generation quality. Project page :\nhttps://cvlab-kaist.github.io/3DScenePrompt/",
            "headline_zh": "提出3DScenePrompt框架，通过3D场景记忆实现场景一致和相机可控的视频生成。",
            "intro_zh": [
                "核心问题：长视频生成中场景一致性和相机控制难以兼顾，动态元素干扰静态几何。",
                "方法要点：使用动态SLAM和动态掩码分离静态几何，构建3D场景记忆作为空间提示。",
                "实验或效果：在场景一致性、相机可控性和生成质量上显著优于现有方法。"
            ],
            "tags_zh": [
                "视频生成",
                "3D场景建模",
                "相机控制",
                "场景一致性",
                "动态SLAM",
                "空间提示"
            ],
            "_index": 20
        },
        {
            "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin",
            "authors": [
                "Binghao Huang",
                "Jie Xu",
                "Iretiayo Akinola",
                "Wei Yang",
                "Balakumar Sundaralingam",
                "Rowland O'Flaherty",
                "Dieter Fox",
                "Xiaolong Wang",
                "Arsalan Mousavian",
                "Yu-Wei Chao",
                "Yunzhu Li"
            ],
            "arxiv_id": "2510.14930v1",
            "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\n-- a capability that remains difficult to replicate in robots through\nbehavioral cloning alone, due to the suboptimality and limited diversity of\nhuman demonstrations. In this work, we present VT-Refine, a visuo-tactile\npolicy learning framework that combines real-world demonstrations,\nhigh-fidelity tactile simulation, and reinforcement learning to tackle precise,\ncontact-rich bimanual assembly. We begin by training a diffusion policy on a\nsmall set of demonstrations using synchronized visual and tactile inputs. This\npolicy is then transferred to a simulated digital twin equipped with simulated\ntactile sensors and further refined via large-scale reinforcement learning to\nenhance robustness and generalization. To enable accurate sim-to-real transfer,\nwe leverage high-resolution piezoresistive tactile sensors that provide normal\nforce signals and can be realistically modeled in parallel using\nGPU-accelerated simulation. Experimental results show that VT-Refine improves\nassembly performance in both simulation and the real world by increasing data\ndiversity and enabling more effective policy fine-tuning. Our project page is\navailable at https://binghao-huang.github.io/vt_refine/.",
            "headline_zh": "提出VT-Refine框架，结合视觉触觉反馈与仿真微调，解决机器人双手精确装配问题。",
            "intro_zh": [
                "核心问题：仅靠行为克隆难以复制人类双手装配的触觉适应能力，因演示数据有限且次优。",
                "方法要点：使用扩散策略从真实演示学习，再通过仿真强化学习微调，提升鲁棒性和泛化。",
                "实验效果：在仿真和真实世界中，VT-Refine通过增加数据多样性和有效微调，提高装配性能。"
            ],
            "tags_zh": [
                "双手装配",
                "视觉触觉反馈",
                "扩散策略",
                "强化学习",
                "仿真到真实迁移",
                "触觉传感器"
            ],
            "_index": 21
        },
        {
            "title": "Design of Paper Robot Building Kits",
            "authors": [
                "Ruhan Yang",
                "Ellen Yi-Luen Do"
            ],
            "arxiv_id": "2510.14914v1",
            "summary": "Building robots is an engaging activity that provides opportunities for\nhands-on learning. However, traditional robot-building kits are usually costly\nwith limited functionality due to material and technology constraints. To\nimprove the accessibility and flexibility of such kits, we take paper as the\nbuilding material and extensively explore the versatility of paper-based\ninteractions. Based on an analysis of current robot-building kits and\npaper-based interaction research, we propose a design space for devising paper\nrobots. We also analyzed our building kit designs using this design space,\nwhere these kits demonstrate the potential of paper as a cost-effective\nmaterial for robot building. As a starting point, our design space and building\nkit examples provide a guideline that inspires and informs future research and\ndevelopment of novel paper robot-building kits.",
            "headline_zh": "提出纸基机器人设计空间与构建套件，以低成本材料提升机器人构建的可及性和灵活性。",
            "intro_zh": [
                "传统机器人构建套件成本高且功能受限，材料和技术限制是核心问题。",
                "基于纸材料探索交互多样性，提出设计空间指导纸机器人开发。",
                "分析套件设计展示纸材料成本效益潜力，为未来研究提供起点。"
            ],
            "tags_zh": [
                "纸基机器人",
                "设计空间",
                "构建套件",
                "低成本材料",
                "交互探索"
            ],
            "_index": 22
        },
        {
            "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos",
            "authors": [
                "Gabriel Fiastre",
                "Antoine Yang",
                "Cordelia Schmid"
            ],
            "arxiv_id": "2510.14904v1",
            "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.",
            "headline_zh": "提出MaskCaptioner以联合分割和描述视频中物体轨迹，解决密集视频物体描述任务。",
            "intro_zh": [
                "核心问题：密集视频物体描述需联合检测、跟踪和描述物体轨迹，但手动标注成本高，现有方法采用分离训练导致性能不佳。",
                "方法要点：利用先进视觉语言模型生成合成描述，扩展LVIS和LV-VIS数据集，训练端到端模型实现联合分割和描述。",
                "实验或效果：在VidSTG、VLN和BenSMOT基准测试中达到最先进性能，提供公开数据集和代码。"
            ],
            "tags_zh": [
                "密集视频物体描述",
                "端到端学习",
                "合成数据生成",
                "物体轨迹分割",
                "视觉语言模型",
                "视频理解"
            ],
            "_index": 23
        },
        {
            "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation",
            "authors": [
                "Han Zhao",
                "Jiaxuan Zhang",
                "Wenxuan Song",
                "Pengxiang Ding",
                "Donglin Wang"
            ],
            "arxiv_id": "2510.14902v1",
            "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.",
            "headline_zh": "提出VLA^2代理框架以解决视觉-语言-动作模型对未见概念泛化失败问题",
            "intro_zh": [
                "核心问题：VLA模型对训练数据外对象概念泛化能力差，成功率显著下降",
                "方法要点：利用OpenVLA执行骨干，集成外部模块如网络检索和物体检测提供额外知识",
                "实验或效果：在硬级泛化基准上成功率提升44.2%，平均提升20.2%且域内任务无退化"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "代理框架",
                "泛化能力",
                "外部模块集成",
                "未见概念处理"
            ],
            "_index": 24
        },
        {
            "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection",
            "authors": [
                "Furkan Mumcu",
                "Michael J. Jones",
                "Anoop Cherian",
                "Yasin Yilmaz"
            ],
            "arxiv_id": "2510.14896v1",
            "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies.",
            "headline_zh": "提出基于多模态大语言模型描述活动的半监督视频异常检测框架，以提升复杂异常检测与可解释性。",
            "intro_zh": [
                "现有半监督视频异常检测方法难以检测对象交互的复杂异常且缺乏可解释性。",
                "利用MLLM生成对象活动与交互的文本描述，通过比较训练和测试视频描述检测异常。",
                "在基准数据集上验证了方法对交互异常的有效检测，并在无交互异常数据集上达到先进性能。"
            ],
            "tags_zh": [
                "视频异常检测",
                "多模态大语言模型",
                "半监督学习",
                "可解释性",
                "对象交互"
            ],
            "_index": 25
        },
        {
            "title": "STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search",
            "authors": [
                "Helene J. Levy",
                "Brett T. Lopez"
            ],
            "arxiv_id": "2510.14893v1",
            "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners.",
            "headline_zh": "提出STITCHER框架，通过实时运动基元搜索在已知环境中规划约束轨迹",
            "intro_zh": [
                "核心问题：高速自主导航需实时生成动态可行、无碰撞且满足约束的轨迹，优化方法存在计算时间和数值稳定性限制",
                "方法要点：采用无优化方法，缝合短轨迹段，结合图搜索实现长距离、表达性强且近最优的实时规划",
                "实验或效果：仿真测试显示在50m x 50m环境中数毫秒内生成安全轨迹，硬件测试验证实时跟踪路径并处理非凸约束"
            ],
            "tags_zh": [
                "轨迹规划",
                "实时运动基元搜索",
                "无优化规划",
                "约束处理",
                "自主导航",
                "图搜索"
            ],
            "_index": 26
        },
        {
            "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction",
            "authors": [
                "Logan Lawrence",
                "Oindrila Saha",
                "Megan Wei",
                "Chen Sun",
                "Subhransu Maji",
                "Grant Van Horn"
            ],
            "arxiv_id": "2510.14885v1",
            "summary": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage.",
            "headline_zh": "提出nlg2choice方法以提升多模态大语言模型在细粒度视觉识别中的性能",
            "intro_zh": [
                "核心问题：多模态大语言模型在细粒度视觉分类中，处理自由形式响应和多选项问题存在挑战",
                "方法要点：采用两阶段方法，先开放问答，后文本约束解码提取最可能选项",
                "实验或效果：在七个细粒度视觉数据集上，分类和检索性能均得到提升"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "细粒度视觉分类",
                "答案提取",
                "约束解码",
                "零样本学习"
            ],
            "_index": 27
        },
        {
            "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention",
            "authors": [
                "Keli Liu",
                "Zhendong Wang",
                "Wengang Zhou",
                "Shaodong Xu",
                "Ruixiao Dong",
                "Houqiang Li"
            ],
            "arxiv_id": "2510.14882v1",
            "summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released.",
            "headline_zh": "提出ScaleWeaver框架，通过多尺度参考注意力实现高效可控文本到图像生成。",
            "intro_zh": [
                "核心问题：视觉自回归模型中可控生成机制不足，难以实现精确灵活控制。",
                "方法要点：引入参考注意力模块，优化MMDiT块，减少计算成本并稳定控制注入。",
                "实验或效果：实验显示生成质量高、控制精确，效率优于基于扩散的方法。"
            ],
            "tags_zh": [
                "文本到图像生成",
                "视觉自回归模型",
                "可控生成",
                "参考注意力",
                "参数高效微调",
                "多尺度处理"
            ],
            "_index": 28
        },
        {
            "title": "BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data",
            "authors": [
                "Roni Goldshmidt",
                "Hamish Scott",
                "Lorenzo Niccolini",
                "Shizhan Zhu",
                "Daniel Moura",
                "Orly Zvitia"
            ],
            "arxiv_id": "2510.14876v1",
            "summary": "Existing collision prediction methods often fail to distinguish between\nego-vehicle threats and random accidents not involving the ego vehicle, leading\nto excessive false alerts in real-world deployment. We present BADAS, a family\nof collision prediction models trained on Nexar's real-world dashcam collision\ndataset -- the first benchmark designed explicitly for ego-centric evaluation.\nWe re-annotate major benchmarks to identify ego involvement, add consensus\nalert-time labels, and synthesize negatives where needed, enabling fair AP/AUC\nand temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and\ncomes in two variants: BADAS-Open (trained on our 1.5k public videos) and\nBADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and\nNexar, BADAS achieves state-of-the-art AP/AUC and outperforms a\nforward-collision ADAS baseline while producing more realistic time-to-accident\nestimates. We release our BADAS-Open model weights and code, along with\nre-annotations of all evaluation datasets to promote ego-centric collision\nprediction research.",
            "headline_zh": "提出BADAS模型，利用真实行车记录仪数据预测自车碰撞，减少误报。",
            "intro_zh": [
                "现有碰撞预测方法难以区分自车威胁与无关事故，导致高误报率。",
                "使用V-JEPA2骨干网络端到端训练，提供公开和专有数据变体。",
                "在多个数据集上实现最优AP/AUC，并改进碰撞时间估计。"
            ],
            "tags_zh": [
                "碰撞预测",
                "自车中心评估",
                "行车记录仪数据",
                "端到端训练",
                "V-JEPA2骨干网络"
            ],
            "_index": 29
        },
        {
            "title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions",
            "authors": [
                "Guangyi Han",
                "Wei Zhai",
                "Yuhang Yang",
                "Yang Cao",
                "Zheng-Jun Zha"
            ],
            "arxiv_id": "2510.14874v1",
            "summary": "Hand-object interaction (HOI) is fundamental for humans to express intent.\nExisting HOI generation research is predominantly confined to fixed grasping\npatterns, where control is tied to physical priors such as force closure or\ngeneric intent instructions, even when expressed through elaborate language.\nSuch an overly general conditioning imposes a strong inductive bias for stable\ngrasps, thus failing to capture the diversity of daily HOI. To address these\nlimitations, we introduce Free-Form HOI Generation, which aims to generate\ncontrollable, diverse, and physically plausible HOI conditioned on fine-grained\nintent, extending HOI from grasping to free-form interactions, like pushing,\npoking, and rotating. To support this task, we construct WildO2, an in-the-wild\ndiverse 3D HOI dataset, which includes diverse HOI derived from internet\nvideos. Specifically, it contains 4.4k unique interactions across 92 intents\nand 610 object categories, each with detailed semantic annotations. Building on\nthis dataset, we propose TOUCH, a three-stage framework centered on a\nmulti-level diffusion model that facilitates fine-grained semantic control to\ngenerate versatile hand poses beyond grasping priors. This process leverages\nexplicit contact modeling for conditioning and is subsequently refined with\ncontact consistency and physical constraints to ensure realism. Comprehensive\nexperiments demonstrate our method's ability to generate controllable, diverse,\nand physically plausible hand interactions representative of daily activities.\nThe project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.",
            "headline_zh": "提出TOUCH框架以生成可控、多样且物理合理的手-物体自由交互。",
            "intro_zh": [
                "现有手-物体交互生成局限于固定抓取模式，缺乏日常交互多样性。",
                "TOUCH使用多级扩散模型，结合细粒度语义控制，生成超越抓取的手势。",
                "实验验证方法能生成可控、多样且物理合理的手交互，代表日常活动。"
            ],
            "tags_zh": [
                "手-物体交互生成",
                "自由形式交互",
                "多级扩散模型",
                "细粒度语义控制",
                "物理约束优化",
                "WildO2数据集"
            ],
            "_index": 30
        },
        {
            "title": "Benchmarking Multimodal Large Language Models for Face Recognition",
            "authors": [
                "Hatef Otroshi Shahreza",
                "Sébastien Marcel"
            ],
            "arxiv_id": "2510.14866v1",
            "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\nacross diverse vision-and-language tasks. However, their potential in face\nrecognition remains underexplored. In particular, the performance of\nopen-source MLLMs needs to be evaluated and compared with existing face\nrecognition models on standard benchmarks with similar protocol. In this work,\nwe present a systematic benchmark of state-of-the-art MLLMs for face\nrecognition on several face recognition datasets, including LFW, CALFW, CPLFW,\nCFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich\nsemantic cues useful for face-related tasks, they lag behind specialized models\nin high-precision recognition scenarios in zero-shot applications. This\nbenchmark provides a foundation for advancing MLLM-based face recognition,\noffering insights for the design of next-generation models with higher accuracy\nand generalization. The source code of our benchmark is publicly available in\nthe project page.",
            "headline_zh": "基准测试多模态大语言模型在标准人脸识别数据集上的性能",
            "intro_zh": [
                "核心问题：多模态大语言模型在人脸识别中的潜力未被充分探索，需与专业模型比较。",
                "方法要点：系统评估开源MLLMs在LFW等数据集上的零样本识别能力。",
                "实验或效果：MLLMs捕获语义线索，但在高精度场景下落后于专业模型。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "人脸识别",
                "基准测试",
                "零样本学习",
                "开源模型"
            ],
            "_index": 31
        },
        {
            "title": "Multi-modal video data-pipelines for machine learning with minimal human supervision",
            "authors": [
                "Mihai-Cristian Pîrvu",
                "Marius Leordeanu"
            ],
            "arxiv_id": "2510.14862v1",
            "summary": "The real-world is inherently multi-modal at its core. Our tools observe and\ntake snapshots of it, in digital form, such as videos or sounds, however much\nof it is lost. Similarly for actions and information passing between humans,\nlanguages are used as a written form of communication. Traditionally, Machine\nLearning models have been unimodal (i.e. rgb -> semantic or text ->\nsentiment_class). Recent trends go towards bi-modality, where images and text\nare learned together, however, in order to truly understand the world, we need\nto integrate all these independent modalities. In this work we try to combine\nas many visual modalities as we can using little to no human supervision. In\norder to do this, we use pre-trained experts and procedural combinations\nbetween them on top of raw videos using a fully autonomous data-pipeline, which\nwe also open-source. We then make use of PHG-MAE, a model specifically designed\nto leverage multi-modal data. We show that this model which was efficiently\ndistilled into a low-parameter (<1M) can have competitive results compared to\nmodels of ~300M parameters. We deploy this model and analyze the use-case of\nreal-time semantic segmentation from handheld devices or webcams on commodity\nhardware. Finally, we deploy other off-the-shelf models using the same\nframework, such as DPT for near real-time depth estimation.",
            "headline_zh": "提出多模态视频数据流水线，实现无监督学习并部署于实时语义分割",
            "intro_zh": [
                "核心问题：现实世界多模态数据在数字化中丢失，需整合多模态以提升理解。",
                "方法要点：使用预训练专家和自主数据流水线，结合PHG-MAE模型进行多模态学习。",
                "实验或效果：模型参数量小于1M，性能媲美300M参数模型，并部署于实时应用。"
            ],
            "tags_zh": [
                "多模态学习",
                "无监督数据流水线",
                "模型蒸馏",
                "实时语义分割",
                "深度估计"
            ],
            "_index": 32
        },
        {
            "title": "A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation",
            "authors": [
                "Harsha Kotla",
                "Arun Kumar Rajasekaran",
                "Hannah Rana"
            ],
            "arxiv_id": "2510.14855v1",
            "summary": "Early detection of melanoma has grown to be essential because it\nsignificantly improves survival rates, but automated analysis of skin lesions\nstill remains challenging. ABCDE, which stands for Asymmetry, Border\nirregularity, Color variation, Diameter, and Evolving, is a well-known\nclassification method for skin lesions, but most deep learning mechanisms treat\nit as a black box, as most of the human interpretable features are not\nexplained. In this work, we propose a deep learning framework that both\nclassifies skin lesions into categories and also quantifies scores for each\nABCD feature. It simulates the evolution of these features over time in order\nto represent the E aspect, opening more windows for future exploration. The A,\nB, C, and D values are quantified particularly within this work. Moreover, this\nframework also visualizes ABCD feature trajectories in latent space as skin\nlesions evolve from benign nevuses to malignant melanoma. The experiments are\nconducted using the HAM10000 dataset that contains around ten thousand images\nof skin lesions of varying stages. In summary, the classification worked with\nan accuracy of around 89 percent, with melanoma AUC being 0.96, while the\nfeature evaluation performed well in predicting asymmetry, color variation, and\ndiameter, though border irregularity remains more difficult to model. Overall,\nthis work provides a deep learning framework that will allow doctors to link ML\ndiagnoses to clinically relevant criteria, thus improving our understanding of\nskin cancer progression.",
            "headline_zh": "提出多任务深度学习框架，用于皮肤病变分类、ABCDE特征量化与演化模拟。",
            "intro_zh": [
                "核心问题：皮肤病变自动分析挑战大，ABCDE特征在深度学习中缺乏可解释性。",
                "方法要点：框架同时分类病变并量化ABCD特征，模拟E特征演化。",
                "实验或效果：在HAM10000数据集上，分类准确率约89%，AUC达0.96。"
            ],
            "tags_zh": [
                "皮肤病变分类",
                "多任务学习",
                "特征量化",
                "演化模拟",
                "医学图像分析"
            ],
            "_index": 33
        },
        {
            "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time",
            "authors": [
                "Jakob Bichler",
                "Andreu Matoses Gimenez",
                "Javier Alonso-Mora"
            ],
            "arxiv_id": "2510.14851v1",
            "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous\nmulti-robot teams that incorporates dynamic coalition formation and task\nprecedence constraints. Sadcher is trained through Imitation Learning and\ncombines graph attention and transformers to predict assignment rewards between\nrobots and tasks. Based on the predicted rewards, a relaxed bipartite matching\nstep generates high-quality schedules with feasibility guarantees. We\nexplicitly model robot and task positions, task durations, and robots'\nremaining processing times, enabling advanced temporal and spatial reasoning\nand generalization to environments with different spatiotemporal distributions\ncompared to training. Trained on optimally solved small-scale instances, our\nmethod can scale to larger task sets and team sizes. Sadcher outperforms other\nlearning-based and heuristic baselines on randomized, unseen problems for small\nand medium-sized teams with computation times suitable for real-time operation.\nWe also explore sampling-based variants and evaluate scalability across robot\nand task counts. In addition, we release our dataset of 250,000 optimal\nschedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/",
            "headline_zh": "提出SADCHER框架，用于异构多机器人实时任务分配，结合动态联盟与任务优先级约束。",
            "intro_zh": [
                "核心问题：异构多机器人团队在实时环境中的任务分配，需处理动态联盟和任务优先级约束。",
                "方法要点：使用模仿学习训练，结合图注意力和变换器预测奖励，并通过松弛二分匹配生成可行调度。",
                "实验或效果：在随机未见问题上优于其他方法，计算时间适合实时操作，并评估了可扩展性。"
            ],
            "tags_zh": [
                "异构多机器人系统",
                "实时任务分配",
                "动态联盟形成",
                "图注意力网络",
                "模仿学习",
                "任务调度"
            ],
            "_index": 34
        },
        {
            "title": "Multi Agent Switching Mode Controller for Sound Source localization",
            "authors": [
                "Marcello Sorge",
                "Nicola Cigarini",
                "Riccardo Lorigiola",
                "Giulia Michieletto",
                "Andrea Masiero",
                "Angelo Cenedese",
                "Alberto Guarnieri"
            ],
            "arxiv_id": "2510.14849v1",
            "summary": "Source seeking is an important topic in robotic research, especially\nconsidering sound-based sensors since they allow the agents to locate a target\neven in critical conditions where it is not possible to establish a direct line\nof sight. In this work, we design a multi- agent switching mode control\nstrategy for acoustic-based target localization. Two scenarios are considered:\nsingle source localization, in which the agents are driven maintaining a rigid\nformation towards the target, and multi-source scenario, in which each agent\nsearches for the targets independently from the others.",
            "headline_zh": "提出多智能体切换模式控制策略，用于单源和多源声源定位场景。",
            "intro_zh": [
                "核心问题：在无直接视线条件下，机器人如何利用声学传感器定位目标。",
                "方法要点：设计多智能体切换模式控制，支持刚性编队和独立搜索。",
                "实验或效果：评估单源和多源场景下的定位性能，未知具体效果。"
            ],
            "tags_zh": [
                "声源定位",
                "多智能体控制",
                "切换模式控制",
                "机器人感知",
                "声学传感器"
            ],
            "_index": 35
        },
        {
            "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
            "authors": [
                "Meiqi Wu",
                "Jiashu Zhu",
                "Xiaokun Feng",
                "Chubin Chen",
                "Chen Zhu",
                "Bingze Song",
                "Fangyuan Mao",
                "Jiahong Wu",
                "Xiangxiang Chu",
                "Kaiqi Huang"
            ],
            "arxiv_id": "2510.14847v1",
            "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
            "headline_zh": "提出ImagerySearch自适应测试时搜索策略以提升想象力场景视频生成质量",
            "intro_zh": [
                "核心问题：视频生成模型在想象力场景中性能下降，因提示包含罕见共现概念和长距离语义关系。",
                "方法要点：动态调整推理搜索空间和奖励函数，适应提示语义关系，提升视频连贯性和视觉合理性。",
                "实验或效果：在LDT-Bench和VBench上优于基线方法，验证了策略的有效性和泛化能力。"
            ],
            "tags_zh": [
                "视频生成",
                "测试时搜索",
                "自适应策略",
                "长距离语义",
                "想象力场景",
                "基准评估"
            ],
            "_index": 36
        },
        {
            "title": "Backdoor Unlearning by Linear Task Decomposition",
            "authors": [
                "Amel Abdelraheem",
                "Alessandro Favero",
                "Gerome Bovet",
                "Pascal Frossard"
            ],
            "arxiv_id": "2510.14845v1",
            "summary": "Foundation models have revolutionized computer vision by enabling broad\ngeneralization across diverse tasks. Yet, they remain highly susceptible to\nadversarial perturbations and targeted backdoor attacks. Mitigating such\nvulnerabilities remains an open challenge, especially given that the\nlarge-scale nature of the models prohibits retraining to ensure safety.\nExisting backdoor removal approaches rely on costly fine-tuning to override the\nharmful behavior, and can often degrade performance on other unrelated tasks.\nThis raises the question of whether backdoors can be removed without\ncompromising the general capabilities of the models. In this work, we address\nthis question and study how backdoors are encoded in the model weight space,\nfinding that they are disentangled from other benign tasks. Specifically, this\nseparation enables the isolation and erasure of the backdoor's influence on the\nmodel with minimal impact on clean performance. Building on this insight, we\nintroduce a simple unlearning method that leverages such disentanglement.\nThrough extensive experiments with CLIP-based models and common adversarial\ntriggers, we show that, given the knowledge of the attack, our method achieves\napproximately perfect unlearning, while retaining, on average, 96% of clean\naccuracy. Additionally, we demonstrate that even when the attack and its\npresence are unknown, our method successfully unlearns backdoors by proper\nestimation using reverse-engineered triggers. Overall, our method consistently\nyields better unlearning and clean accuracy tradeoffs when compared to present\nstate-of-the-art defenses.",
            "headline_zh": "提出线性任务分解方法以移除基础模型中的后门攻击",
            "intro_zh": [
                "基础模型易受后门攻击，现有方法需高成本微调且可能损害其他任务性能",
                "发现后门在权重空间中与良性任务解耦，可隔离并擦除其影响",
                "实验显示在已知攻击下实现近乎完美移除，平均保留96%干净准确率"
            ],
            "tags_zh": [
                "后门移除",
                "基础模型安全",
                "线性任务分解",
                "权重空间分析",
                "CLIP模型",
                "对抗攻击防御"
            ],
            "_index": 37
        },
        {
            "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models",
            "authors": [
                "Yixuan Li",
                "Yuhui Chen",
                "Mingcai Zhou",
                "Haoran Li"
            ],
            "arxiv_id": "2510.14836v1",
            "summary": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)\nmodels to accomplish fine-grained manipulation tasks. However, existing\napproaches often lack the ability to understand and reason over the essential\n3D structures necessary for precise control. To address this limitation, we\npropose QDepth-VLA, a general framework that augments VLA models with an\nauxiliary depth prediction task. A dedicated depth expert is designed to\npredict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,\nenabling the model to learn depth-aware representations that capture critical\ngeometric cues. Experimental results on the simulation benchmarks and\nreal-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning\nand competitive performance on manipulation tasks.",
            "headline_zh": "提出QDepth-VLA框架，通过量化深度预测增强VLA模型的空间推理能力",
            "intro_zh": [
                "现有VLA模型缺乏对3D结构的理解，影响精细操作任务性能",
                "引入辅助深度预测任务，使用VQ-VAE编码量化深度图以学习深度感知表示",
                "在仿真和真实任务中验证，提升空间推理和操作任务表现"
            ],
            "tags_zh": [
                "视觉语言动作模型",
                "深度预测",
                "空间推理",
                "量化表示",
                "辅助监督",
                "操作任务"
            ],
            "_index": 38
        },
        {
            "title": "Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data",
            "authors": [
                "Qi Chen",
                "Xinze Zhou",
                "Chen Liu",
                "Hao Chen",
                "Wenxuan Li",
                "Zekun Jiang",
                "Ziyan Huang",
                "Yuxuan Zhao",
                "Dexin Yu",
                "Junjun He",
                "Yefeng Zheng",
                "Ling Shao",
                "Alan Yuille",
                "Zongwei Zhou"
            ],
            "arxiv_id": "2510.14831v1",
            "summary": "AI for tumor segmentation is limited by the lack of large, voxel-wise\nannotated datasets, which are hard to create and require medical experts. In\nour proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found\nthat AI performance stopped improving after 1,500 scans. With synthetic data,\nwe reached the same performance using only 500 real scans. This finding\nsuggests that synthetic data can steepen data scaling laws, enabling more\nefficient model training than real data alone. Motivated by these lessons, we\ncreated AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130\ntumor instances per-voxel manually annotated in six organs (pancreas, liver,\nkidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23\nexpert radiologists, it is several orders of magnitude larger than existing\npublic tumor datasets. While we continue expanding the dataset, the current\nversion of AbdomenAtlas 2.0 already provides a strong foundation--based on\nlessons from the JHH dataset--for training AI to segment tumors in six organs.\nIt achieves notable improvements over public datasets, with a +7% DSC gain on\nin-distribution tests and +16% on out-of-distribution tests.",
            "headline_zh": "提出AbdomenAtlas 2.0数据集，结合合成数据提升肿瘤分割效率与泛化能力",
            "intro_zh": [
                "核心问题：肿瘤分割AI受限于大规模体素标注数据稀缺，标注成本高且依赖专家",
                "方法要点：利用合成数据增强真实数据，在JHH数据集上验证合成数据可减少真实数据需求",
                "实验或效果：AbdomenAtlas 2.0在分布内测试DSC提升7%，分布外测试提升16%"
            ],
            "tags_zh": [
                "肿瘤分割",
                "合成数据",
                "医学影像",
                "CT扫描",
                "数据增强",
                "泛化性能"
            ],
            "_index": 39
        },
        {
            "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning",
            "authors": [
                "Kun Lei",
                "Huanyu Li",
                "Dongjie Yu",
                "Zhenyu Wei",
                "Lingxiao Guo",
                "Zhennan Jiang",
                "Ziyu Wang",
                "Shiyu Liang",
                "Huazhe Xu"
            ],
            "arxiv_id": "2510.14830v1",
            "summary": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours.",
            "headline_zh": "提出RL-100框架，通过三阶段强化学习实现真实世界机器人操作的高性能。",
            "intro_zh": [
                "核心问题：真实世界机器人操作需高可靠性和效率，接近或超越人类水平。",
                "方法要点：结合模仿学习、离线强化学习和在线强化学习，使用扩散策略和一致性蒸馏。",
                "实验或效果：在七项任务中实现100%成功率，共900次试验，展示多小时鲁棒性。"
            ],
            "tags_zh": [
                "机器人操作",
                "强化学习",
                "扩散策略",
                "离线策略评估",
                "一致性蒸馏",
                "真实世界应用"
            ],
            "_index": 40
        },
        {
            "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning",
            "authors": [
                "Jinrui Liu",
                "Bingyan Nie",
                "Boyu Li",
                "Yaran Chen",
                "Yuze Wang",
                "Shunsen He",
                "Haoran Li"
            ],
            "arxiv_id": "2510.14828v1",
            "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.",
            "headline_zh": "提出RoboGPT-R1两阶段微调框架以增强机器人在长视距操作任务中的规划能力",
            "intro_zh": [
                "核心问题：通用视觉语言模型在机器人规划中泛化差、物理理解不足，难以完成复杂长视距操作任务。",
                "方法要点：采用监督微调获取基础知识，再通过强化学习提升视觉空间理解和推理能力。",
                "实验或效果：在EmbodiedBench基准上，优于GPT-4o-mini 21.33%，优于其他基于Qwen2.5-VL-7B的工作20.33%。"
            ],
            "tags_zh": [
                "机器人规划",
                "强化学习",
                "视觉语言模型",
                "长视距操作",
                "两阶段微调",
                "奖励函数设计"
            ],
            "_index": 41
        },
        {
            "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping",
            "authors": [
                "Yufei Zhu",
                "Shih-Min Yang",
                "Andrey Rudenko",
                "Tomasz P. Kucner",
                "Achim J. Lilienthal",
                "Martin Magnusson"
            ],
            "arxiv_id": "2510.14827v1",
            "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns.",
            "headline_zh": "提出基于隐式神经函数的连续时空运动映射方法以提升机器人操作安全与效率",
            "intro_zh": [
                "核心问题：现有动态地图使用离散空间采样，构建成本高且难以处理不均匀采样区域。",
                "方法要点：采用隐式神经函数直接映射坐标到半包裹高斯混合模型参数，实现连续时空建模。",
                "实验或效果：在真实世界数据集上验证，精度更高、稀疏区域速度分布更平滑且计算高效。"
            ],
            "tags_zh": [
                "隐式神经表示",
                "动态地图建模",
                "时空运动模式",
                "机器人导航",
                "高斯混合模型"
            ],
            "_index": 42
        },
        {
            "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking",
            "authors": [
                "Ziqi Dai",
                "Xin Zhang",
                "Mingxin Li",
                "Yanzhao Zhang",
                "Dingkun Long",
                "Pengjun Xie",
                "Meishan Zhang",
                "Wenjie Li",
                "Min Zhang"
            ],
            "arxiv_id": "2510.14824v1",
            "summary": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area.",
            "headline_zh": "比较监督微调与对比学习在LLM重排序中的效果，发现SFT更优",
            "intro_zh": [
                "核心问题：LLM重排序中监督微调与对比学习哪种目标更有效",
                "方法要点：分解目标为权重和方向，提出统一分析框架",
                "实验或效果：SFT权重更强，在MRB基准上实现SOTA"
            ],
            "tags_zh": [
                "LLM重排序",
                "监督微调",
                "对比学习",
                "多模态检索",
                "权重分析"
            ],
            "_index": 43
        },
        {
            "title": "FraQAT: Quantization Aware Training with Fractional bits",
            "authors": [
                "Luca Morreale",
                "Alberto Gil C. P. Ramos",
                "Malcolm Chadwick",
                "Mehid Noroozi",
                "Ruchika Chavhan",
                "Abhinav Mehrotra",
                "Sourav Bhattacharya"
            ],
            "arxiv_id": "2510.14823v1",
            "summary": "State-of-the-art (SOTA) generative models have demonstrated impressive\ncapabilities in image synthesis or text generation, often with a large capacity\nmodel. However, these large models cannot be deployed on smartphones due to the\nlimited availability of on-board memory and computations. Quantization methods\nlower the precision of the model parameters, allowing for efficient\ncomputations, \\eg, in \\INT{8}. Although aggressive quantization addresses\nefficiency and memory constraints, preserving the quality of the model remains\na challenge. To retain quality in previous aggressive quantization, we propose\na new fractional bits quantization (\\short) approach. The novelty is a simple\nyet effective idea: we progressively reduce the model's precision from 32 to 4\nbits per parameter, and exploit the fractional bits during optimization to\nmaintain high generation quality. We show that the \\short{} yields improved\nquality on a variety of diffusion models, including SD3.5-Medium, Sana,\n\\pixart, and FLUX.1-schnell, while achieving $4-7\\%$ lower FiD than standard\nQAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the\nQualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).",
            "headline_zh": "提出分数位量化方法以在移动设备上保持生成模型质量",
            "intro_zh": [
                "核心问题：大模型量化后质量下降，难以部署于内存受限设备",
                "方法要点：渐进降低精度至4位，利用分数位优化保持生成质量",
                "实验或效果：在多种扩散模型上FiD降低4-7%，成功部署于三星S25U"
            ],
            "tags_zh": [
                "量化感知训练",
                "分数位量化",
                "生成模型",
                "移动部署",
                "扩散模型"
            ],
            "_index": 44
        },
        {
            "title": "Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning",
            "authors": [
                "Ji Cao",
                "Yu Wang",
                "Tongya Zheng",
                "Zujie Ren",
                "Canghong Jin",
                "Gang Chen",
                "Mingli Song"
            ],
            "arxiv_id": "2510.14819v1",
            "summary": "Trajectory Representation Learning (TRL) aims to encode raw trajectories into\nlow-dimensional vectors, which can then be leveraged in various downstream\ntasks, including travel time estimation, location prediction, and trajectory\nsimilarity analysis. However, existing TRL methods suffer from a key oversight:\ntreating trajectories as isolated spatio-temporal sequences, without\nconsidering the external environment and internal route choice behavior that\ngovern their formation. To bridge this gap, we propose a novel framework that\nunifies comprehensive environment \\textbf{P}erception and explicit\n\\textbf{R}oute choice modeling for effective \\textbf{Traj}ectory representation\nlearning, dubbed \\textbf{PRTraj}. Specifically, PRTraj first introduces an\nEnvironment Perception Module to enhance the road network by capturing\nmulti-granularity environmental semantics from surrounding POI distributions.\nBuilding on this environment-aware backbone, a Route Choice Encoder then\ncaptures the route choice behavior inherent in each trajectory by modeling its\nconstituent road segment transitions as a sequence of decisions. These\nroute-choice-aware representations are finally aggregated to form the global\ntrajectory embedding. Extensive experiments on 3 real-world datasets across 5\ndownstream tasks validate the effectiveness and generalizability of PRTraj.\nMoreover, PRTraj demonstrates strong data efficiency, maintaining robust\nperformance under few-shot scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/PRTraj.",
            "headline_zh": "提出PRTraj框架，统一环境感知与路径选择建模以改进轨迹表示学习",
            "intro_zh": [
                "现有轨迹表示学习方法忽略外部环境和内部路径选择行为，导致表示不完整",
                "PRTraj通过环境感知模块和路径选择编码器，捕获多粒度语义和决策序列",
                "在3个真实数据集和5个下游任务中验证有效性，并展示强数据效率"
            ],
            "tags_zh": [
                "轨迹表示学习",
                "环境感知",
                "路径选择建模",
                "多粒度语义",
                "下游任务评估"
            ],
            "_index": 45
        },
        {
            "title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks",
            "authors": [
                "Pedro R. A. S. Bassi",
                "Xinze Zhou",
                "Wenxuan Li",
                "Szymon Płotka",
                "Jieneng Chen",
                "Qi Chen",
                "Zheren Zhu",
                "Jakub Prządo",
                "Ibrahim E. Hamacı",
                "Sezgin Er",
                "Yuhan Wang",
                "Ashwin Kumar",
                "Bjoern Menze",
                "Jarosław B. Ćwikła",
                "Yuyin Zhou",
                "Akshay S. Chaudhari",
                "Curtis P. Langlotz",
                "Sergio Decherchi",
                "Andrea Cavalli",
                "Kang Wang",
                "Yang Yang",
                "Alan L. Yuille",
                "Zongwei Zhou"
            ],
            "arxiv_id": "2510.14803v1",
            "summary": "Early tumor detection save lives. Each year, more than 300 million computed\ntomography (CT) scans are performed worldwide, offering a vast opportunity for\neffective cancer screening. However, detecting small or early-stage tumors on\nthese CT scans remains challenging, even for experts. Artificial intelligence\n(AI) models can assist by highlighting suspicious regions, but training such\nmodels typically requires extensive tumor masks--detailed, voxel-wise outlines\nof tumors manually drawn by radiologists. Drawing these masks is costly,\nrequiring years of effort and millions of dollars. In contrast, nearly every CT\nscan in clinical practice is already accompanied by medical reports describing\nthe tumor's size, number, appearance, and sometimes, pathology\nresults--information that is rich, abundant, and often underutilized for AI\ntraining. We introduce R-Super, which trains AI to segment tumors that match\ntheir descriptions in medical reports. This approach scales AI training with\nlarge collections of readily available medical reports, substantially reducing\nthe need for manually drawn tumor masks. When trained on 101,654 reports, AI\nmodels achieved performance comparable to those trained on 723 masks. Combining\nreports and masks further improved sensitivity by +13% and specificity by +8%,\nsurpassing radiologists in detecting five of the seven tumor types. Notably,\nR-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,\nbladder, uterus, and esophagus, for which no public masks or AI models\npreviously existed. This study challenges the long-held belief that\nlarge-scale, labor-intensive tumor mask creation is indispensable, establishing\na scalable and accessible path toward early detection across diverse tumor\ntypes.\n  We plan to release our trained models, code, and dataset at\nhttps://github.com/MrGiovanni/R-Super",
            "headline_zh": "提出R-Super方法，利用医学报告训练AI进行多肿瘤早期检测，减少对肿瘤掩码的依赖。",
            "intro_zh": [
                "核心问题：肿瘤早期检测依赖昂贵的手动肿瘤掩码，限制了AI模型的可扩展性。",
                "方法要点：使用医学报告描述训练AI模型，实现肿瘤分割，无需大量掩码。",
                "实验或效果：在101,654份报告上训练，性能媲美723个掩码，结合掩码提升敏感性和特异性。"
            ],
            "tags_zh": [
                "肿瘤分割",
                "医学报告训练",
                "多肿瘤检测",
                "AI可扩展性",
                "早期癌症筛查"
            ],
            "_index": 46
        },
        {
            "title": "Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images",
            "authors": [
                "Usama Sajjad",
                "Abdul Rehman Akbar",
                "Ziyu Su",
                "Deborah Knight",
                "Wendy L. Frankel",
                "Metin N. Gurcan",
                "Wei Chen",
                "Muhammad Khalid Khan Niazi"
            ],
            "arxiv_id": "2510.14800v1",
            "summary": "Colorectal cancer (CRC) remains the third most prevalent malignancy globally,\nwith approximately 154,000 new cases and 54,000 projected deaths anticipated\nfor 2025. The recent advancement of foundation models in computational\npathology has been largely propelled by task agnostic methodologies that can\noverlook organ-specific crucial morphological patterns that represent distinct\nbiological processes that can fundamentally influence tumor behavior,\ntherapeutic response, and patient outcomes. The aim of this study is to develop\na novel, interpretable AI model, PRISM (Prognostic Representation of Integrated\nSpatial Morphology), that incorporates a continuous variability spectrum within\neach distinct morphology to characterize phenotypic diversity and reflecting\nthe principle that malignant transformation occurs through incremental\nevolutionary processes rather than abrupt phenotypic shifts. PRISM is trained\non 8.74 million histological images extracted from surgical resection specimens\nof 424 patients with stage III CRC. PRISM achieved superior prognostic\nperformance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;\nHR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific\nmethods by 15% and AI foundation models by ~23% accuracy. It showed\nsex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable\nperformance across clinicopathological subgroups, with minimal accuracy\nfluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,\nreplicating the Alliance cohort finding of no survival difference between\ntreatments.",
            "headline_zh": "提出PRISM模型，通过整合形态学变异预测结直肠癌五年生存率",
            "intro_zh": [
                "核心问题：现有AI模型忽视器官特异性形态模式，影响结直肠癌预后预测准确性。",
                "方法要点：开发PRISM模型，利用连续变异谱表征形态多样性，模拟渐进性恶性转化过程。",
                "实验或效果：在424患者数据上训练，AUC达0.70，准确率68.37%，优于现有方法。"
            ],
            "tags_zh": [
                "结直肠癌预后预测",
                "形态学整合模型",
                "计算病理学",
                "生存分析",
                "AI可解释性"
            ],
            "_index": 47
        },
        {
            "title": "CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection",
            "authors": [
                "Hojun Choi",
                "Youngsun Lim",
                "Jaeyo Shin",
                "Hyunjung Shim"
            ],
            "arxiv_id": "2510.14792v1",
            "summary": "Open-vocabulary object detection (OVD) seeks to recognize and localize object\ncategories beyond those seen during training. Recent approaches typically\nleverage vision-language models (VLMs) to generate pseudo-labels using\nimage-text alignment, allowing detectors to generalize to unseen classes\nwithout explicit supervision. However, these methods depend heavily on direct\nimage-text matching, neglecting the intermediate reasoning steps essential for\ninterpreting semantically complex scenes. This results in limited robustness\nwhen confronted with crowded or occluded visual contexts. In this paper, we\nintroduce CoT-PL, a new framework that employs structured visual\nchain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL\ndecomposes object understanding into three interpretable steps: (1) region\nperception even for unseen objects, (2) category recognition via zero-shot\nreasoning, and (3) background grounding to separate semantically complex\nobjects. Crucially, the third step naturally motivates our contrastive\nbackground learning (CBL) that uses the pre-computed background cues as\nnegatives to promote feature disentanglement between objects and background. In\nthis way, CoT reasoning and CBL form an integrated pipeline tailored to robust\npseudo-labeling in crowded or occluded scenes. Notably, in these two settings,\nour novel-class pseudo-label quality achieves relative improvements of 103.4%\nand 168.4% over the best prior, respectively. Our extensive experiments\ndemonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9\nmask AP on LVIS for novel classes, setting a new state of the art.",
            "headline_zh": "提出CoT-PL框架，结合视觉链式推理与伪标注，提升拥挤或遮挡场景下的开放词汇目标检测鲁棒性。",
            "intro_zh": [
                "核心问题：现有方法依赖直接图像-文本匹配，忽略中间推理步骤，在拥挤或遮挡场景中鲁棒性不足。",
                "方法要点：引入视觉链式推理，分解为区域感知、类别识别和背景接地三步，并集成对比背景学习。",
                "实验或效果：在开放词汇COCO和LVIS数据集上，新类别检测性能显著提升，伪标签质量改进超100%。"
            ],
            "tags_zh": [
                "开放词汇目标检测",
                "视觉链式推理",
                "伪标注",
                "对比背景学习",
                "零样本识别",
                "场景鲁棒性"
            ],
            "_index": 48
        },
        {
            "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning",
            "authors": [
                "Aderik Verraest",
                "Stavrow Bahnam",
                "Robin Ferede",
                "Guido de Croon",
                "Christophe De Wagter"
            ],
            "arxiv_id": "2510.14783v1",
            "summary": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight.",
            "headline_zh": "提出SkyDreamer以解决端到端视觉无人机竞速的鲁棒性与高性能问题",
            "intro_zh": [
                "核心问题：现有自主无人机竞速系统缺乏端到端视觉方法，无法同时实现全仿真到真实转移、板载执行和冠军级性能。",
                "方法要点：基于模型强化学习，世界模型解码特权信息，作为隐式状态估计器，提升可解释性。",
                "实验或效果：在真实世界实现高速飞行，速度达21 m/s，加速度6 g，并展示对视觉模糊和电池耗尽的鲁棒性。"
            ],
            "tags_zh": [
                "无人机竞速",
                "端到端视觉",
                "模型强化学习",
                "仿真到真实转移",
                "板载执行",
                "状态估计"
            ],
            "_index": 49
        },
        {
            "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation",
            "authors": [
                "Xu Chi",
                "Chao Zhang",
                "Yang Su",
                "Lingfeng Dou",
                "Fujia Yang",
                "Jiakuo Zhao",
                "Haoyu Zhou",
                "Xiaoyou Jia",
                "Yong Zhou",
                "Shan An"
            ],
            "arxiv_id": "2510.14771v1",
            "summary": "Accurate and high-fidelity demonstration data acquisition is a critical\nbottleneck for deploying robot Imitation Learning (IL) systems, particularly\nwhen dealing with heterogeneous robotic platforms. Existing teleoperation\nsystems often fail to guarantee high-precision data collection across diverse\ntypes of teleoperation devices. To address this, we developed Open TeleDex, a\nunified teleoperation framework engineered for demonstration data collection.\nOpen TeleDex specifically tackles the TripleAny challenge, seamlessly\nsupporting any robotic arm, any dexterous hand, and any external input device.\nFurthermore, we propose a novel hand pose retargeting algorithm that\nsignificantly boosts the interoperability of Open TeleDex, enabling robust and\naccurate compatibility with an even wider spectrum of heterogeneous master and\nslave equipment. Open TeleDex establishes a foundational, high-quality, and\npublicly available platform for accelerating both academic research and\nindustry development in complex robotic manipulation and IL.",
            "headline_zh": "提出Open TeleDex统一遥操作系统以解决异构机器人平台模仿学习数据采集难题",
            "intro_zh": [
                "核心问题：异构机器人平台难以获取高精度演示数据，阻碍模仿学习部署。",
                "方法要点：开发硬件无关框架，支持任意机械臂、灵巧手和输入设备。",
                "实验或效果：提出新手部姿态重定向算法，提升系统兼容性和数据质量。"
            ],
            "tags_zh": [
                "遥操作系统",
                "模仿学习",
                "灵巧操作",
                "数据采集",
                "硬件无关",
                "姿态重定向"
            ],
            "_index": 50
        },
        {
            "title": "MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks",
            "authors": [
                "Zhang Nengbo",
                "Hann Woei Ho",
                "Ye Zhou"
            ],
            "arxiv_id": "2510.14770v1",
            "summary": "Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in\nenvironments, where conventional radio-based methods suffer from spectrum\ncongestion, jamming, and high power consumption. Inspired by the waggle dance\nof honeybees, which efficiently communicate the location of food sources\nwithout sound or contact, we propose a novel visual communication framework for\nMAV swarms using motion-based signaling. In this framework, MAVs convey\ninformation, such as heading and distance, through deliberate flight patterns,\nwhich are passively captured by event cameras and interpreted using a\npredefined visual codebook of four motion primitives: vertical (up/down),\nhorizontal (left/right), left-to-up-to-right, and left-to-down-to-right,\nrepresenting control symbols (``start'', ``end'', ``1'', ``0''). To decode\nthese signals, we design an event frame-based segmentation model and a\nlightweight Spiking Neural Network (SNN) for action recognition. An integrated\ndecoding algorithm then combines segmentation and classification to robustly\ninterpret MAV motion sequences. Experimental results validate the framework's\neffectiveness, which demonstrates accurate decoding and low power consumption,\nand highlights its potential as an energy-efficient alternative for MAV\ncommunication in constrained environments.",
            "headline_zh": "提出基于运动和事件视觉的MAV间通信框架以解决受限环境中的通信挑战",
            "intro_zh": [
                "核心问题：MAV群在频谱拥堵和干扰环境中可靠通信困难",
                "方法要点：使用事件相机和SNN解码MAV飞行模式传递信息",
                "实验或效果：验证了准确解码和低功耗，适合受限环境"
            ],
            "tags_zh": [
                "微型飞行器通信",
                "事件视觉",
                "脉冲神经网络",
                "运动基信号",
                "视觉解码",
                "低功耗通信"
            ],
            "_index": 51
        },
        {
            "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery",
            "authors": [
                "Fan Yang",
                "Zixuan Huang",
                "Abhinav Kumar",
                "Sergio Aguilera Marinovic",
                "Soshi Iba",
                "Rana Soltani Zarrin",
                "Dmitry Berenson"
            ],
            "arxiv_id": "2510.14768v1",
            "summary": "Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries.",
            "headline_zh": "提出CADRE强化学习框架，利用神经描述场学习接触感知动态恢复，以应对物体掉落问题。",
            "intro_zh": [
                "核心问题：现实灵巧操作中意外扰动导致物体掉落，需在抓取范围内恢复有利配置。",
                "方法要点：结合神经描述场模块提取隐式接触特征，适应不同物体几何，提升推理能力。",
                "实验或效果：接触特征提高训练效率与收敛性能，实现零样本泛化至未见物体。"
            ],
            "tags_zh": [
                "强化学习",
                "神经描述场",
                "接触感知",
                "动态恢复",
                "零样本泛化",
                "灵巧操作"
            ],
            "_index": 52
        },
        {
            "title": "Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality",
            "authors": [
                "Giuseppe Lorenzo Catalano",
                "Agata Marta Soccini"
            ],
            "arxiv_id": "2510.14765v1",
            "summary": "Space exploration increasingly relies on Virtual Reality for several tasks,\nsuch as mission planning, multidisciplinary scientific analysis, and astronaut\ntraining. A key factor for the reliability of the simulations is having\naccurate 3D representations of planetary terrains. Extraterrestrial heightmaps\nderived from satellite imagery often contain missing values due to acquisition\nand transmission constraints. Mars is among the most studied planets beyond\nEarth, and its extensive terrain datasets make the Martian surface\nreconstruction a valuable task, although many areas remain unmapped. Deep\nlearning algorithms can support void-filling tasks; however, whereas Earth's\ncomprehensive datasets enables the use of conditional methods, such approaches\ncannot be applied to Mars. Current approaches rely on simpler interpolation\ntechniques which, however, often fail to preserve geometric coherence. In this\nwork, we propose a method for reconstructing the surface of Mars based on an\nunconditional diffusion model. Training was conducted on an augmented dataset\nof 12000 Martian heightmaps derived from NASA's HiRISE survey. A\nnon-homogeneous rescaling strategy captures terrain features across multiple\nscales before resizing to a fixed 128x128 model resolution. We compared our\nmethod against established void-filling and inpainting techniques, including\nInverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an\nevaluation set of 1000 samples. Results show that our approach consistently\noutperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)\nand perceptual similarity (29-81% on LPIPS) with the original data.",
            "headline_zh": "提出无条件扩散模型以重建火星表面缺失数据",
            "intro_zh": [
                "火星高度图存在缺失值，传统插值方法难以保持几何一致性",
                "使用无条件扩散模型，在12000张增强高度图上训练，多尺度特征提取",
                "相比现有方法，重建精度和感知相似度显著提升"
            ],
            "tags_zh": [
                "火星表面重建",
                "扩散模型",
                "高度图修复",
                "虚拟现实",
                "深度学习"
            ],
            "_index": 53
        },
        {
            "title": "LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement",
            "authors": [
                "Xu Wu",
                "Zhihui Lai",
                "Xianxu Hou",
                "Jie Zhou",
                "Ya-nan Zhang",
                "Linlin Shen"
            ],
            "arxiv_id": "2510.14753v1",
            "summary": "Low-light image enhancement (LLIE) aims to improve illumination while\npreserving high-quality color and texture. However, existing methods often fail\nto extract reliable feature representations due to severely degraded\npixel-level information under low-light conditions, resulting in poor texture\nrestoration, color inconsistency, and artifact. To address these challenges, we\npropose LightQANet, a novel framework that introduces quantized and adaptive\nfeature learning for low-light enhancement, aiming to achieve consistent and\nrobust image quality across diverse lighting conditions. From the static\nmodeling perspective, we design a Light Quantization Module (LQM) to explicitly\nextract and quantify illumination-related factors from image features. By\nenforcing structured light factor learning, LQM enhances the extraction of\nlight-invariant representations and mitigates feature inconsistency across\nvarying illumination levels. From the dynamic adaptation perspective, we\nintroduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors\ninto learnable prompts to dynamically guide the feature learning process. LAPM\nenables the model to flexibly adapt to complex and continuously changing\nlighting conditions, further improving image enhancement. Extensive experiments\non multiple low-light datasets demonstrate that our method achieves\nstate-of-the-art performance, delivering superior qualitative and quantitative\nresults across various challenging lighting scenarios.",
            "headline_zh": "提出LightQANet，通过量化与自适应特征学习解决低光图像增强问题",
            "intro_zh": [
                "现有方法在低光条件下难以提取可靠特征，导致纹理恢复差和颜色不一致",
                "引入光量化模块和光感知提示模块，分别静态建模和动态适应光照变化",
                "在多个数据集上实现先进性能，提升图像质量与一致性"
            ],
            "tags_zh": [
                "低光图像增强",
                "特征量化",
                "自适应学习",
                "光照建模",
                "图像质量提升"
            ],
            "_index": 54
        },
        {
            "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
            "authors": [
                "Simone Carnemolla",
                "Matteo Pennisi",
                "Sarinda Samarasinghe",
                "Giovanni Bellitto",
                "Simone Palazzo",
                "Daniela Giordano",
                "Mubarak Shah",
                "Concetto Spampinato"
            ],
            "arxiv_id": "2510.14741v1",
            "summary": "Understanding and explaining the behavior of machine learning models is\nessential for building transparent and trustworthy AI systems. We introduce\nDEXTER, a data-free framework that employs diffusion models and large language\nmodels to generate global, textual explanations of visual classifiers. DEXTER\noperates by optimizing text prompts to synthesize class-conditional images that\nstrongly activate a target classifier. These synthetic samples are then used to\nelicit detailed natural language reports that describe class-specific decision\npatterns and biases. Unlike prior work, DEXTER enables natural language\nexplanation about a classifier's decision process without access to training\ndata or ground-truth labels. We demonstrate DEXTER's flexibility across three\ntasks-activation maximization, slice discovery and debiasing, and bias\nexplanation-each illustrating its ability to uncover the internal mechanisms of\nvisual classifiers. Quantitative and qualitative evaluations, including a user\nstudy, show that DEXTER produces accurate, interpretable outputs. Experiments\non ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms\nexisting approaches in global model explanation and class-level bias reporting.\nCode is available at https://github.com/perceivelab/dexter.",
            "headline_zh": "提出DEXTER框架，利用扩散模型和语言模型生成视觉分类器的全局文本解释。",
            "intro_zh": [
                "核心问题：无需训练数据或真实标签，解释视觉分类器的决策过程。",
                "方法要点：优化文本提示合成类条件图像，激活分类器并生成语言报告。",
                "实验或效果：在ImageNet等数据集上优于现有方法，提供准确可解释输出。"
            ],
            "tags_zh": [
                "视觉模型解释",
                "扩散模型",
                "语言模型",
                "全局解释",
                "数据自由框架",
                "分类器偏见分析"
            ],
            "_index": 55
        },
        {
            "title": "Free-Grained Hierarchical Recognition",
            "authors": [
                "Seulki Park",
                "Zilin Wang",
                "Stella X. Yu"
            ],
            "arxiv_id": "2510.14737v1",
            "summary": "Hierarchical image classification predicts labels across a semantic taxonomy,\nbut existing methods typically assume complete, fine-grained annotations, an\nassumption rarely met in practice. Real-world supervision varies in\ngranularity, influenced by image quality, annotator expertise, and task\ndemands; a distant bird may be labeled Bird, while a close-up reveals Bald\neagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet\nand structured into cognitively inspired basic, subordinate, and fine-grained\nlevels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,\nmixed-granularity labels reflecting human annotation behavior. We propose\nfree-grain learning, with heterogeneous supervision across instances. We\ndevelop methods that enhance semantic guidance via pseudo-attributes from\nvision-language models and visual guidance via semi-supervised learning. These,\nalong with strong baselines, substantially improve performance under mixed\nsupervision. Together, our benchmark and methods advance hierarchical\nclassification under real-world constraints.",
            "headline_zh": "提出自由粒度学习以解决混合粒度监督下的层次图像分类问题",
            "intro_zh": [
                "核心问题：现实世界图像标注粒度不一，现有方法依赖完整细粒度标注，难以实用。",
                "方法要点：利用CLIP模拟混合粒度标签，结合伪属性和半监督学习增强语义与视觉引导。",
                "实验或效果：在ImageNet-F基准上，方法显著提升混合监督下的分类性能。"
            ],
            "tags_zh": [
                "层次图像分类",
                "混合粒度监督",
                "自由粒度学习",
                "视觉语言模型",
                "半监督学习",
                "ImageNet-F基准"
            ],
            "_index": 56
        },
        {
            "title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection",
            "authors": [
                "Dingzhou Xie",
                "Rushi Lan",
                "Cheng Pang",
                "Enhao Ning",
                "Jiahao Zeng",
                "Wei Zheng"
            ],
            "arxiv_id": "2510.14726v1",
            "summary": "Recent object detection methods have made remarkable progress by leveraging\nattention mechanisms to improve feature discriminability. However, most\nexisting approaches are confined to refining single-layer or fusing dual-layer\nfeatures, overlooking the rich inter-layer dependencies across multi-scale\nrepresentations. This limits their ability to capture comprehensive contextual\ninformation essential for detecting objects with large scale variations. In\nthis paper, we propose a novel Cross-Layer Feature Self-Attention Module\n(CFSAM), which holistically models both local and global dependencies within\nmulti-scale feature maps. CFSAM consists of three key components: a\nconvolutional local feature extractor, a Transformer-based global modeling unit\nthat efficiently captures cross-layer interactions, and a feature fusion\nmechanism to restore and enhance the original representations. When integrated\ninto the SSD300 framework, CFSAM significantly boosts detection performance,\nachieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO\n(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the\nmodule accelerates convergence during training without introducing substantial\ncomputational overhead. Our work highlights the importance of explicit\ncross-layer attention modeling in advancing multi-scale object detection.",
            "headline_zh": "提出跨层特征自注意力模块以提升多尺度目标检测性能",
            "intro_zh": [
                "现有方法忽视多尺度特征间的跨层依赖，限制检测大尺度变化物体的能力",
                "CFSAM结合卷积局部特征提取、Transformer全局建模和特征融合，增强多尺度特征表示",
                "集成SSD300框架，在PASCAL VOC和COCO数据集上显著提升mAP，并加速训练收敛"
            ],
            "tags_zh": [
                "多尺度目标检测",
                "跨层注意力",
                "特征融合",
                "自注意力机制",
                "Transformer模型"
            ],
            "_index": 57
        },
        {
            "title": "Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models",
            "authors": [
                "Tingyu Lin",
                "Armin Dadras",
                "Florian Kleber",
                "Robert Sablatnig"
            ],
            "arxiv_id": "2510.14713v1",
            "summary": "Camera movement conveys spatial and narrative information essential for\nunderstanding video content. While recent camera movement classification (CMC)\nmethods perform well on modern datasets, their generalization to historical\nfootage remains unexplored. This paper presents the first systematic evaluation\nof deep video CMC models on archival film material. We summarize representative\nmethods and datasets, highlighting differences in model design and label\ndefinitions. Five standard video classification models are assessed on the\nHISTORIAN dataset, which includes expert-annotated World War II footage. The\nbest-performing model, Video Swin Transformer, achieves 80.25% accuracy,\nshowing strong convergence despite limited training data. Our findings\nhighlight the challenges and potential of adapting existing models to\nlow-quality video and motivate future work combining diverse input modalities\nand temporal architectures.",
            "headline_zh": "评估深度视频模型在历史影像中的相机运动分类性能",
            "intro_zh": [
                "核心问题：现有相机运动分类方法在现代数据集表现良好，但泛化到历史影像未知。",
                "方法要点：系统评估五种标准视频分类模型，使用Video Swin Transformer等。",
                "实验或效果：在HISTORIAN数据集上，最佳模型准确率达80.25%，显示强收敛性。"
            ],
            "tags_zh": [
                "相机运动分类",
                "历史影像分析",
                "视频分类模型",
                "Video Swin Transformer",
                "HISTORIAN数据集"
            ],
            "_index": 58
        },
        {
            "title": "Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery",
            "authors": [
                "Caleb Robinson",
                "Kimberly T. Goetz",
                "Christin B. Khan",
                "Meredith Sackett",
                "Kathleen Leonard",
                "Rahul Dodhia",
                "Juan M. Lavista Ferres"
            ],
            "arxiv_id": "2510.14709v1",
            "summary": "Effective monitoring of whale populations is critical for conservation, but\ntraditional survey methods are expensive and difficult to scale. While prior\nwork has shown that whales can be identified in very high-resolution (VHR)\nsatellite imagery, large-scale automated detection remains challenging due to a\nlack of annotated imagery, variability in image quality and environmental\nconditions, and the cost of building robust machine learning pipelines over\nmassive remote sensing archives. We present a semi-automated approach for\nsurfacing possible whale detections in VHR imagery using a statistical anomaly\ndetection method that flags spatial outliers, i.e. \"interesting points\". We\npair this detector with a web-based labeling interface designed to enable\nexperts to quickly annotate the interesting points. We evaluate our system on\nthree benchmark scenes with known whale annotations and achieve recalls of\n90.3% to 96.4%, while reducing the area requiring expert inspection by up to\n99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method\ndoes not rely on labeled training data and offers a scalable first step toward\nfuture machine-assisted marine mammal monitoring from space. We have open\nsourced this pipeline at https://github.com/microsoft/whales.",
            "headline_zh": "提出半自动化方法以解决高分辨率卫星图像中鲸鱼检测的规模化难题",
            "intro_zh": [
                "核心问题：传统鲸鱼监测方法成本高、难以扩展，自动化检测面临标注数据不足和环境多变挑战。",
                "方法要点：使用统计异常检测识别空间异常点，结合专家标注界面快速筛选可能鲸鱼。",
                "实验或效果：在三个基准场景中，召回率达90.3%-96.4%，专家检查面积减少高达99.8%。"
            ],
            "tags_zh": [
                "鲸鱼检测",
                "高分辨率卫星图像",
                "异常检测",
                "半自动化方法",
                "海洋哺乳动物监测"
            ],
            "_index": 59
        },
        {
            "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
            "authors": [
                "Seungjoo Shin",
                "Jaesik Park",
                "Sunghyun Cho"
            ],
            "arxiv_id": "2510.14705v1",
            "summary": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently\nachieved considerable success in minimizing storage overhead for 3D Gaussians\nwhile preserving high rendering quality. Despite the impressive storage\nreduction, the lack of learned priors restricts further advances in the\nrate-distortion trade-off for 3DGS compression tasks. To address this, we\nintroduce a novel 3DGS compression framework that leverages the powerful\nrepresentational capacity of learned image priors to recover\ncompression-induced quality degradation. Built upon initially compressed\nGaussians, our restoration network effectively models the compression artifacts\nin the image space between degraded and original Gaussians. To enhance the\nrate-distortion performance, we provide coarse rendering residuals into the\nrestoration network as side information. By leveraging the supervision of\nrestored images, the compressed Gaussians are refined, resulting in a highly\ncompact representation with enhanced rendering performance. Our framework is\ndesigned to be compatible with existing Gaussian compression methods, making it\nbroadly applicable across different baselines. Extensive experiments validate\nthe effectiveness of our framework, demonstrating superior rate-distortion\nperformance and outperforming the rendering quality of state-of-the-art 3DGS\ncompression methods while requiring substantially less storage.",
            "headline_zh": "提出基于学习图像先验的3D高斯压缩框架，以提升率失真性能。",
            "intro_zh": [
                "核心问题：现有3D高斯压缩方法缺乏学习先验，限制率失真权衡的进一步优化。",
                "方法要点：利用学习图像先验和渲染残差，通过恢复网络减少压缩伪影并精炼高斯表示。",
                "实验或效果：在广泛实验中，框架实现优越率失真性能，超越现有方法且存储需求更低。"
            ],
            "tags_zh": [
                "3D高斯压缩",
                "学习图像先验",
                "率失真优化",
                "压缩伪影恢复",
                "渲染质量提升"
            ],
            "_index": 60
        },
        {
            "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks",
            "authors": [
                "Steffen Hagedorn",
                "Luka Donkov",
                "Aron Distelzweig",
                "Alexandru P. Condurache"
            ],
            "arxiv_id": "2510.14677v1",
            "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.",
            "headline_zh": "集成SMART智能体以解决nuPlan中基于规则交通代理的评估偏差问题",
            "intro_zh": [
                "基于规则的交通代理在闭环模拟中行为简单，隐藏规划器缺陷并导致排名偏差",
                "将学习型交通代理模型SMART集成到nuPlan中，实现更真实的模拟环境",
                "实验显示IDM模拟高估性能，SMART下多数规划器得分下降但交互能力提升"
            ],
            "tags_zh": [
                "交通代理模拟",
                "闭环规划评估",
                "学习型代理",
                "nuPlan基准",
                "模拟到现实差距"
            ],
            "_index": 61
        },
        {
            "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning",
            "authors": [
                "Jinglei Zhang",
                "Yuanfan Guo",
                "Rolandos Alexandros Potamias",
                "Jiankang Deng",
                "Hang Xu",
                "Chao Ma"
            ],
            "arxiv_id": "2510.14672v1",
            "summary": "In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io",
            "headline_zh": "提出VTimeCoT框架以解决视频时序定位与推理问题",
            "intro_zh": [
                "多模态大语言模型在视频时序定位与推理方面存在不足",
                "引入进度条工具和视觉时序链式思维过程，无需训练",
                "在Qwen2VL-7B和GPT4o基准上性能显著提升，推理过程可解释"
            ],
            "tags_zh": [
                "视频时序定位",
                "多模态推理",
                "进度条工具",
                "链式思维",
                "视频问答",
                "零训练框架"
            ],
            "_index": 62
        },
        {
            "title": "WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging",
            "authors": [
                "Md. Abdur Rahman",
                "Mohaimenul Azam Khan Raiaan",
                "Sami Azam",
                "Asif Karim",
                "Jemima Beissbarth",
                "Amanda Leach"
            ],
            "arxiv_id": "2510.14668v1",
            "summary": "Knowledge distillation (KD) has traditionally relied on a static\nteacher-student framework, where a large, well-trained teacher transfers\nknowledge to a single student model. However, these approaches often suffer\nfrom knowledge degradation, inefficient supervision, and reliance on either a\nvery strong teacher model or large labeled datasets, which limits their\neffectiveness in real-world, limited-data scenarios. To address these, we\npresent the first-ever Weakly-supervised Chain-based KD network (WeCKD) that\nredefines knowledge transfer through a structured sequence of interconnected\nmodels. Unlike conventional KD, it forms a progressive distillation chain,\nwhere each model not only learns from its predecessor but also refines the\nknowledge before passing it forward. This structured knowledge transfer further\nenhances feature learning, reduces data dependency, and mitigates the\nlimitations of one-step KD. Each model in the distillation chain is trained on\nonly a fraction of the dataset and demonstrates that effective learning can be\nachieved with minimal supervision. Extensive evaluations across four otoscopic\nimaging datasets demonstrate that it not only matches but in many cases\nsurpasses the performance of existing supervised methods. Experimental results\non two other datasets further underscore its generalization across diverse\nmedical imaging modalities, including microscopic and magnetic resonance\nimaging. Furthermore, our evaluations resulted in cumulative accuracy gains of\nup to +23% over a single backbone trained on the same limited data, which\nhighlights its potential for real-world adoption.",
            "headline_zh": "提出弱监督链式蒸馏网络以解决有限数据下医学图像分析问题",
            "intro_zh": [
                "传统知识蒸馏依赖强教师或大标注数据，易导致知识退化与效率低下",
                "采用链式结构，模型逐级学习并精炼知识，减少数据依赖并增强特征学习",
                "在多个医学图像数据集上验证，性能超越监督方法，准确率提升高达23%"
            ],
            "tags_zh": [
                "知识蒸馏",
                "弱监督学习",
                "医学图像分析",
                "链式结构",
                "多模态成像"
            ],
            "_index": 63
        },
        {
            "title": "EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)",
            "authors": [
                "Weikang Yu",
                "Vincent Nwazelibe",
                "Xianping Ma",
                "Xiaokang Zhang",
                "Richard Gloaguen",
                "Xiao Xiang Zhu",
                "Pedram Ghamisi"
            ],
            "arxiv_id": "2510.14661v1",
            "summary": "Mining activities are essential for industrial and economic development, but\nremain a leading source of environmental degradation, contributing to\ndeforestation, soil erosion, and water contamination. Sustainable resource\nmanagement and environmental governance require consistent, long-term\nmonitoring of mining-induced land surface changes, yet existing datasets are\noften limited in temporal depth or geographic scope. To address this gap, we\npresent EuroMineNet, the first comprehensive multitemporal benchmark for mining\nfootprint mapping and monitoring based on Sentinel-2 multispectral imagery.\nSpanning 133 mining sites across the European Union, EuroMineNet provides\nannual observations and expert-verified annotations from 2015 to 2024, enabling\nGeoAI-based models to analyze environmental dynamics at a continental scale. It\nsupports two sustainability-driven tasks: (1) multitemporal mining footprint\nmapping for consistent annual land-use delineation, evaluated with a novel\nChange-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change\ndetection to capture both gradual and abrupt surface transformations.\nBenchmarking 20 state-of-the-art deep learning models reveals that while GeoAI\nmethods effectively identify long-term environmental changes, challenges remain\nin detecting short-term dynamics critical for timely mitigation. By advancing\ntemporally consistent and explainable mining monitoring, EuroMineNet\ncontributes to sustainable land-use management, environmental resilience, and\nthe broader goal of applying GeoAI for social and environmental good. We\nrelease the codes and datasets by aligning with FAIR and the open science\nparadigm at https://github.com/EricYu97/EuroMineNet.",
            "headline_zh": "提出EuroMineNet基准以解决欧盟采矿足迹长期监测问题",
            "intro_zh": [
                "采矿活动导致环境退化，现有数据集时空覆盖不足",
                "基于Sentinel-2影像提供多时相采矿足迹标注，支持GeoAI模型分析",
                "基准20种深度学习模型，揭示长期变化检测有效但短期动态挑战未知"
            ],
            "tags_zh": [
                "多时相遥感",
                "采矿足迹监测",
                "GeoAI基准",
                "环境变化检测",
                "Sentinel-2影像",
                "可持续发展"
            ],
            "_index": 64
        },
        {
            "title": "Decorrelation Speeds Up Vision Transformers",
            "authors": [
                "Kieran Carrigg",
                "Rob van Gastel",
                "Melda Yeghaian",
                "Sander Dalm",
                "Faysal Boughorbel",
                "Marcel van Gerven"
            ],
            "arxiv_id": "2510.14657v1",
            "summary": "Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields\nstrong performance in low-label regimes but comes with substantial\ncomputational costs, making it impractical in time- and resource-constrained\nindustrial settings. We address this by integrating Decorrelated\nBackpropagation (DBP) into MAE pre-training, an optimization method that\niteratively reduces input correlations at each layer to accelerate convergence.\nApplied selectively to the encoder, DBP achieves faster pre-training without\nloss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE\nreduces wall-clock time to baseline performance by 21.1%, lowers carbon\nemissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe\nsimilar gains when pre-training and fine-tuning on proprietary industrial data,\nconfirming the method's applicability in real-world scenarios. These results\ndemonstrate that DBP can reduce training time and energy use while improving\ndownstream performance for large-scale ViT pre-training.",
            "headline_zh": "提出去相关反向传播以加速视觉变换器预训练，降低计算成本",
            "intro_zh": [
                "MAE预训练ViT在低标签场景性能强，但计算成本高，工业应用受限",
                "集成DBP方法，迭代减少层间输入相关性，选择性应用于编码器加速收敛",
                "实验显示预训练时间减少21.1%，碳排放降低21.4%，分割mIoU提升1.1点"
            ],
            "tags_zh": [
                "视觉变换器",
                "去相关反向传播",
                "掩码自编码器",
                "预训练加速",
                "图像分割"
            ],
            "_index": 65
        },
        {
            "title": "Requirement Identification for Traffic Simulations in Driving Simulators",
            "authors": [
                "Sven Tarlowski",
                "Lutz Eckstein"
            ],
            "arxiv_id": "2510.14653v1",
            "summary": "This paper addresses the challenge of ensuring realistic traffic conditions\nby proposing a methodology that systematically identifies traffic simulation\nrequirements. Using a structured approach based on sub-goals in each study\nphase, specific technical needs are derived for microscopic levels, agent\nmodels, and visual representation. The methodology aims to maintain a high\ndegree of fidelity, enhancing both the validity of experimental outcomes and\nparticipant engagement. By providing a clear link between study objectives and\ntraffic simulation design, this approach supports robust automotive development\nand testing.",
            "headline_zh": "提出系统识别交通仿真需求的方法论，以提升驾驶模拟器中交通条件的真实性。",
            "intro_zh": [
                "核心问题：确保驾驶模拟器中交通条件的真实性，以支持有效实验。",
                "方法要点：基于研究阶段子目标，系统识别微观层面、代理模型和视觉表示的需求。",
                "实验或效果：增强实验结果的效度和参与者参与度，促进汽车开发与测试。"
            ],
            "tags_zh": [
                "交通仿真",
                "驾驶模拟器",
                "需求识别",
                "微观仿真",
                "代理模型",
                "视觉表示"
            ],
            "_index": 66
        },
        {
            "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
            "authors": [
                "Xinyao Liao",
                "Xianfang Zeng",
                "Ziye Song",
                "Zhoujie Fu",
                "Gang Yu",
                "Guosheng Lin"
            ],
            "arxiv_id": "2510.14648v1",
            "summary": "Despite the rapid progress of instruction-based image editing, its extension\nto video remains underexplored, primarily due to the prohibitive cost and\ncomplexity of constructing large-scale paired video editing datasets. To\naddress this challenge, we introduce a low-cost pretraining strategy for\ninstruction-based video editing that leverages in-context learning from\nunpaired video clips. We show that pretraining a foundation video generation\nmodel with this strategy endows it with general editing capabilities, such as\nadding, replacing, or deleting operations, according to input editing\ninstructions. The pretrained model can then be efficiently refined with a small\namount of high-quality paired editing data. Built upon HunyuanVideoT2V, our\nframework first pretrains on approximately 1M real video clips to learn basic\nediting concepts, and subsequently fine-tunes on fewer than 150k curated\nediting pairs to extend more editing tasks and improve the editing quality.\nComparative experiments show that our method surpasses existing\ninstruction-based video editing approaches in both instruction alignment and\nvisual fidelity, achieving a 12\\% improvement in editing instruction following\nand a 15\\% improvement in editing quality.",
            "headline_zh": "提出基于非配对视频剪辑的上下文学习策略，以低成本实现指令视频编辑。",
            "intro_zh": [
                "核心问题：视频编辑数据集构建成本高，阻碍指令视频编辑发展。",
                "方法要点：预训练使用非配对视频学习编辑概念，再微调少量配对数据。",
                "实验或效果：在指令对齐和视觉保真度上优于现有方法，提升12%和15%。"
            ],
            "tags_zh": [
                "指令视频编辑",
                "上下文学习",
                "非配对数据",
                "预训练策略",
                "视频生成模型"
            ],
            "_index": 67
        },
        {
            "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation",
            "authors": [
                "Jialei Huang",
                "Yang Ye",
                "Yuanqing Gong",
                "Xuezhou Zhu",
                "Yang Gao",
                "Kaifeng Zhang"
            ],
            "arxiv_id": "2510.14647v1",
            "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage.",
            "headline_zh": "提出SaTA框架，通过空间锚定触觉特征提升灵巧操作精度",
            "intro_zh": [
                "现有视觉触觉学习方法在亚毫米精度任务中表现不佳，缺乏触觉信号与手部运动的空间关联利用",
                "SaTA通过前向运动学将触觉特征锚定于手部坐标系，实现无模型几何推理",
                "在USB-C插接等任务中，成功率提升30%，完成时间减少27%"
            ],
            "tags_zh": [
                "灵巧操作",
                "触觉感知",
                "空间锚定",
                "几何推理",
                "强化学习"
            ],
            "_index": 68
        },
        {
            "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation",
            "authors": [
                "Lara Brudermüller",
                "Brandon Hung",
                "Xinghao Zhu",
                "Jiuguang Wang",
                "Nick Hawes",
                "Preston Culbertson",
                "Simon Le Cleac'h"
            ],
            "arxiv_id": "2510.14643v1",
            "summary": "We present a generative predictive control (GPC) framework that amortizes\nsampling-based Model Predictive Control (SPC) by bootstrapping it with\nconditional flow-matching models trained on SPC control sequences collected in\nsimulation. Unlike prior work relying on iterative refinement or gradient-based\nsolvers, we show that meaningful proposal distributions can be learned directly\nfrom noisy SPC data, enabling more efficient and informed sampling during\nonline planning. We further demonstrate, for the first time, the application of\nthis approach to real-world contact-rich loco-manipulation with a quadruped\nrobot. Extensive experiments in simulation and on hardware show that our method\nimproves sample efficiency, reduces planning horizon requirements, and\ngeneralizes robustly across task variations.",
            "headline_zh": "提出生成预测控制框架，通过引导采样优化接触丰富的机器人操作",
            "intro_zh": [
                "核心问题：采样模型预测控制在接触丰富操作中效率低、规划需求高",
                "方法要点：使用条件流匹配模型从模拟数据学习提议分布，提升在线采样",
                "实验或效果：在仿真和硬件中提高样本效率、减少规划需求，并泛化任务变化"
            ],
            "tags_zh": [
                "生成预测控制",
                "采样模型预测控制",
                "条件流匹配",
                "机器人操作",
                "接触丰富任务",
                "样本效率"
            ],
            "_index": 69
        },
        {
            "title": "SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation",
            "authors": [
                "Jihyun Yu",
                "Yoojin Oh",
                "Wonho Bae",
                "Mingyu Kim",
                "Junhyug Noh"
            ],
            "arxiv_id": "2510.14634v1",
            "summary": "Test-time adaptation (TTA) aims to correct performance degradation of deep\nmodels under distribution shifts by updating models or inputs using unlabeled\ntest data. Input-only diffusion-based TTA methods improve robustness for\nclassification to corruptions but rely on gradient guidance, limiting\nexploration and generalization across distortion types. We propose SteeringTTA,\nan inference-only framework that adapts Feynman-Kac steering to guide\ndiffusion-based input adaptation for classification with rewards driven by\npseudo-label. SteeringTTA maintains multiple particle trajectories, steered by\na combination of cumulative top-K probabilities and an entropy schedule, to\nbalance exploration and confidence. On ImageNet-C, SteeringTTA consistently\noutperforms the baseline without any model updates or source data.",
            "headline_zh": "提出SteeringTTA以解决分布偏移下测试时适应问题，通过引导扩散轨迹提升分类鲁棒性。",
            "intro_zh": [
                "核心问题：测试时适应方法在分布偏移下性能下降，现有方法依赖梯度指导，限制探索和泛化。",
                "方法要点：采用Feynman-Kac引导，结合伪标签奖励和多粒子轨迹，平衡探索与置信度。",
                "实验或效果：在ImageNet-C上优于基线，无需模型更新或源数据，提升鲁棒性。"
            ],
            "tags_zh": [
                "测试时适应",
                "扩散模型",
                "分布偏移",
                "分类鲁棒性",
                "Feynman-Kac引导",
                "伪标签奖励"
            ],
            "_index": 70
        },
        {
            "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation",
            "authors": [
                "Ming Gui",
                "Johannes Schusterbauer",
                "Timy Phan",
                "Felix Krause",
                "Josh Susskind",
                "Miguel Angel Bautista",
                "Björn Ommer"
            ],
            "arxiv_id": "2510.14630v1",
            "summary": "We introduce Representation Tokenizer (RepTok), a generative modeling\nframework that represents an image using a single continuous latent token\nobtained from self-supervised vision transformers. Building on a pre-trained\nSSL encoder, we fine-tune only the semantic token embedding and pair it with a\ngenerative decoder trained jointly using a standard flow matching objective.\nThis adaptation enriches the token with low-level, reconstruction-relevant\ndetails, enabling faithful image reconstruction. To preserve the favorable\ngeometry of the original SSL space, we add a cosine-similarity loss that\nregularizes the adapted token, ensuring the latent space remains smooth and\nsuitable for generation. Our single-token formulation resolves spatial\nredundancies of 2D latent spaces and significantly reduces training costs.\nDespite its simplicity and efficiency, RepTok achieves competitive results on\nclass-conditional ImageNet generation and naturally extends to text-to-image\nsynthesis, reaching competitive zero-shot performance on MS-COCO under\nextremely limited training budgets. Our findings highlight the potential of\nfine-tuned SSL representations as compact and effective latent spaces for\nefficient generative modeling.",
            "headline_zh": "提出RepTok框架，利用自监督表示作为紧凑潜空间，实现高效图像生成。",
            "intro_zh": [
                "核心问题：传统2D潜空间存在空间冗余，训练成本高，影响生成效率。",
                "方法要点：基于预训练SSL编码器，微调语义令牌嵌入，结合流匹配目标训练解码器。",
                "实验效果：在ImageNet类条件生成和MS-COCO文本到图像合成中达到竞争性结果。"
            ],
            "tags_zh": [
                "自监督学习",
                "图像生成",
                "潜空间优化",
                "流匹配",
                "令牌化表示"
            ],
            "_index": 71
        },
        {
            "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement",
            "authors": [
                "Yao Zhong",
                "Hanzhi Chen",
                "Simon Schaefer",
                "Anran Zhang",
                "Stefan Leutenegger"
            ],
            "arxiv_id": "2510.14627v1",
            "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.",
            "headline_zh": "提出GOPLA框架，通过合成增强人类演示学习通用对象放置，以解决机器人家庭组织任务。",
            "intro_zh": [
                "核心问题：机器人对象放置需兼顾语义偏好和几何可行性，面临数据稀缺挑战。",
                "方法要点：使用多模态大语言模型生成结构化计划，结合扩散规划器生成放置位姿。",
                "实验或效果：在真实场景中，放置成功率比次优方法提高30.04个百分点。"
            ],
            "tags_zh": [
                "对象放置学习",
                "合成数据增强",
                "多模态大语言模型",
                "扩散规划器",
                "机器人辅助"
            ],
            "_index": 72
        },
        {
            "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference",
            "authors": [
                "Natan Bagrov",
                "Eugene Khvedchenia",
                "Borys Tymchenko",
                "Shay Aharon",
                "Lior Kadoch",
                "Tomer Keren",
                "Ofri Masad",
                "Yonatan Geifman",
                "Ran Zilberstein",
                "Tuomas Rintamaki",
                "Matthieu Le",
                "Andrew Tao"
            ],
            "arxiv_id": "2510.14624v1",
            "summary": "Vision-language models (VLMs) have recently expanded from static image\nunderstanding to video reasoning, but their scalability is fundamentally\nlimited by the quadratic cost of processing dense frame sequences. Long videos\noften exceed the token budget of modern language models, leading to severe\ncontext limitations and latency issues. We introduce Efficient Video Sampling\n(EVS), a simple, plug-and-play method for reducing token redundancy in videos\nby identifying and pruning temporally static patches -- spatial regions that\nremain unchanged across consecutive frames. EVS preserves positional identity,\nrequires no architectural changes or retraining. We show that EVS substantially\nreduces token count while maintaining semantic fidelity, enabling faster\ninference and longer input sequences. Applied at inference time, EVS reduces\nlarge language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal\naccuracy loss. When combined with an uptraining phase using stochastic pruning\nrates, EVS yields models that are robust to varying compression levels and\nretain full performance under aggressive pruning. Extensive experiments\ndemonstrate that EVS consistently improves efficiency-accuracy trade-offs,\nunlocking scalable video-language understanding without sacrificing quality.",
            "headline_zh": "提出高效视频采样方法，通过剪枝时间冗余令牌加速视觉语言模型推理",
            "intro_zh": [
                "核心问题：视频处理中密集帧序列导致二次计算成本，限制模型可扩展性和延迟",
                "方法要点：识别并剪枝时间静态补丁，无需架构修改或重训练，保持位置身份",
                "实验或效果：推理时应用可减少令牌数，TTFT降低达4倍，精度损失最小"
            ],
            "tags_zh": [
                "视频语言模型",
                "令牌剪枝",
                "推理加速",
                "时间冗余",
                "高效采样"
            ],
            "_index": 73
        },
        {
            "title": "Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding",
            "authors": [
                "Ning Ding",
                "Keisuke Fujii",
                "Toru Tamaki"
            ],
            "arxiv_id": "2510.14617v1",
            "summary": "Tactical understanding in badminton involves interpreting not only individual\nactions but also how tactics are dynamically executed over time. In this paper,\nwe propose \\textbf{Shot2Tactic-Caption}, a novel framework for semantic and\ntemporal multi-scale video captioning in badminton, capable of generating\nshot-level captions that describe individual actions and tactic-level captions\nthat capture how these actions unfold over time within a tactical execution. We\nalso introduce the Shot2Tactic-Caption Dataset, the first badminton captioning\ndataset containing 5,494 shot captions and 544 tactic captions.\nShot2Tactic-Caption adopts a dual-branch design, with both branches including a\nvisual encoder, a spatio-temporal Transformer encoder, and a Transformer-based\ndecoder to generate shot and tactic captions. To support tactic captioning, we\nadditionally introduce a Tactic Unit Detector that identifies valid tactic\nunits, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic\ncaptioning, we further incorporate a shot-wise prompt-guided mechanism, where\nthe predicted tactic type and state are embedded as prompts and injected into\nthe decoder via cross-attention. The shot-wise prompt-guided mechanism enables\nour system not only to describe successfully executed tactics but also to\ncapture tactical executions that are temporarily interrupted and later resumed.\nExperimental results demonstrate the effectiveness of our framework in\ngenerating both shot and tactic captions. Ablation studies show that the\nResNet50-based spatio-temporal encoder outperforms other variants, and that\nshot-wise prompt structuring leads to more coherent and accurate tactic\ncaptioning.",
            "headline_zh": "提出Shot2Tactic-Caption框架，用于羽毛球视频的多尺度战术理解与描述",
            "intro_zh": [
                "核心问题：羽毛球战术理解需描述个体动作和动态战术执行过程。",
                "方法要点：采用双分支设计，结合视觉编码器和Transformer，引入战术单元检测器与提示机制。",
                "实验或效果：在自建数据集上验证有效性，提示机制提升战术描述连贯性和准确性。"
            ],
            "tags_zh": [
                "多尺度视频描述",
                "羽毛球战术理解",
                "Transformer架构",
                "战术单元检测",
                "提示引导机制"
            ],
            "_index": 74
        },
        {
            "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models",
            "authors": [
                "Edward Sandra",
                "Lander Vanroye",
                "Dries Dirckx",
                "Ruben Cartuyvels",
                "Jan Swevers",
                "Wilm Decré"
            ],
            "arxiv_id": "2510.14615v1",
            "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods.",
            "headline_zh": "提出上下文感知运动规划扩散模型以解决机器人运动规划在未知环境中的泛化问题",
            "intro_zh": [
                "传统运动规划方法在高维状态空间和复杂环境中扩展性差",
                "使用无分类器去噪扩散模型，通过注意力机制整合传感器无关上下文信息",
                "在7自由度机械臂上验证，泛化至未见环境，生成高质量多模态轨迹，速度远超现有方法"
            ],
            "tags_zh": [
                "运动规划",
                "扩散模型",
                "上下文条件",
                "多模态轨迹",
                "机器人操纵",
                "泛化能力"
            ],
            "_index": 75
        },
        {
            "title": "Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning",
            "authors": [
                "Gabriel Fischer Abati",
                "João Carlos Virgolino Soares",
                "Giulio Turrisi",
                "Victor Barasuol",
                "Claudio Semini"
            ],
            "arxiv_id": "2510.14612v1",
            "summary": "This paper presents a novel approach for representing proprioceptive\ntime-series data from quadruped robots as structured two-dimensional images,\nenabling the use of convolutional neural networks for learning\nlocomotion-related tasks. The proposed method encodes temporal dynamics from\nmultiple proprioceptive signals, such as joint positions, IMU readings, and\nfoot velocities, while preserving the robot's morphological structure in the\nspatial arrangement of the image. This transformation captures inter-signal\ncorrelations and gait-dependent patterns, providing a richer feature space than\ndirect time-series processing. We apply this concept in the problem of contact\nestimation, a key capability for stable and adaptive locomotion on diverse\nterrains. Experimental evaluations on both real-world datasets and simulated\nenvironments show that our image-based representation consistently enhances\nprediction accuracy and generalization over conventional sequence-based models,\nunderscoring the potential of cross-modal encoding strategies for robotic state\nlearning. Our method achieves superior performance on the contact dataset,\nimproving contact state accuracy from 87.7% to 94.5% over the recently proposed\nMI-HGNN method, using a 15 times shorter window size.",
            "headline_zh": "提出将四足机器人本体感觉数据编码为图像的方法，以提升接触估计学习性能。",
            "intro_zh": [
                "核心问题：四足机器人在多变地形上的稳定运动需要准确估计接触状态。",
                "方法要点：将多通道本体感觉时间序列数据转换为结构化二维图像，保留机器人形态和动态模式。",
                "实验效果：在真实和模拟数据上，图像表示比序列模型准确率从87.7%提升至94.5%。"
            ],
            "tags_zh": [
                "四足机器人",
                "本体感觉数据",
                "图像表示",
                "卷积神经网络",
                "接触估计",
                "跨模态编码"
            ],
            "_index": 76
        },
        {
            "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering",
            "authors": [
                "Yuyang Hong",
                "Jiaqi Gu",
                "Qi Yang",
                "Lubin Fan",
                "Yue Wu",
                "Ying Wang",
                "Kun Ding",
                "Shiming Xiang",
                "Jieping Ye"
            ],
            "arxiv_id": "2510.14605v1",
            "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF",
            "headline_zh": "提出Wiki-PRF三阶段方法以解决知识视觉问答中查询质量和检索结果相关性问题",
            "intro_zh": [
                "核心问题：知识视觉问答中多模态查询质量差和检索结果相关性低",
                "方法要点：通过处理、检索和过滤三阶段，结合视觉工具和强化学习训练",
                "实验或效果：在E-VQA和InfoSeek数据集上答案质量显著提升，达到SOTA"
            ],
            "tags_zh": [
                "知识视觉问答",
                "多模态检索",
                "强化学习训练",
                "检索增强生成",
                "视觉语言模型"
            ],
            "_index": 77
        },
        {
            "title": "Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering",
            "authors": [
                "Hugo Markoff",
                "Jevgenijs Galaktionovs"
            ],
            "arxiv_id": "2510.14596v1",
            "summary": "Camera traps generate millions of wildlife images, yet many datasets contain\nspecies that are absent from existing classifiers. This work evaluates\nzero-shot approaches for organizing unlabeled wildlife imagery using\nself-supervised vision transformers, developed and tested within the Animal\nDetect platform for camera trap analysis. We compare unsupervised clustering\nmethods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)\ncombined with dimensionality reduction techniques (PCA, UMAP), and we\ndemonstrate continuous 1D similarity ordering via t-SNE projection. On a\n5-species test set with ground truth labels used only for evaluation, DINOv2\nwith UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D\nsorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent\nfor fish across 1,500 images. Based on these findings, we deployed continuous\nsimilarity ordering in production, enabling rapid exploratory analysis and\naccelerating manual annotation workflows for biodiversity monitoring.",
            "headline_zh": "评估零样本方法使用视觉变换器对野生动物图像进行聚类和连续相似性排序，以加速生物多样性监测。",
            "intro_zh": [
                "核心问题：相机陷阱图像中物种未标注，现有分类器无法覆盖，需零样本方法组织图像。",
                "方法要点：比较自监督视觉变换器架构与降维技术，实现无监督聚类和连续相似性排序。",
                "实验或效果：DINOv2结合UMAP和GMM在5物种测试集上达88.6%准确率，排序在哺乳动物和鸟类达88.2%一致性。"
            ],
            "tags_zh": [
                "零样本学习",
                "视觉变换器",
                "无监督聚类",
                "相似性排序",
                "生物多样性监测",
                "相机陷阱图像"
            ],
            "_index": 78
        },
        {
            "title": "Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers",
            "authors": [
                "Hugo Markoff",
                "Jevgenijs Galaktionovs"
            ],
            "arxiv_id": "2510.14594v1",
            "summary": "State-of-the-art animal classification models like SpeciesNet provide\npredictions across thousands of species but use conservative rollup strategies,\nresulting in many animals labeled at high taxonomic levels rather than species.\nWe present a hierarchical re-classification system for the Animal Detect\nplatform that combines SpeciesNet EfficientNetV2-M predictions with CLIP\nembeddings and metric learning to refine high-level taxonomic labels toward\nspecies-level identification. Our five-stage pipeline (high-confidence\nacceptance, bird override, centroid building, triplet-loss metric learning, and\nadaptive cosine-distance scoring) is evaluated on a segment of the LILA BC\nDesert Lion Conservation dataset (4,018 images, 15,031 detections). After\nrecovering 761 bird detections from \"blank\" and \"animal\" labels, we re-classify\n456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving\nspecies-level identification for 64.9 percent",
            "headline_zh": "提出分层重分类系统以提升动物检测平台物种级识别精度",
            "intro_zh": [
                "现有动物分类模型保守汇总导致高分类标签而非物种级识别",
                "结合SpeciesNet预测、CLIP嵌入和度量学习优化标签",
                "在LILA数据集上重分类456个检测，准确率96.5%，64.9%达物种级"
            ],
            "tags_zh": [
                "动物分类",
                "分层重分类",
                "Vision Transformer",
                "度量学习",
                "物种识别"
            ],
            "_index": 79
        },
        {
            "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding",
            "authors": [
                "Zhifei Chen",
                "Tianshuo Xu",
                "Leyi Wu",
                "Luozhou Wang",
                "Dongyu Yan",
                "Zihan You",
                "Wenting Luo",
                "Guo Zhang",
                "Yingcong Chen"
            ],
            "arxiv_id": "2510.14588v1",
            "summary": "Video generation has recently made striking visual progress, but maintaining\ncoherent object motion and interactions remains difficult. We trace two\npractical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)\noften collapse to too few effective tokens after encoding, weakening guidance;\nand (ii) optimizing for appearance and motion in a single head can favor\ntexture over temporal consistency. We present STANCE, an image-to-video\nframework that addresses both issues with two simple components. First, we\nintroduce Instance Cues -- a pixel-aligned control signal that turns sparse,\nuser-editable hints into a dense 2.5D (camera-relative) motion field by\naveraging per-instance flow and augmenting with monocular depth over the\ninstance mask. This reduces depth ambiguity compared to 2D arrow inputs while\nremaining easy to use. Second, we preserve the salience of these cues in token\nspace with Dense RoPE, which tags a small set of motion tokens (anchored on the\nfirst frame) with spatial-addressable rotary embeddings. Paired with joint RGB\n\\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors\nstructure while RGB handles appearance, stabilizing optimization and improving\ntemporal coherence without requiring per-frame trajectory scripts.",
            "headline_zh": "提出STANCE框架，通过稀疏到稠密锚定编码解决视频生成中运动一致性问题",
            "intro_zh": [
                "核心问题：现有视频生成方法在对象运动一致性和交互上存在困难，运动提示编码后有效令牌过少，且外观与运动优化冲突",
                "方法要点：引入实例线索生成稠密2.5D运动场，并使用Dense RoPE在令牌空间保持运动提示显著性",
                "实验或效果：结合RGB与辅助图预测，提升时间一致性，无需逐帧轨迹脚本"
            ],
            "tags_zh": [
                "视频生成",
                "运动一致性",
                "实例线索",
                "Dense RoPE",
                "2.5D运动场",
                "RGB辅助预测"
            ],
            "_index": 80
        },
        {
            "title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning",
            "authors": [
                "Benno Wingender",
                "Nils Dengler",
                "Rohit Menon",
                "Sicong Pan",
                "Maren Bennewitz"
            ],
            "arxiv_id": "2510.14584v1",
            "summary": "To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors.",
            "headline_zh": "提出广义可放置性度量，用于无模型统一拾放推理，处理未知物体和噪声点云。",
            "intro_zh": [
                "核心问题：未知物体在噪声点云下可靠拾放受限，现有方法依赖物体先验或平面假设。",
                "方法要点：从点云提取支撑面，生成多方向放置候选，联合评分稳定性、可抓取性和间隙。",
                "实验或效果：在未见真实物体和非平面支撑上，预测稳定性损失精度与CAD相当。"
            ],
            "tags_zh": [
                "拾放推理",
                "可放置性度量",
                "点云处理",
                "稳定性预测",
                "无模型方法"
            ],
            "_index": 81
        },
        {
            "title": "Talking Points: Describing and Localizing Pixels",
            "authors": [
                "Matan Rusanovsky",
                "Shimon Malnick",
                "Shai Avidan"
            ],
            "arxiv_id": "2510.14583v1",
            "summary": "Vision-language models have achieved remarkable success in cross-modal\nunderstanding. Yet, these models remain limited to object-level or region-level\ngrounding, lacking the capability for pixel-precise keypoint comprehension\nthrough natural language. We introduce a novel framework for pixel level\ngrounding. The framework consists of two complementary components: a Point\nDescriptor that generates rich, contextual descriptions of individual\nkeypoints, and a Point Localizer that regresses precise pixel coordinates from\nthese descriptions. Unlike prior work that relies on templated prompts or\nkeypoint names, our approach produces free-form, coarse-to-fine descriptions\nthat situate keypoints within their visual context. Since there is no available\ndataset to train such a system, we introduce LlamaPointInPart, a carefully\ncurated dataset of 20K+ image-keypoint-description triplets synthesized from\nmultiple vision-language models, capturing multi-scale information from\nscene-level context to visual features around the keypoint. For cross-category\ngeneralization, we optimize the Point Descriptor on AP-10K via GRPO, using the\nfrozen Point Localizer as a reward model to produce descriptions that maximize\nlocalization accuracy. To evaluate our results we establish a new evaluation\nprotocol. Instead of comparing the text description produced by our method to\nthe ground truth, we use the localizer to determine how close is the predicted\npoint generated to the ground truth point. Experiments demonstrate superior\nperformance compared to baseline models on LlamaPointInPart.The bidirectional\nnature of our framework should enable future applications in both\nkeypoint-guided image understanding and language-guided precise localization.\nOur code and dataset are publicly available at\nhttps://github.com/matanr/Talking_Points.",
            "headline_zh": "提出双向框架以解决像素级关键点描述与定位问题",
            "intro_zh": [
                "核心问题：视觉语言模型缺乏像素级关键点理解能力，仅支持对象或区域级定位。",
                "方法要点：结合点描述器和点定位器，生成上下文描述并回归像素坐标。",
                "实验或效果：在LlamaPointInPart数据集上优于基线，建立新评估协议。"
            ],
            "tags_zh": [
                "像素级定位",
                "关键点描述",
                "视觉语言模型",
                "数据集合成",
                "双向框架"
            ],
            "_index": 82
        },
        {
            "title": "CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification",
            "authors": [
                "Dongwook Lee",
                "Sol Han",
                "Jinwhan Kim"
            ],
            "arxiv_id": "2510.14576v1",
            "summary": "This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based\nmulti-branch neural network for vehicle re-identification. The proposed model\naddresses the challenge of learning discriminative and complementary features\nfrom three-dimensional point clouds to distinguish between vehicles. CALM-Net\nemploys a multi-branch architecture that integrates edge convolution, point\nattention, and a curvature embedding that characterizes local surface variation\nin point clouds. By combining these mechanisms, the model learns richer\ngeometric and contextual features that are well suited for the\nre-identification task. Experimental evaluation on the large-scale nuScenes\ndataset demonstrates that CALM-Net achieves a mean re-identification accuracy\nimprovement of approximately 1.97\\% points compared with the strongest baseline\nin our study. The results confirms the effectiveness of incorporating curvature\ninformation into deep learning architectures and highlight the benefit of\nmulti-branch feature learning for LiDAR point cloud-based vehicle\nre-identification.",
            "headline_zh": "提出CALM-Net，通过多分支网络结合曲率信息解决LiDAR点云车辆重识别问题",
            "intro_zh": [
                "核心问题：从三维点云中学习区分性特征以识别不同车辆",
                "方法要点：采用多分支架构，集成边缘卷积、点注意力和曲率嵌入",
                "实验或效果：在nuScenes数据集上，平均重识别准确率提升约1.97%"
            ],
            "tags_zh": [
                "LiDAR点云",
                "车辆重识别",
                "多分支神经网络",
                "曲率嵌入",
                "边缘卷积",
                "点注意力"
            ],
            "_index": 83
        },
        {
            "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU",
            "authors": [
                "Junyi Wu",
                "Jiaming Xu",
                "Jinhao Li",
                "Yongkang Zhou",
                "Jiayi Pan",
                "Xingyang Li",
                "Guohao Dai"
            ],
            "arxiv_id": "2510.14564v1",
            "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction\ntechnique. The traditional 3DGS training pipeline follows three sequential\nsteps: Gaussian densification, Gaussian projection, and color splatting.\nDespite its promising reconstruction quality, this conventional approach\nsuffers from three critical inefficiencies: (1) Skewed density allocation\nduring Gaussian densification, (2) Imbalanced computation workload during\nGaussian projection and (3) Fragmented memory access during color splatting.\n  To tackle the above challenges, we introduce BalanceGS, the algorithm-system\nco-design for efficient training in 3DGS. (1) At the algorithm level, we\npropose heuristic workload-sensitive Gaussian density control to automatically\nbalance point distributions - removing 80% redundant Gaussians in dense regions\nwhile filling gaps in sparse areas. (2) At the system level, we propose\nSimilarity-based Gaussian sampling and merging, which replaces the static\none-to-one thread-pixel mapping with adaptive workload distribution - threads\nnow dynamically process variable numbers of Gaussians based on local cluster\ndensity. (3) At the mapping level, we propose reordering-based memory access\nmapping strategy that restructures RGB storage and enables batch loading in\nshared memory.\n  Extensive experiments demonstrate that compared with 3DGS, our approach\nachieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible\nquality degradation.",
            "headline_zh": "提出BalanceGS以解决3D高斯溅射训练中的效率问题",
            "intro_zh": [
                "传统3DGS训练存在密度分配不均、计算负载不平衡和内存访问碎片化问题",
                "通过算法-系统协同设计，包括启发式密度控制、相似性采样合并和内存重映射策略",
                "实验显示在A100 GPU上训练速度提升1.44倍，质量损失可忽略"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "算法-系统协同设计",
                "GPU训练优化",
                "内存访问优化",
                "计算负载平衡"
            ],
            "_index": 84
        },
        {
            "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video",
            "authors": [
                "Yulin Zhang",
                "Cheng Shi",
                "Yang Wang",
                "Sibei Yang"
            ],
            "arxiv_id": "2510.14560v1",
            "summary": "Envision an AI capable of functioning in human-like settings, moving beyond\nmere observation to actively understand, anticipate, and proactively respond to\nunfolding events. Towards this vision, we focus on the innovative task where,\ngiven ego-streaming video input, an assistant proactively answers diverse,\nevolving questions at the opportune moment, while maintaining synchronized\nperception and reasoning. This task embodies three key properties: (1)\nProactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized\nEfficiency. To evaluate and address these properties, we first introduce\nESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a\nnovel framework designed for their rigorous assessment. Secondly, we propose a\ncomprehensive technical pipeline to enable models to tackle this challenging\ntask. This pipeline comprises: (1) a data engine, (2) a multi-stage training\nstrategy, and (3) a proactive dynamic compression technique. Our proposed model\neffectively addresses these critical properties while outperforming multiple\nbaselines across diverse online and offline benchmarks. Project\nPage:https://zhangyl4.github.io/publications/eyes-wide-open/",
            "headline_zh": "提出Ego Proactive Video-LLM以解决流视频中主动理解与响应问题",
            "intro_zh": [
                "核心问题：在流视频中主动理解、预测并适时响应动态问题，需满足主动一致性、及时响应和同步效率。",
                "方法要点：引入ESTP-Bench评估框架，开发数据引擎、多阶段训练和主动动态压缩技术。",
                "实验或效果：模型在多个基准测试中优于基线，有效处理主动任务。"
            ],
            "tags_zh": [
                "流视频理解",
                "主动视觉语言模型",
                "多阶段训练",
                "动态压缩",
                "基准评估"
            ],
            "_index": 85
        },
        {
            "title": "Consistent text-to-image generation via scene de-contextualization",
            "authors": [
                "Song Tang",
                "Peihao Gong",
                "Kunyu Li",
                "Kai Guo",
                "Boyu Wang",
                "Mao Ye",
                "Jianwei Zhang",
                "Xiatian Zhu"
            ],
            "arxiv_id": "2510.14553v1",
            "summary": "Consistent text-to-image (T2I) generation seeks to produce\nidentity-preserving images of the same subject across diverse scenes, yet it\noften fails due to a phenomenon called identity (ID) shift. Previous methods\nhave tackled this issue, but typically rely on the unrealistic assumption of\nknowing all target scenes in advance. This paper reveals that a key source of\nID shift is the native correlation between subject and scene context, called\nscene contextualization, which arises naturally as T2I models fit the training\ndistribution of vast natural images. We formally prove the near-universality of\nthis scene-ID correlation and derive theoretical bounds on its strength. On\nthis basis, we propose a novel, efficient, training-free prompt embedding\nediting approach, called Scene De-Contextualization (SDeC), that imposes an\ninversion process of T2I's built-in scene contextualization. Specifically, it\nidentifies and suppresses the latent scene-ID correlation within the ID\nprompt's embedding by quantifying the SVD directional stability to adaptively\nre-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene\nuse (one scene per prompt) without requiring prior access to all target scenes.\nThis makes it a highly flexible and general solution well-suited to real-world\napplications where such prior knowledge is often unavailable or varies over\ntime. Experiments demonstrate that SDeC significantly enhances identity\npreservation while maintaining scene diversity.",
            "headline_zh": "提出场景去上下文化方法以解决文本到图像生成中的身份偏移问题",
            "intro_zh": [
                "核心问题：文本到图像生成中身份偏移源于主题与场景上下文的内在相关性",
                "方法要点：基于SVD方向稳定性量化，自适应重加权特征值以抑制场景-身份相关性",
                "实验或效果：显著提升身份保持能力，同时维持场景多样性，无需预知所有目标场景"
            ],
            "tags_zh": [
                "文本到图像生成",
                "身份保持",
                "场景去上下文化",
                "训练自由方法",
                "提示嵌入编辑"
            ],
            "_index": 86
        },
        {
            "title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps",
            "authors": [
                "Matti Pekkanen",
                "Francesco Verdoja",
                "Ville Kyrki"
            ],
            "arxiv_id": "2510.14546v1",
            "summary": "Embeddings from Visual-Language Models are increasingly utilized to represent\nsemantics in robotic maps, offering an open-vocabulary scene understanding that\nsurpasses traditional, limited labels. Embeddings enable on-demand querying by\ncomparing embedded user text prompts to map embeddings via a similarity metric.\nThe key challenge in performing the task indicated in a query is that the robot\nmust determine the parts of the environment relevant to the query.\n  This paper proposes a solution to this challenge. We leverage\nnatural-language synonyms and antonyms associated with the query within the\nembedding space, applying heuristics to estimate the language space relevant to\nthe query, and use that to train a classifier to partition the environment into\nmatches and non-matches. We evaluate our method through extensive experiments,\nquerying both maps and standard image benchmarks. The results demonstrate\nincreased queryability of maps and images. Our querying technique is agnostic\nto the representation and encoder used, and requires limited training.",
            "headline_zh": "提出QuASH方法，利用自然语言启发式查询视觉语言机器人地图。",
            "intro_zh": [
                "核心问题：机器人需确定查询相关的环境部分，以执行任务。",
                "方法要点：利用查询的同义词和反义词，训练分类器分割环境。",
                "实验或效果：评估显示地图和图像查询能力提升，方法通用且训练需求低。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "机器人地图",
                "自然语言查询",
                "嵌入空间",
                "分类器训练",
                "开放词汇理解"
            ],
            "_index": 87
        },
        {
            "title": "Exploring Cross-Modal Flows for Few-Shot Learning",
            "authors": [
                "Ziqi Jiang",
                "Yanghao Wang",
                "Long Chen"
            ],
            "arxiv_id": "2510.14543v1",
            "summary": "Aligning features from different modalities, is one of the most fundamental\nchallenges for cross-modal tasks. Although pre-trained vision-language models\ncan achieve a general alignment between image and text, they often require\nparameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT\nmethods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively\nfine-tune a subset of parameters, which can slightly adjust either visual or\ntextual features, and avoid overfitting. In this paper, we are the first to\nhighlight that all existing PEFT methods perform one-step adjustment. It is\ninsufficient for complex (or difficult) datasets, where features of different\nmodalities are highly entangled. To this end, we propose the first\nmodel-agnostic multi-step adjustment approach by learning a cross-modal\nvelocity field: Flow Matching Alignment (FMA). Specifically, to ensure the\ncorrespondence between categories during training, we first utilize a fixed\ncoupling strategy. Then, we propose a noise augmentation strategy to alleviate\nthe data scarcity issue. Finally, we design an early-stopping solver, which\nterminates the transformation process earlier, improving both efficiency and\naccuracy. Compared with one-step PEFT methods, FMA has the multi-step\nrectification ability to achieve more precise and robust alignment. Extensive\nresults have demonstrated that FMA can consistently yield significant\nperformance gains across various benchmarks and backbones, particularly on\nchallenging datasets.",
            "headline_zh": "提出Flow Matching Alignment以解决复杂数据集中跨模态特征对齐问题",
            "intro_zh": [
                "现有参数高效微调方法仅单步调整，难以处理高度纠缠的跨模态特征",
                "FMA通过多步调整学习跨模态速度场，提升对齐精度和鲁棒性",
                "实验显示FMA在多种基准和骨干网络上显著提升性能，尤其在挑战性数据集"
            ],
            "tags_zh": [
                "跨模态学习",
                "少样本学习",
                "参数高效微调",
                "特征对齐",
                "Flow Matching Alignment"
            ],
            "_index": 88
        },
        {
            "title": "Exploring Image Representation with Decoupled Classical Visual Descriptors",
            "authors": [
                "Chenyuan Qu",
                "Hao Chen",
                "Jianbo Jiao"
            ],
            "arxiv_id": "2510.14536v1",
            "summary": "Exploring and understanding efficient image representations is a\nlong-standing challenge in computer vision. While deep learning has achieved\nremarkable progress across image understanding tasks, its internal\nrepresentations are often opaque, making it difficult to interpret how visual\ninformation is processed. In contrast, classical visual descriptors (e.g. edge,\ncolour, and intensity distribution) have long been fundamental to image\nanalysis and remain intuitively understandable to humans. Motivated by this\ngap, we ask a central question: Can modern learning benefit from these\nclassical cues? In this paper, we answer it with VisualSplit, a framework that\nexplicitly decomposes images into decoupled classical descriptors, treating\neach as an independent but complementary component of visual knowledge. Through\na reconstruction-driven pre-training scheme, VisualSplit learns to capture the\nessence of each visual descriptor while preserving their interpretability. By\nexplicitly decomposing visual attributes, our method inherently facilitates\neffective attribute control in various advanced visual tasks, including image\ngeneration and editing, extending beyond conventional classification and\nsegmentation, suggesting the effectiveness of this new learning approach for\nvisual understanding. Project page: https://chenyuanqu.com/VisualSplit/.",
            "headline_zh": "提出VisualSplit框架，通过解耦经典视觉描述符提升图像表示的可解释性和控制能力。",
            "intro_zh": [
                "核心问题：深度学习图像表示不透明，难以解释视觉信息处理过程。",
                "方法要点：使用解耦经典描述符，通过重建预训练学习独立视觉属性。",
                "实验或效果：在图像生成和编辑等任务中实现有效属性控制。"
            ],
            "tags_zh": [
                "图像表示",
                "经典视觉描述符",
                "解耦学习",
                "重建预训练",
                "属性控制",
                "可解释性"
            ],
            "_index": 89
        },
        {
            "title": "Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval",
            "authors": [
                "Keima Abe",
                "Hayato Muraki",
                "Shuhei Tomoshige",
                "Kenichi Oishi",
                "Hitoshi Iyatomi"
            ],
            "arxiv_id": "2510.14535v1",
            "summary": "Medical images like MR scans often show domain shifts across imaging sites\ndue to scanner and protocol differences, which degrade machine learning\nperformance in tasks such as disease classification. Domain harmonization is\nthus a critical research focus. Recent approaches encode brain images\n$\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then\ndisentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and\n$\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these\nmethods often lack interpretability$-$an essential requirement in medical\napplications$-$leaving practical issues unresolved. We propose\nPseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a\ngeneral framework for domain harmonization and interpretable representation\nlearning that preserves disease-relevant information in brain MR images.\nPL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract\n$\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image\n$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the\nencoder and domain predictor, the model learns to reconstruct the input image\n$\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and\n$\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared\nto prior methods, PL-SE-ADA achieves equal or better performance in image\nreconstruction, disease classification, and domain recognition. It also enables\nvisualization of both domain-independent brain features and domain-specific\ncomponents, offering high interpretability across the entire framework.",
            "headline_zh": "提出PL-SE-ADA框架以解决脑MR图像领域偏移问题，实现可解释的领域协调。",
            "intro_zh": [
                "脑MR图像因扫描器和协议差异存在领域偏移，影响机器学习性能。",
                "使用双编码器分离领域不变和特定特征，结合对抗训练和图像重建。",
                "在图像重建、疾病分类和领域识别中表现优异，支持特征可视化。"
            ],
            "tags_zh": [
                "脑MR图像",
                "领域协调",
                "可解释表示学习",
                "对抗训练",
                "图像重建",
                "疾病分类"
            ],
            "_index": 90
        },
        {
            "title": "Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology",
            "authors": [
                "Xinrui Huang",
                "Fan Xiao",
                "Dongming He",
                "Anqi Gao",
                "Dandan Li",
                "Xiaofan Zhang",
                "Shaoting Zhang",
                "Xudong Wang"
            ],
            "arxiv_id": "2510.14532v1",
            "summary": "Oral and maxillofacial radiology plays a vital role in dental healthcare, but\nradiographic image interpretation is limited by a shortage of trained\nprofessionals. While AI approaches have shown promise, existing dental AI\nsystems are restricted by their single-modality focus, task-specific design,\nand reliance on costly labeled data, hindering their generalization across\ndiverse clinical scenarios. To address these challenges, we introduce DentVFM,\nthe first family of vision foundation models (VFMs) designed for dentistry.\nDentVFM generates task-agnostic visual representations for a wide range of\ndental applications and uses self-supervised learning on DentVista, a large\ncurated dental imaging dataset with approximately 1.6 million multi-modal\nradiographic images from various medical centers. DentVFM includes 2D and 3D\nvariants based on the Vision Transformer (ViT) architecture. To address gaps in\ndental intelligence assessment and benchmarks, we introduce DentBench, a\ncomprehensive benchmark covering eight dental subspecialties, more diseases,\nimaging modalities, and a wide geographical distribution. DentVFM shows\nimpressive generalist intelligence, demonstrating robust generalization to\ndiverse dental tasks, such as disease diagnosis, treatment analysis, biomarker\nidentification, and anatomical landmark detection and segmentation.\nExperimental results indicate DentVFM significantly outperforms supervised,\nself-supervised, and weakly supervised baselines, offering superior\ngeneralization, label efficiency, and scalability. Additionally, DentVFM\nenables cross-modality diagnostics, providing more reliable results than\nexperienced dentists in situations where conventional imaging is unavailable.\nDentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and\nlabel-efficient model to improve intelligent dental healthcare and address\ncritical gaps in global oral healthcare.",
            "headline_zh": "提出DentVFM视觉基础模型以解决牙科AI泛化不足问题",
            "intro_zh": [
                "牙科放射图像解读受限于专业医生短缺和AI系统单模态、任务特定设计",
                "基于ViT架构开发2D和3D模型，使用自监督学习在160万张多模态图像上训练",
                "在DentBench基准测试中显著优于基线，实现跨任务和跨模态泛化"
            ],
            "tags_zh": [
                "视觉基础模型",
                "自监督学习",
                "牙科放射学",
                "多模态图像",
                "泛化能力",
                "基准测试"
            ],
            "_index": 91
        },
        {
            "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
            "authors": [
                "Cheng Cui",
                "Ting Sun",
                "Suyin Liang",
                "Tingquan Gao",
                "Zelun Zhang",
                "Jiaxuan Liu",
                "Xueqing Wang",
                "Changda Zhou",
                "Hongen Liu",
                "Manhui Lin",
                "Yue Zhang",
                "Yubo Zhang",
                "Handong Zheng",
                "Jing Zhang",
                "Jun Zhang",
                "Yi Liu",
                "Dianhai Yu",
                "Yanjun Ma"
            ],
            "arxiv_id": "2510.14528v1",
            "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
            "headline_zh": "提出PaddleOCR-VL以提升多语言文档解析效率，采用0.9B超紧凑视觉语言模型。",
            "intro_zh": [
                "核心问题：多语言文档解析需高效识别文本、表格、公式等复杂元素。",
                "方法要点：集成NaViT视觉编码器与ERNIE语言模型，支持109种语言。",
                "实验或效果：在公开和内部基准测试中达到SOTA性能，推理速度快。"
            ],
            "tags_zh": [
                "文档解析",
                "视觉语言模型",
                "多语言支持",
                "资源效率",
                "元素识别"
            ],
            "_index": 92
        },
        {
            "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models",
            "authors": [
                "Yunze Tong",
                "Didi Zhu",
                "Zijing Hu",
                "Jinluan Yang",
                "Ziyu Zhao"
            ],
            "arxiv_id": "2510.14526v1",
            "summary": "In text-to-image generation, different initial noises induce distinct\ndenoising paths with a pretrained Stable Diffusion (SD) model. While this\npattern could output diverse images, some of them may fail to align well with\nthe prompt. Existing methods alleviate this issue either by altering the\ndenoising dynamics or by drawing multiple noises and conducting post-selection.\nIn this paper, we attribute the misalignment to a training-inference mismatch:\nduring training, prompt-conditioned noises lie in a prompt-specific subset of\nthe latent space, whereas at inference the noise is drawn from a\nprompt-agnostic Gaussian prior. To close this gap, we propose a noise projector\nthat applies text-conditioned refinement to the initial noise before denoising.\nConditioned on the prompt embedding, it maps the noise to a prompt-aware\ncounterpart that better matches the distribution observed during SD training,\nwithout modifying the SD model. Our framework consists of these steps: we first\nsample some noises and obtain token-level feedback for their corresponding\nimages from a vision-language model (VLM), then distill these signals into a\nreward model, and finally optimize the noise projector via a quasi-direct\npreference optimization. Our design has two benefits: (i) it requires no\nreference images or handcrafted priors, and (ii) it incurs small inference\ncost, replacing multi-sample selection with a single forward pass. Extensive\nexperiments further show that our prompt-aware noise projection improves\ntext-image alignment across diverse prompts.",
            "headline_zh": "提出噪声投影器以解决扩散模型中文本-图像不对齐问题",
            "intro_zh": [
                "核心问题：训练-推理不匹配导致文本-图像不对齐，噪声分布差异是关键",
                "方法要点：使用噪声投影器将初始噪声映射为提示感知版本，无需修改SD模型",
                "实验或效果：通过VLM反馈和优化，提升多样提示下的文本-图像对齐效果"
            ],
            "tags_zh": [
                "文本到图像生成",
                "扩散模型",
                "噪声投影",
                "训练-推理不匹配",
                "视觉语言模型",
                "偏好优化"
            ],
            "_index": 93
        },
        {
            "title": "Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing",
            "authors": [
                "Qurrat Ul Ain",
                "Atif Aftab Ahmed Jilani",
                "Zunaira Shafqat",
                "Nigar Azhar Butt"
            ],
            "arxiv_id": "2510.14525v1",
            "summary": "Defective surgical instruments pose serious risks to sterility, mechanical\nintegrity, and patient safety, increasing the likelihood of surgical\ncomplications. However, quality control in surgical instrument manufacturing\noften relies on manual inspection, which is prone to human error and\ninconsistency. This study introduces SurgScan, an AI-powered defect detection\nframework for surgical instruments. Using YOLOv8, SurgScan classifies defects\nin real-time, ensuring high accuracy and industrial scalability. The model is\ntrained on a high-resolution dataset of 102,876 images, covering 11 instrument\ntypes and five major defect categories. Extensive evaluation against\nstate-of-the-art CNN architectures confirms that SurgScan achieves the highest\naccuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,\nmaking it suitable for industrial deployment. Statistical analysis demonstrates\nthat contrast-enhanced preprocessing significantly improves defect detection,\naddressing key limitations in visual inspection. SurgScan provides a scalable,\ncost-effective AI solution for automated quality control, reducing reliance on\nmanual inspection while ensuring compliance with ISO 13485 and FDA standards,\npaving the way for enhanced defect detection in medical manufacturing.",
            "headline_zh": "提出SurgScan框架，通过AI实时检测手术器械缺陷以提升质量控制。",
            "intro_zh": [
                "手术器械缺陷威胁无菌性和患者安全，手动检测易出错且不一致。",
                "基于YOLOv8构建AI框架，实现高精度实时缺陷分类与工业可扩展性。",
                "在102,876图像数据集上训练，准确率达99.3%，推理速度4.2-5.8毫秒。"
            ],
            "tags_zh": [
                "实时缺陷检测",
                "手术器械质量控制",
                "YOLOv8模型",
                "非破坏性测试",
                "工业AI部署",
                "图像预处理增强"
            ],
            "_index": 94
        },
        {
            "title": "Vision Mamba for Permeability Prediction of Porous Media",
            "authors": [
                "Ali Kashefi",
                "Tapan Mukerji"
            ],
            "arxiv_id": "2510.14516v1",
            "summary": "Vision Mamba has recently received attention as an alternative to Vision\nTransformers (ViTs) for image classification. The network size of Vision Mamba\nscales linearly with input image resolution, whereas ViTs scale quadratically,\na feature that improves computational and memory efficiency. Moreover, Vision\nMamba requires a significantly smaller number of trainable parameters than\ntraditional convolutional neural networks (CNNs), and thus, they can be more\nmemory efficient. Because of these features, we introduce, for the first time,\na neural network that uses Vision Mamba as its backbone for predicting the\npermeability of three-dimensional porous media. We compare the performance of\nVision Mamba with ViT and CNN models across multiple aspects of permeability\nprediction and perform an ablation study to assess the effects of its\ncomponents on accuracy. We demonstrate in practice the aforementioned\nadvantages of Vision Mamba over ViTs and CNNs in the permeability prediction of\nthree-dimensional porous media. We make the source code publicly available to\nfacilitate reproducibility and to enable other researchers to build on and\nextend this work. We believe the proposed framework has the potential to be\nintegrated into large vision models in which Vision Mamba is used instead of\nViTs.",
            "headline_zh": "提出基于Vision Mamba的神经网络以预测三维多孔介质渗透率",
            "intro_zh": [
                "核心问题：预测三维多孔介质的渗透率，替代传统ViT和CNN方法。",
                "方法要点：使用Vision Mamba作为骨干网络，具有线性计算复杂度和较少参数。",
                "实验或效果：通过比较和消融研究，验证其在计算效率和内存使用上的优势。"
            ],
            "tags_zh": [
                "渗透率预测",
                "Vision Mamba",
                "多孔介质",
                "计算效率",
                "神经网络",
                "图像分类"
            ],
            "_index": 95
        },
        {
            "title": "Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots",
            "authors": [
                "Mingtian Du",
                "Suhas Raghavendra Kulkarni",
                "Simone Kager",
                "Domenico Campolo"
            ],
            "arxiv_id": "2510.14511v1",
            "summary": "This paper establishes analytical stability criteria for robot-mediated\nhuman-human (dyadic) interaction systems, focusing on haptic communication\nunder network-induced time delays. Through frequency-domain analysis supported\nby numerical simulations, we identify both delay-independent and\ndelay-dependent stability criteria. The delay-independent criterion guarantees\nstability irrespective of the delay, whereas the delay-dependent criterion is\ncharacterised by a maximum tolerable delay before instability occurs. The\ncriteria demonstrate dependence on controller and robot dynamic parameters,\nwhere increasing stiffness reduces the maximum tolerable delay in a non-linear\nmanner, thereby heightening system vulnerability. The proposed criteria can be\ngeneralised to a wide range of robot-mediated interactions and serve as design\nguidelines for stable remote dyadic systems. Experiments with robots performing\nhuman-like movements further illustrate the correlation between stability and\nmotor performance. The findings of this paper suggest the prerequisites for\neffective delay-compensation strategies.",
            "headline_zh": "提出机器人介导延迟触觉交互的稳定性准则，分析延迟对系统稳定性的影响。",
            "intro_zh": [
                "核心问题：网络延迟下机器人介导的人-人触觉交互系统稳定性分析。",
                "方法要点：通过频域分析和数值模拟，建立延迟无关和延迟相关稳定性准则。",
                "实验或效果：机器人实验验证稳定性与运动性能的相关性，指导远程系统设计。"
            ],
            "tags_zh": [
                "触觉交互",
                "稳定性分析",
                "机器人介导",
                "网络延迟",
                "频域方法",
                "运动性能"
            ],
            "_index": 96
        },
        {
            "title": "Grazing Detection using Deep Learning and Sentinel-2 Time Series Data",
            "authors": [
                "Aleksis Pirinen",
                "Delia Fano Yela",
                "Smita Chakraborty",
                "Erik Källman"
            ],
            "arxiv_id": "2510.14493v1",
            "summary": "Grazing shapes both agricultural production and biodiversity, yet scalable\nmonitoring of where grazing occurs remains limited. We study seasonal grazing\ndetection from Sentinel-2 L2A time series: for each polygon-defined field\nboundary, April-October imagery is used for binary prediction (grazed / not\ngrazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance\nfeatures, and achieve an average F1 score of 77 percent across five validation\nsplits, with 90 percent recall on grazed pastures. Operationally, if inspectors\ncan visit at most 4 percent of sites annually, prioritising fields predicted by\nour model as non-grazed yields 17.2 times more confirmed non-grazing sites than\nrandom inspection. These results indicate that coarse-resolution, freely\navailable satellite data can reliably steer inspection resources for\nconservation-aligned land-use compliance. Code and models have been made\npublicly available.",
            "headline_zh": "提出基于CNN-LSTM集成模型的放牧检测方法，利用Sentinel-2时序数据优化资源分配。",
            "intro_zh": [
                "核心问题：规模化监测放牧活动以支持农业和生物多样性保护。",
                "方法要点：使用Sentinel-2 L2A时间序列数据，训练CNN-LSTM集成模型进行二元分类。",
                "实验或效果：平均F1分数77%，召回率90%，资源分配效率提升17.2倍。"
            ],
            "tags_zh": [
                "放牧检测",
                "深度学习",
                "时序数据分析",
                "卫星遥感",
                "资源优化"
            ],
            "_index": 97
        },
        {
            "title": "Restoring Noisy Demonstration for Imitation Learning With Diffusion Models",
            "authors": [
                "Shang-Fu Chen",
                "Co Yong",
                "Shao-Hua Sun"
            ],
            "arxiv_id": "2510.14467v1",
            "summary": "Imitation learning (IL) aims to learn a policy from expert demonstrations and\nhas been applied to various applications. By learning from the expert policy,\nIL methods do not require environmental interactions or reward signals.\nHowever, most existing imitation learning algorithms assume perfect expert\ndemonstrations, but expert demonstrations often contain imperfections caused by\nerrors from human experts or sensor/control system inaccuracies. To address the\nabove problems, this work proposes a filter-and-restore framework to best\nleverage expert demonstrations with inherent noise. Our proposed method first\nfilters clean samples from the demonstrations and then learns conditional\ndiffusion models to recover the noisy ones. We evaluate our proposed framework\nand existing methods in various domains, including robot arm manipulation,\ndexterous manipulation, and locomotion. The experiment results show that our\nproposed framework consistently outperforms existing methods across all the\ntasks. Ablation studies further validate the effectiveness of each component\nand demonstrate the framework's robustness to different noise types and levels.\nThese results confirm the practical applicability of our framework to noisy\noffline demonstration data.",
            "headline_zh": "提出过滤-恢复框架以解决模仿学习中噪声专家演示问题",
            "intro_zh": [
                "核心问题：专家演示常含噪声，影响模仿学习性能",
                "方法要点：先过滤干净样本，再用条件扩散模型恢复噪声样本",
                "实验效果：在机器人操作和运动任务中优于现有方法，验证鲁棒性"
            ],
            "tags_zh": [
                "模仿学习",
                "噪声演示",
                "扩散模型",
                "机器人操作",
                "离线学习"
            ],
            "_index": 98
        },
        {
            "title": "Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration",
            "authors": [
                "Thomas Katraouras",
                "Dimitrios Rafailidis"
            ],
            "arxiv_id": "2510.14463v1",
            "summary": "Image quality is a critical factor in delivering visually appealing content\non web platforms. However, images often suffer from degradation due to lossy\noperations applied by online social networks (OSNs), negatively affecting user\nexperience. Image restoration is the process of recovering a clean high-quality\nimage from a given degraded input. Recently, multi-task (all-in-one) image\nrestoration models have gained significant attention, due to their ability to\nsimultaneously handle different types of image degradations. However, these\nmodels often come with an excessively high number of trainable parameters,\nmaking them computationally inefficient. In this paper, we propose a strategy\nfor compressing multi-task image restoration models. We aim to discover highly\nsparse subnetworks within overparameterized deep models that can match or even\nsurpass the performance of their dense counterparts. The proposed model, namely\nMIR-L, utilizes an iterative pruning strategy that removes low-magnitude\nweights across multiple rounds, while resetting the remaining weights to their\noriginal initialization. This iterative process is important for the multi-task\nimage restoration model's optimization, effectively uncovering \"winning\ntickets\" that maintain or exceed state-of-the-art performance at high sparsity\nlevels. Experimental evaluation on benchmark datasets for the deraining,\ndehazing, and denoising tasks shows that MIR-L retains only 10% of the\ntrainable parameters while maintaining high image restoration performance. Our\ncode, datasets and pre-trained models are made publicly available at\nhttps://github.com/Thomkat/MIR-L.",
            "headline_zh": "提出迭代剪枝策略以压缩多任务图像修复模型，提升计算效率",
            "intro_zh": [
                "多任务图像修复模型参数过多，导致计算效率低下",
                "采用迭代剪枝移除低幅值权重，并重置剩余权重以发现稀疏子网络",
                "实验显示在去雨、去雾、去噪任务中，仅保留10%参数仍保持高性能"
            ],
            "tags_zh": [
                "图像修复",
                "多任务学习",
                "模型剪枝",
                "稀疏网络",
                "计算效率"
            ],
            "_index": 99
        },
        {
            "title": "Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review",
            "authors": [
                "Youwan Mahé",
                "Elise Bannier",
                "Stéphanie Leplaideur",
                "Elisa Fromont",
                "Francesca Galassi"
            ],
            "arxiv_id": "2510.14462v1",
            "summary": "Unsupervised deep generative models are emerging as a promising alternative\nto supervised methods for detecting and segmenting anomalies in brain imaging.\nUnlike fully supervised approaches, which require large voxel-level annotated\ndatasets and are limited to well-characterised pathologies, these models can be\ntrained exclusively on healthy data and identify anomalies as deviations from\nlearned normative brain structures. This PRISMA-guided scoping review\nsynthesises recent work on unsupervised deep generative models for anomaly\ndetection in neuroimaging, including autoencoders, variational autoencoders,\ngenerative adversarial networks, and denoising diffusion models. A total of 49\nstudies published between 2018 - 2025 were identified, covering applications to\nbrain MRI and, less frequently, CT across diverse pathologies such as tumours,\nstroke, multiple sclerosis, and small vessel disease. Reported performance\nmetrics are compared alongside architectural design choices. Across the\nincluded studies, generative models achieved encouraging performance for large\nfocal lesions and demonstrated progress in addressing more subtle\nabnormalities. A key strength of generative models is their ability to produce\ninterpretable pseudo-healthy (also referred to as counterfactual)\nreconstructions, which is particularly valuable when annotated data are scarce,\nas in rare or heterogeneous diseases. Looking ahead, these models offer a\ncompelling direction for anomaly detection, enabling semi-supervised learning,\nsupporting the discovery of novel imaging biomarkers, and facilitating within-\nand cross-disease deviation mapping in unified end-to-end frameworks. To\nrealise clinical impact, future work should prioritise anatomy-aware modelling,\ndevelopment of foundation models, task-appropriate evaluation metrics, and\nrigorous clinical validation.",
            "headline_zh": "综述无监督深度生成模型在神经影像异常检测中的应用与进展",
            "intro_zh": [
                "核心问题：监督方法依赖大量标注数据，难以处理罕见或异质性疾病。",
                "方法要点：使用健康数据训练生成模型，检测异常作为与规范结构的偏差。",
                "实验或效果：模型在大型局灶性病变中表现良好，并生成可解释的伪健康重建。"
            ],
            "tags_zh": [
                "无监督学习",
                "深度生成模型",
                "神经影像异常检测",
                "伪健康重建",
                "脑部MRI",
                "综述研究"
            ],
            "_index": 100
        },
        {
            "title": "Structured Universal Adversarial Attacks on Object Detection for Video Sequences",
            "authors": [
                "Sven Jacob",
                "Weijia Shao",
                "Gjergji Kasneci"
            ],
            "arxiv_id": "2510.14460v1",
            "summary": "Video-based object detection plays a vital role in safety-critical\napplications. While deep learning-based object detectors have achieved\nimpressive performance, they remain vulnerable to adversarial attacks,\nparticularly those involving universal perturbations. In this work, we propose\na minimally distorted universal adversarial attack tailored for video object\ndetection, which leverages nuclear norm regularization to promote structured\nperturbations concentrated in the background. To optimize this formulation\nefficiently, we employ an adaptive, optimistic exponentiated gradient method\nthat enhances both scalability and convergence. Our results demonstrate that\nthe proposed attack outperforms both low-rank projected gradient descent and\nFrank-Wolfe based attacks in effectiveness while maintaining high stealthiness.\nAll code and data are publicly available at\nhttps://github.com/jsve96/AO-Exp-Attack.",
            "headline_zh": "提出结构化通用对抗攻击以提升视频目标检测的鲁棒性评估",
            "intro_zh": [
                "视频目标检测在安全关键应用中易受通用对抗攻击影响",
                "采用核范数正则化生成背景集中的结构化扰动，并使用自适应指数梯度优化",
                "实验显示攻击效果优于低秩投影梯度下降和Frank-Wolfe方法，保持高隐蔽性"
            ],
            "tags_zh": [
                "视频目标检测",
                "通用对抗攻击",
                "核范数正则化",
                "指数梯度优化",
                "结构化扰动"
            ],
            "_index": 101
        },
        {
            "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking",
            "authors": [
                "Tao Huang",
                "Huayi Wang",
                "Junli Ren",
                "Kangning Yin",
                "Zirui Wang",
                "Xiao Chen",
                "Feiyu Jia",
                "Wentao Zhang",
                "Junfeng Long",
                "Jingbo Wang",
                "Jiangmiao Pang"
            ],
            "arxiv_id": "2510.14454v1",
            "summary": "Humanoid robots are envisioned to adapt demonstrated motions to diverse\nreal-world conditions while accurately preserving motion patterns. Existing\nmotion prior approaches enable well adaptability with a few motions but often\nsacrifice imitation accuracy, whereas motion-tracking methods achieve accurate\nimitation yet require many training motions and a test-time target motion to\nadapt. To combine their strengths, we introduce AdaMimic, a novel motion\ntracking algorithm that enables adaptable humanoid control from a single\nreference motion. To reduce data dependence while ensuring adaptability, our\nmethod first creates an augmented dataset by sparsifying the single reference\nmotion into keyframes and applying light editing with minimal physical\nassumptions. A policy is then initialized by tracking these sparse keyframes to\ngenerate dense intermediate motions, and adapters are subsequently trained to\nadjust tracking speed and refine low-level actions based on the adjustment,\nenabling flexible time warping that further improves imitation accuracy and\nadaptability. We validate these significant improvements in our approach in\nboth simulation and the real-world Unitree G1 humanoid robot in multiple tasks\nacross a wide range of adaptation conditions. Videos and code are available at\nhttps://taohuang13.github.io/adamimic.github.io/.",
            "headline_zh": "提出AdaMimic算法，实现从单一参考运动进行自适应人形机器人控制",
            "intro_zh": [
                "核心问题：现有方法在运动适应性和模仿精度间难以平衡，数据依赖高",
                "方法要点：通过稀疏关键帧增强数据集，训练策略和适配器实现灵活时间扭曲",
                "实验或效果：在仿真和真实机器人中验证，适应多种条件，提升精度和适应性"
            ],
            "tags_zh": [
                "人形机器人控制",
                "运动跟踪",
                "自适应算法",
                "关键帧增强",
                "时间扭曲"
            ],
            "_index": 102
        },
        {
            "title": "Real-Time Neural Video Compression with Unified Intra and Inter Coding",
            "authors": [
                "Hui Xiang",
                "Yifan Bian",
                "Li Li",
                "Jingran Wu",
                "Xianguo Zhang",
                "Dong Liu"
            ],
            "arxiv_id": "2510.14431v1",
            "summary": "Neural video compression (NVC) technologies have advanced rapidly in recent\nyears, yielding state-of-the-art schemes such as DCVC-RT that offer superior\ncompression efficiency to H.266/VVC and real-time encoding/decoding\ncapabilities. Nonetheless, existing NVC schemes have several limitations,\nincluding inefficiency in dealing with disocclusion and new content, interframe\nerror propagation and accumulation, among others. To eliminate these\nlimitations, we borrow the idea from classic video coding schemes, which allow\nintra coding within inter-coded frames. With the intra coding tool enabled,\ndisocclusion and new content are properly handled, and interframe error\npropagation is naturally intercepted without the need for manual refresh\nmechanisms. We present an NVC framework with unified intra and inter coding,\nwhere every frame is processed by a single model that is trained to perform\nintra/inter coding adaptively. Moreover, we propose a simultaneous two-frame\ncompression design to exploit interframe redundancy not only forwardly but also\nbackwardly. Experimental results show that our scheme outperforms DCVC-RT by an\naverage of 10.7\\% BD-rate reduction, delivers more stable bitrate and quality\nper frame, and retains real-time encoding/decoding performances. Code and\nmodels will be released.",
            "headline_zh": "提出统一帧内帧间编码的神经视频压缩框架，以解决遮挡和误差传播问题。",
            "intro_zh": [
                "核心问题：现有神经视频压缩在处理遮挡、新内容和帧间误差传播时效率低下。",
                "方法要点：引入帧内编码工具，使用单一模型自适应处理帧内和帧间编码。",
                "实验效果：相比DCVC-RT平均BD-rate降低10.7%，保持实时编码解码性能。"
            ],
            "tags_zh": [
                "神经视频压缩",
                "帧内帧间编码",
                "实时编码",
                "误差传播抑制",
                "遮挡处理"
            ],
            "_index": 103
        },
        {
            "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
            "authors": [
                "Ho Yin Au",
                "Jie Chen",
                "Junkun Jiang",
                "Jingyu Xiang"
            ],
            "arxiv_id": "2510.14427v1",
            "summary": "Recent research on motion generation has shown significant progress in\ngenerating semantically aligned motion with singular semantics. However, when\nemploying these models to create composite sequences containing multiple\nsemantically generated motion clips, they often struggle to preserve the\ncontinuity of motion dynamics at the transition boundaries between clips,\nresulting in awkward transitions and abrupt artifacts. To address these\nchallenges, we present Compositional Phase Diffusion, which leverages the\nSemantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module\n(TPDM) to progressively incorporate semantic guidance and phase details from\nadjacent motion clips into the diffusion process. Specifically, SPDM and TPDM\noperate within the latent motion frequency domain established by the\npre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them\nto learn semantically important and transition-aware phase information from\nvariable-length motion clips during training. Experimental results demonstrate\nthe competitive performance of our proposed framework in generating\ncompositional motion sequences that align semantically with the input\nconditions, while preserving phase transitional continuity between preceding\nand succeeding motion clips. Additionally, motion inbetweening task is made\npossible by keeping the phase parameter of the input motion sequences fixed\nthroughout the diffusion process, showcasing the potential for extending the\nproposed framework to accommodate various application scenarios. Codes are\navailable at https://github.com/asdryau/TransPhase.",
            "headline_zh": "提出组合相位扩散方法以解决长运动序列生成中的过渡不连续问题",
            "intro_zh": [
                "核心问题：现有模型生成多语义运动序列时，过渡边界处运动动态不连续，产生突兀伪影。",
                "方法要点：利用SPDM和TPDM在潜在运动频率域中逐步融合语义指导和相邻运动相位细节。",
                "实验或效果：框架在生成语义对齐的组合运动序列中表现竞争性，保持相位过渡连续性。"
            ],
            "tags_zh": [
                "运动序列生成",
                "扩散模型",
                "相位扩散",
                "语义对齐",
                "过渡连续性",
                "运动插值"
            ],
            "_index": 104
        },
        {
            "title": "RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit",
            "authors": [
                "Baris Baysal",
                "Omid Arfaie",
                "Ramazan Unal"
            ],
            "arxiv_id": "2510.14414v1",
            "summary": "This study presents a powered transtibial prosthesis with complete push-off\nassistance, RoboANKLE. The design aims to fulfill specific requirements, such\nas a sufficient range of motion (RoM) while providing the necessary torque for\nachieving natural ankle motion in daily activities. Addressing the challenges\nfaced in designing active transtibial prostheses, such as maintaining energetic\nautonomy and minimizing weight, is vital for the study. With this aim, we try\nto imitate the human ankle by providing extensive push-off assistance to\nachieve a natural-like torque profile. Thus, Energy Store and Extended Release\nmechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.\nKinematic and kinetic analyses are carried out to determine the design\nparameters and assess the design performance. Subsequently, a Computer-Aided\nDesign (CAD) model is built and used in comprehensive dynamic and structural\nanalyses. These analyses are used for the design performance evaluation and\ndetermine the forces and torques applied to the prosthesis, which aids in\noptimizing the design for minimal weight via structural analysis and topology\noptimization. The design of the prototype is then finalized and manufactured\nfor experimental evaluation to validate the design and functionality. The\nprototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.\nThe Functional evaluations of the RoboANKLE revealed that it is capable of\nachieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,\nThanks to the implemented mechanisms, the results show that RoboANKLE can\ngenerate 57% higher than the required torque for natural walking. The result of\nthe power generation capacity of the RoboANKLE is 10% more than the natural\npower during the gait cycle.",
            "headline_zh": "提出RoboANKLE机器人踝关节，通过电机化顺应单元实现完整推离辅助，以模拟自然踝关节运动。",
            "intro_zh": [
                "核心问题：设计主动式胫骨假体需平衡能量自主性、重量最小化及自然运动范围。",
                "方法要点：采用能量存储与扩展释放机制及额外能量存储机制，进行运动学和动力学分析。",
                "实验或效果：原型重1.92公斤，能实现95%自然最大背屈角度，扭矩和功率分别超出自然行走需求57%和10%。"
            ],
            "tags_zh": [
                "机器人踝关节",
                "主动假体设计",
                "能量存储机制",
                "运动学分析",
                "结构优化",
                "功能评估"
            ],
            "_index": 105
        },
        {
            "title": "DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis",
            "authors": [
                "Chao Tu",
                "Kun Huang",
                "Jie Zhang",
                "Qianjin Feng",
                "Yu Zhang",
                "Zhenyuan Ning"
            ],
            "arxiv_id": "2510.14403v1",
            "summary": "The burgeoning discipline of computational pathology shows promise in\nharnessing whole slide images (WSIs) to quantify morphological heterogeneity\nand develop objective prognostic modes for human cancers. However, progress is\nimpeded by the computational bottleneck of gigapixel-size inputs and the\nscarcity of dense manual annotations. Current methods often overlook\nfine-grained information across multi-magnification WSIs and variations in\ntumor microenvironments. Here, we propose an easy-to-hard progressive\nrepresentation learning model, termed dual-curriculum contrastive\nmulti-instance learning (DCMIL), to efficiently process WSIs for cancer\nprognosis. The model does not rely on dense annotations and enables the direct\ntransformation of gigapixel-size WSIs into outcome predictions. Extensive\nexperiments on twelve cancer types (5,954 patients, 12.54 million tiles)\ndemonstrate that DCMIL outperforms standard WSI-based prognostic models.\nAdditionally, DCMIL identifies fine-grained prognosis-salient regions, provides\nrobust instance uncertainty estimation, and captures morphological differences\nbetween normal and tumor tissues, with the potential to generate new biological\ninsights. All codes have been made publicly accessible at\nhttps://github.com/tuuuc/DCMIL.",
            "headline_zh": "提出DCMIL模型以解决全切片图像癌症预后分析中的计算瓶颈和标注稀缺问题",
            "intro_zh": [
                "核心问题：全切片图像尺寸巨大且缺乏密集标注，阻碍癌症预后模型发展",
                "方法要点：采用双课程对比多实例学习，无需密集标注，渐进学习多放大级别特征",
                "实验或效果：在12种癌症类型上验证，性能优于标准模型，识别预后关键区域"
            ],
            "tags_zh": [
                "全切片图像分析",
                "多实例学习",
                "癌症预后",
                "对比学习",
                "计算病理学"
            ],
            "_index": 106
        },
        {
            "title": "BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble",
            "authors": [
                "Brandon Hill",
                "Kma Solaiman"
            ],
            "arxiv_id": "2510.14389v1",
            "summary": "Motherboard defect detection is critical for ensuring reliability in\nhigh-volume electronics manufacturing. While prior research in PCB inspection\nhas largely targeted bare-board or trace-level defects, assembly-level\ninspection of full motherboards inspection remains underexplored. In this work,\nwe present BoardVision, a reproducible framework for detecting assembly-level\ndefects such as missing screws, loose fan wiring, and surface scratches. We\nbenchmark two representative detectors - YOLOv7 and Faster R-CNN, under\ncontrolled conditions on the MiracleFactory motherboard dataset, providing the\nfirst systematic comparison in this domain. To mitigate the limitations of\nsingle models, where YOLO excels in precision but underperforms in recall and\nFaster R-CNN shows the reverse, we propose a lightweight ensemble,\nConfidence-Temporal Voting (CTV Voter), that balances precision and recall\nthrough interpretable rules. We further evaluate robustness under realistic\nperturbations including sharpness, brightness, and orientation changes,\nhighlighting stability challenges often overlooked in motherboard defect\ndetection. Finally, we release a deployable GUI-driven inspection tool that\nbridges research evaluation with operator usability. Together, these\ncontributions demonstrate how computer vision techniques can transition from\nbenchmark results to practical quality assurance for assembly-level motherboard\nmanufacturing.",
            "headline_zh": "提出BoardVision框架以解决主板组装缺陷检测问题，通过YOLO+Faster-RCNN集成提升精度与召回率。",
            "intro_zh": [
                "核心问题：主板组装级缺陷检测在电子制造中未充分探索，如缺失螺丝和表面划痕。",
                "方法要点：使用YOLOv7和Faster-RCNN基准测试，并设计轻量级CTV Voter集成方法。",
                "实验或效果：在MiracleFactory数据集评估，增强鲁棒性并发布可部署GUI工具。"
            ],
            "tags_zh": [
                "主板缺陷检测",
                "目标检测集成",
                "鲁棒性评估",
                "部署工具",
                "YOLOv7",
                "Faster-RCNN"
            ],
            "_index": 107
        },
        {
            "title": "DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights",
            "authors": [
                "Danish Ali",
                "Ajmal Mian",
                "Naveed Akhtar",
                "Ghulam Mubashar Hassan"
            ],
            "arxiv_id": "2510.14383v1",
            "summary": "Accurate brain tumor segmentation is significant for clinical diagnosis and\ntreatment. It is challenging due to the heterogeneity of tumor subregions.\nMamba-based State Space Models have demonstrated promising performance.\nHowever, they incur significant computational overhead due to sequential\nfeature computation across multiple spatial axes. Moreover, their robustness\nacross diverse BraTS data partitions remains largely unexplored, leaving a\ncritical gap in reliable evaluation. To address these limitations, we propose\ndual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation\nmodel that captures multi-scale long-range dependencies with minimal\ncomputational overhead. We leverage a space-filling curve to preserve spatial\nlocality during 3D-to-1D feature mapping, thereby reducing reliance on\ncomputationally expensive multi-axial feature scans. To enrich feature\nrepresentation, we propose a gated fusion module that adaptively integrates\nforward and reverse contexts, along with a quantization block that discretizes\nfeatures to improve robustness. In addition, we propose five systematic folds\non BraTS2023 for rigorous evaluation of segmentation techniques under diverse\nconditions and present detailed analysis of common failure scenarios. On the\n20\\% test set used by recent methods, our model achieves Dice improvements of\n0.10\\% for whole tumor, 1.75\\% for tumor core, and 0.93\\% for enhancing tumor.\nEvaluations on the proposed systematic five folds demonstrate that our model\nmaintains competitive whole tumor accuracy while achieving clear average Dice\ngains of 0.86\\% for tumor core and 1.45\\% for enhancing tumor over existing\nstate-of-the-art. Furthermore, our model attains 15 times improvement in\nefficiency while maintaining high segmentation accuracy, highlighting its\nrobustness and computational advantage over existing approaches.",
            "headline_zh": "提出DRBD-Mamba模型以高效鲁棒地分割脑肿瘤",
            "intro_zh": [
                "脑肿瘤分割因肿瘤亚区异质性而具挑战性，现有Mamba模型计算开销大且鲁棒性未知",
                "采用双分辨率双向Mamba捕获多尺度长程依赖，结合空间填充曲线和门控融合模块提升效率与特征表示",
                "在BraTS2023上验证，模型在肿瘤核心和增强肿瘤分割上Dice提升，效率提高15倍"
            ],
            "tags_zh": [
                "脑肿瘤分割",
                "状态空间模型",
                "多尺度依赖",
                "计算效率",
                "鲁棒性评估"
            ],
            "_index": 108
        },
        {
            "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation",
            "authors": [
                "Dongnam Byun",
                "Jungwon Park",
                "Jumgmin Ko",
                "Changin Choi",
                "Wonjong Rhee"
            ],
            "arxiv_id": "2510.14376v1",
            "summary": "Recent progress in text-to-image (T2I) generative models has led to\nsignificant improvements in generating high-quality images aligned with text\nprompts. However, these models still struggle with prompts involving multiple\nobjects, often resulting in object neglect or object mixing. Through extensive\nstudies, we identify four problematic scenarios, Similar Shapes, Similar\nTextures, Dissimilar Background Biases, and Many Objects, where inter-object\nrelationships frequently lead to such failures. Motivated by two key\nobservations about CLIP embeddings, we propose DOS (Directional Object\nSeparation), a method that modifies three types of CLIP text embeddings before\npassing them into text-to-image models. Experimental results show that DOS\nconsistently improves the success rate of multi-object image generation and\nreduces object mixing. In human evaluations, DOS significantly outperforms four\ncompeting methods, receiving 26.24%-43.04% more votes across four benchmarks.\nThese results highlight DOS as a practical and effective solution for improving\nmulti-object image generation.",
            "headline_zh": "提出DOS方法以解决多对象图像生成中的对象忽略与混合问题",
            "intro_zh": [
                "多对象图像生成常出现对象忽略或混合，源于相似形状、纹理、背景偏差和对象过多",
                "DOS通过修改CLIP文本嵌入，分离对象方向，提升多对象生成成功率",
                "实验显示DOS显著优于其他方法，人类评估中获26.24%-43.04%更多投票"
            ],
            "tags_zh": [
                "多对象图像生成",
                "文本到图像模型",
                "CLIP嵌入",
                "对象分离",
                "文本提示对齐"
            ],
            "_index": 109
        },
        {
            "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding",
            "authors": [
                "Han Qiu",
                "Peng Gao",
                "Lewei Lu",
                "Xiaoqin Zhang",
                "Ling Shao",
                "Shijian Lu"
            ],
            "arxiv_id": "2510.14374v1",
            "summary": "Multimodal large language models~(MLLMs) have demonstrated promising spatial\nunderstanding capabilities, such as referencing and grounding object\ndescriptions. Despite their successes, MLLMs still fall short in fine-grained\nspatial perception abilities, such as generating detailed region descriptions\nor accurately localizing objects. Additionally, they often fail to respond to\nthe user's requirements for desired fine-grained spatial understanding. This\nissue might arise because existing approaches primarily focus on tuning MLLMs\nto model pre-annotated instruction data to inject spatial knowledge, without\ndirect supervision of MLLMs' actual responses. We address this issue by SPR, a\nSpatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial\ncapabilities by rewarding MLLMs' detailed responses with precise object\nlocalization over vague or inaccurate responses. With randomly selected image\nregions and region descriptions from MLLMs, SPR introduces semantic and\nlocalization scores to comprehensively evaluate the text quality and\nlocalization quality in MLLM-generated descriptions. We also refine the MLLM\ndescriptions with better localization accuracy and pair the best-scored\nrefinement with the initial descriptions of the lowest score for direct\npreference optimization, thereby enhancing fine-grained alignment with visual\ninput. Extensive experiments over standard referring and grounding benchmarks\nshow that SPR improves MLLM spatial understanding capabilities effectively with\nminimal overhead in training. Data and code will be released at\nhttps://github.com/hanqiu-hq/SPR",
            "headline_zh": "提出空间偏好奖励方法以增强多模态大语言模型的空间理解能力",
            "intro_zh": [
                "核心问题：MLLMs在细粒度空间感知中表现不足，如生成详细区域描述或精确定位对象。",
                "方法要点：通过语义和定位评分奖励详细响应，结合直接偏好优化提升视觉对齐。",
                "实验或效果：在标准基准测试中有效改进空间理解，训练开销最小。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "空间理解",
                "偏好奖励",
                "直接偏好优化",
                "对象定位",
                "细粒度感知"
            ],
            "_index": 110
        },
        {
            "title": "AI for Service: Proactive Assistance with AI Glasses",
            "authors": [
                "Zichen Wen",
                "Yiyu Wang",
                "Chenfei Liao",
                "Boxue Yang",
                "Junxian Li",
                "Weifeng Liu",
                "Haocong He",
                "Bolong Feng",
                "Xuyang Liu",
                "Yuanhuiyi Lyu",
                "Xu Zheng",
                "Xuming Hu",
                "Linfeng Zhang"
            ],
            "arxiv_id": "2510.14359v1",
            "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.",
            "headline_zh": "提出Alpha-Service框架，通过AI眼镜实现主动实时服务辅助。",
            "intro_zh": [
                "核心问题：现有AI服务多为被动响应，无法主动预测用户需求。",
                "方法要点：基于von Neumann架构，构建多组件系统以检测服务机会并提供个性化服务。",
                "实验或效果：案例研究展示在Blackjack、博物馆导览等场景中实现无缝感知与及时辅助。"
            ],
            "tags_zh": [
                "主动AI服务",
                "AI眼镜",
                "多代理系统",
                "个性化辅助",
                "实时感知"
            ],
            "_index": 111
        },
        {
            "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation",
            "authors": [
                "Xiaobei Zhao",
                "Xingqi Lyu",
                "Xiang Li"
            ],
            "arxiv_id": "2510.14357v1",
            "summary": "Agricultural robots are emerging as powerful assistants across a wide range\nof agricultural tasks, nevertheless, still heavily rely on manual operation or\nfixed rail systems for movement. The AgriVLN method and the A2A benchmark\npioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural\ndomain, enabling robots to navigate to the target positions following the\nnatural language instructions. In practical agricultural scenarios, navigation\ninstructions often repeatedly occur, yet AgriVLN treat each instruction as an\nindependent episode, overlooking the potential of past experiences to provide\nspatial context for subsequent ones. To bridge this gap, we propose the method\nof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation\n(SUM-AgriVLN), in which the SUM module employs spatial understanding and save\nspatial memory through 3D reconstruction and representation. When evaluated on\nthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47\nto 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,\ndemonstrating the state-of-the-art performance in the agricultural domain.\nCode: https://github.com/AlexTraveling/SUM-AgriVLN.",
            "headline_zh": "提出空间理解记忆模块以提升农业视觉语言导航中的重复指令处理能力",
            "intro_zh": [
                "农业机器人导航中，重复指令缺乏空间上下文利用，导致导航效率受限。",
                "SUM模块通过3D重建与表示保存空间记忆，增强对过去经验的利用。",
                "在A2A基准测试中，成功率从0.47提升至0.54，导航误差略有增加。"
            ],
            "tags_zh": [
                "农业机器人导航",
                "视觉语言导航",
                "空间记忆",
                "3D重建",
                "A2A基准"
            ],
            "_index": 112
        },
        {
            "title": "Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration",
            "authors": [
                "Siddharth Tourani",
                "Jayaram Reddy",
                "Sarvesh Thakur",
                "K Madhava Krishna",
                "Muhammad Haris Khan",
                "N Dinesh Reddy"
            ],
            "arxiv_id": "2510.14354v1",
            "summary": "With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has\nbecome available. This prompts the question of how to utilize this data for\ngeometric reasoning of scenes. While many RGB-D registration meth- ods rely on\ngeometric and feature-based similarity, we take a different approach. We use\ncycle-consistent keypoints as salient points to enforce spatial coherence\nconstraints during matching, improving correspondence accuracy. Additionally,\nwe introduce a novel pose block that combines a GRU recurrent unit with\ntransformation synchronization, blending historical and multi-view data. Our\napproach surpasses previous self- supervised registration methods on ScanNet\nand 3DMatch, even outperforming some older supervised methods. We also\nintegrate our components into existing methods, showing their effectiveness.",
            "headline_zh": "提出基于循环一致锚点的自监督RGB-D配准方法，提升场景几何推理精度。",
            "intro_zh": [
                "核心问题：如何利用未标记RGB-D数据进行场景几何推理，避免依赖几何或特征相似性。",
                "方法要点：使用循环一致关键点增强空间一致性，结合GRU与变换同步融合历史与多视图数据。",
                "实验或效果：在ScanNet和3DMatch上超越先前自监督方法，部分优于旧有监督方法。"
            ],
            "tags_zh": [
                "RGB-D配准",
                "自监督学习",
                "循环一致性",
                "关键点检测",
                "变换同步",
                "多视图融合"
            ],
            "_index": 113
        },
        {
            "title": "Vision-Centric Activation and Coordination for Multimodal Large Language Models",
            "authors": [
                "Yunnan Wang",
                "Fan Lu",
                "Kecheng Zheng",
                "Ziyuan Huang",
                "Ziqiang Li",
                "Wenjun Zeng",
                "Xin Jin"
            ],
            "arxiv_id": "2510.14349v1",
            "summary": "Multimodal large language models (MLLMs) integrate image features from visual\nencoders with LLMs, demonstrating advanced comprehension capabilities. However,\nmainstream MLLMs are solely supervised by the next-token prediction of textual\ntokens, neglecting critical vision-centric information essential for analytical\nabilities. To track this dilemma, we introduce VaCo, which optimizes MLLM\nrepresentations through Vision-Centric activation and Coordination from\nmultiple vision foundation models (VFMs). VaCo introduces visual discriminative\nalignment to integrate task-aware perceptual features extracted from VFMs,\nthereby unifying the optimization of both textual and visual outputs in MLLMs.\nSpecifically, we incorporate the learnable Modular Task Queries (MTQs) and\nVisual Alignment Layers (VALs) into MLLMs, activating specific visual signals\nunder the supervision of diverse VFMs. To coordinate representation conflicts\nacross VFMs, the crafted Token Gateway Mask (TGM) restricts the information\nflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo\nsignificantly improves the performance of different MLLMs on various\nbenchmarks, showcasing its superior capabilities in visual comprehension.",
            "headline_zh": "提出VaCo方法以优化多模态大语言模型的视觉中心表示",
            "intro_zh": [
                "主流MLLMs仅监督文本标记预测，忽视视觉中心信息，影响分析能力。",
                "引入视觉判别对齐，整合多视觉基础模型特征，统一文本和视觉输出优化。",
                "实验显示VaCo显著提升多种MLLMs在基准测试中的视觉理解性能。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视觉中心激活",
                "视觉基础模型",
                "表示优化",
                "视觉理解"
            ],
            "_index": 114
        },
        {
            "title": "A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities",
            "authors": [
                "Siva Teja Kakileti",
                "Bharath Govindaraju",
                "Sudhakar Sampangi",
                "Geetha Manjunath"
            ],
            "arxiv_id": "2510.14340v1",
            "summary": "Mammography, the current standard for breast cancer screening, has reduced\nsensitivity in women with dense breast tissue, contributing to missed or\ndelayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures\nfunctional vascular and metabolic cues that may complement mammographic\nstructural data. This study investigates whether a breast density-informed\nmulti-modal AI framework can improve cancer detection by dynamically selecting\nthe appropriate imaging modality based on breast tissue composition. A total of\n324 women underwent both mammography and thermal imaging. Mammography images\nwere analyzed using a multi-view deep learning model, while Thermalytix\nassessed thermal images through vascular and thermal radiomics. The proposed\nframework utilized Mammography AI for fatty breasts and Thermalytix AI for\ndense breasts, optimizing predictions based on tissue type. This multi-modal AI\nframework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity\nof 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI\n(sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity\n92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography\ndropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%),\nwhereas Thermalytix AI maintained high and consistent sensitivity in both\n(92.59% and 92.86%, respectively). This demonstrates that a density-informed\nmulti-modal AI framework can overcome key limitations of unimodal screening and\ndeliver high performance across diverse breast compositions. The proposed\nframework is interpretable, low-cost, and easily deployable, offering a\npractical path to improving breast cancer screening outcomes in both\nhigh-resource and resource-limited settings.",
            "headline_zh": "提出基于乳腺密度的多模态AI框架，以提升不同密度乳腺癌检测性能",
            "intro_zh": [
                "乳腺X线摄影在致密乳腺组织中敏感性降低，导致漏诊或延迟诊断",
                "结合乳腺X线AI和热成像AI，根据乳腺密度动态选择最优模态",
                "多模态框架敏感性达94.55%，优于单模态方法，尤其在致密乳腺中表现稳定"
            ],
            "tags_zh": [
                "乳腺癌检测",
                "多模态AI",
                "乳腺密度",
                "热成像",
                "深度学习",
                "放射组学"
            ],
            "_index": 115
        },
        {
            "title": "Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion",
            "authors": [
                "Yuanhong Zeng",
                "Anushri Dixit"
            ],
            "arxiv_id": "2510.14338v1",
            "summary": "In this work, we study risk-aware reinforcement learning for quadrupedal\nlocomotion. Our approach trains a family of risk-conditioned policies using a\nConditional Value-at-Risk (CVaR) constrained policy optimization technique that\nprovides improved stability and sample efficiency. At deployment, we adaptively\nselect the best performing policy from the family of policies using a\nmulti-armed bandit framework that uses only observed episodic returns, without\nany privileged environment information, and adapts to unknown conditions on the\nfly. Hence, we train quadrupedal locomotion policies at various levels of\nrobustness using CVaR and adaptively select the desired level of robustness\nonline to ensure performance in unknown environments. We evaluate our method in\nsimulation across eight unseen settings (by changing dynamics, contacts,\nsensing noise, and terrain) and on a Unitree Go2 robot in previously unseen\nterrains. Our risk-aware policy attains nearly twice the mean and tail\nperformance in unseen environments compared to other baselines and our\nbandit-based adaptation selects the best-performing risk-aware policy in\nunknown terrain within two minutes of operation.",
            "headline_zh": "提出风险感知强化学习与多臂老虎机自适应方法，以提升四足机器人在未知环境中的运动性能。",
            "intro_zh": [
                "核心问题：四足机器人在未知环境中运动时，面临稳定性与性能下降的风险。",
                "方法要点：使用CVaR约束策略优化训练风险条件策略族，并通过多臂老虎机在线自适应选择最优策略。",
                "实验或效果：在仿真和真实机器人测试中，性能优于基线，自适应选择在未知地形中快速生效。"
            ],
            "tags_zh": [
                "风险感知强化学习",
                "四足机器人运动",
                "CVaR约束优化",
                "多臂老虎机",
                "策略自适应",
                "未知环境适应"
            ],
            "_index": 116
        },
        {
            "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection",
            "authors": [
                "Shivangi Yadav",
                "Arun Ross"
            ],
            "arxiv_id": "2510.14314v1",
            "summary": "An iris biometric system can be compromised by presentation attacks (PAs)\nwhere artifacts such as artificial eyes, printed eye images, or cosmetic\ncontact lenses are presented to the system. To counteract this, several\npresentation attack detection (PAD) methods have been developed. However, there\nis a scarcity of datasets for training and evaluating iris PAD techniques due\nto the implicit difficulties in constructing and imaging PAs. To address this,\nwe introduce the Multi-domain Image Translative Diffusion StyleGAN\n(MID-StyleGAN), a new framework for generating synthetic ocular images that\ncaptures the PA and bonafide characteristics in multiple domains such as\nbonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the\nstrengths of diffusion models and generative adversarial networks (GANs) to\nproduce realistic and diverse synthetic data. Our approach utilizes a\nmulti-domain architecture that enables the translation between bonafide ocular\nimages and different PA domains. The model employs an adaptive loss function\ntailored for ocular data to maintain domain consistency. Extensive experiments\ndemonstrate that MID-StyleGAN outperforms existing methods in generating\nhigh-quality synthetic ocular images. The generated data was used to\nsignificantly enhance the performance of PAD systems, providing a scalable\nsolution to the data scarcity problem in iris and ocular biometrics. For\nexample, on the LivDet2020 dataset, the true detect rate at 1% false detect\nrate improved from 93.41% to 98.72%, showcasing the impact of the proposed\nmethod.",
            "headline_zh": "提出MID-StyleGAN以解决虹膜呈现攻击检测中数据稀缺问题",
            "intro_zh": [
                "核心问题：虹膜生物识别系统易受呈现攻击，但缺乏训练数据。",
                "方法要点：结合扩散模型与GAN，生成多领域合成眼部图像。",
                "实验或效果：在LivDet2020数据集上，真检测率从93.41%提升至98.72%。"
            ],
            "tags_zh": [
                "虹膜呈现攻击检测",
                "合成数据生成",
                "多领域图像翻译",
                "扩散模型",
                "生成对抗网络"
            ],
            "_index": 117
        },
        {
            "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding",
            "authors": [
                "Kyungryul Back",
                "Seongbeom Park",
                "Milim Kim",
                "Mincheol Kwon",
                "SangHyeok Lee",
                "Hyunyoung Lee",
                "Junhee Cho",
                "Seunghyun Park",
                "Jinkyu Kim"
            ],
            "arxiv_id": "2510.14304v1",
            "summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on\nvarious multimodal tasks, even achieving human-comparable performance in\ncertain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often\nrely heavily on a single modality or memorize training data without properly\ngrounding their outputs. To address this, we propose a training-free, tri-layer\ncontrastive decoding with watermarking, which proceeds in three steps: (1)\nselect a mature layer and an amateur layer among the decoding layers, (2)\nidentify a pivot layer using a watermark-related question to assess whether the\nlayer is visually well-grounded, and (3) apply tri-layer contrastive decoding\nto generate the final output. Experiments on public benchmarks such as POPE,\nMME and AMBER demonstrate that our method achieves state-of-the-art performance\nin reducing hallucinations in LVLMs and generates more visually grounded\nresponses.",
            "headline_zh": "提出基于水印的三层对比解码方法以减少视觉语言模型的幻觉问题",
            "intro_zh": [
                "核心问题：视觉语言模型易产生幻觉，依赖单一模态或记忆训练数据",
                "方法要点：无训练三层对比解码，选择成熟与业余层，利用水印问题识别视觉基础层",
                "实验或效果：在POPE、MME和AMBER基准上实现最先进性能，减少幻觉并增强视觉基础"
            ],
            "tags_zh": [
                "视觉语言模型",
                "幻觉减少",
                "对比解码",
                "水印技术",
                "无训练方法",
                "视觉基础"
            ],
            "_index": 118
        },
        {
            "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning",
            "authors": [
                "Weijie Shen",
                "Yitian Liu",
                "Yuhao Wu",
                "Zhixuan Liang",
                "Sijia Gu",
                "Dehui Wang",
                "Tian Nian",
                "Lei Xu",
                "Yusen Qin",
                "Jiangmiao Pang",
                "Xinping Guan",
                "Xiaokang Yang",
                "Yao Mu"
            ],
            "arxiv_id": "2510.14300v1",
            "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.",
            "headline_zh": "提出AdaMoE以解决视觉-语言-动作模型扩展中的计算效率与性能平衡问题",
            "intro_zh": [
                "核心问题：VLA模型扩展需高计算资源与稀缺机器人数据，实时控制需平衡模型容量与效率",
                "方法要点：采用MoE架构继承预训练权重，通过解耦专家选择与权重实现协作专家利用",
                "实验或效果：在LIBERO和RoboTwin基准上性能提升1.8%和9.3%，真实世界实验提升21.5%"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "专家混合架构",
                "机器人操作",
                "稀疏激活",
                "实时控制",
                "模型扩展"
            ],
            "_index": 119
        },
        {
            "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying",
            "authors": [
                "Yushi Du",
                "Yixuan Li",
                "Baoxiong Jia",
                "Yutang Lin",
                "Pei Zhou",
                "Wei Liang",
                "Yanchao Yang",
                "Siyuan Huang"
            ],
            "arxiv_id": "2510.14293v1",
            "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment.",
            "headline_zh": "提出COLA方法实现人形机器人与人类协作搬运，无需外部传感器",
            "intro_zh": [
                "核心问题：人形机器人全身动力学复杂，难以实现柔顺人机协作搬运",
                "方法要点：使用仅本体感知的强化学习，单策略结合领导与跟随行为",
                "实验或效果：仿真和真实实验显示减少人类努力24.7%，泛化性强"
            ],
            "tags_zh": [
                "人形机器人协作",
                "强化学习",
                "本体感知",
                "轨迹规划",
                "负载平衡"
            ],
            "_index": 120
        },
        {
            "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts",
            "authors": [
                "Kieu-Anh Truong Thi",
                "Huy-Hieu Pham",
                "Duc-Trong Le"
            ],
            "arxiv_id": "2510.14273v1",
            "summary": "Domain shift in histopathology, often caused by differences in acquisition\nprocesses or data sources, poses a major challenge to the generalization\nability of deep learning models. Existing methods primarily rely on modeling\nstatistical correlations by aligning feature distributions or introducing\nstatistical variation, yet they often overlook causal relationships. In this\nwork, we propose a novel causal-inference-based framework that leverages\nsemantic features while mitigating the impact of confounders. Our method\nimplements the front-door principle by designing transformation strategies that\nexplicitly incorporate mediators and observed tissue slides. We validate our\nmethod on the CAMELYON17 dataset and a private histopathology dataset,\ndemonstrating consistent performance gains across unseen domains. As a result,\nour approach achieved up to a 7% improvement in both the CAMELYON17 dataset and\nthe private histopathology dataset, outperforming existing baselines. These\nresults highlight the potential of causal inference as a powerful tool for\naddressing domain shift in histopathology image analysis.",
            "headline_zh": "提出因果学习框架CLEAR以解决组织病理学肿瘤检测中的域偏移问题",
            "intro_zh": [
                "核心问题：组织病理学图像因采集过程或数据源差异导致域偏移，影响深度学习模型泛化能力",
                "方法要点：基于因果推断，利用前门原则设计变换策略，结合中介变量和观察组织切片",
                "实验或效果：在CAMELYON17和私有数据集上验证，域外性能提升达7%，优于基线方法"
            ],
            "tags_zh": [
                "因果推断",
                "组织病理学图像分析",
                "域偏移",
                "肿瘤检测",
                "前门原则"
            ],
            "_index": 121
        },
        {
            "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering",
            "authors": [
                "Alexander Valverde",
                "Brian Xu",
                "Yuyin Zhou",
                "Meng Xu",
                "Hongyun Wang"
            ],
            "arxiv_id": "2510.14270v1",
            "summary": "Scene reconstruction has emerged as a central challenge in computer vision,\nwith approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting\nachieving remarkable progress. While Gaussian Splatting demonstrates strong\nperformance on large-scale datasets, it often struggles to capture fine details\nor maintain realism in regions with sparse coverage, largely due to the\ninherent limitations of sparse 3D training data.\n  In this work, we propose GauSSmart, a hybrid method that effectively bridges\n2D foundational models and 3D Gaussian Splatting reconstruction. Our approach\nintegrates established 2D computer vision techniques, including convex\nfiltering and semantic feature supervision from foundational models such as\nDINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D\nsegmentation priors and high-dimensional feature embeddings, our method guides\nthe densification and refinement of Gaussian splats, improving coverage in\nunderrepresented areas and preserving intricate structural details.\n  We validate our approach across three datasets, where GauSSmart consistently\noutperforms existing Gaussian Splatting in the majority of evaluated scenes.\nOur results demonstrate the significant potential of hybrid 2D-3D approaches,\nhighlighting how the thoughtful combination of 2D foundational models with 3D\nreconstruction pipelines can overcome the limitations inherent in either\napproach alone.",
            "headline_zh": "提出GauSSmart，通过2D基础模型和几何过滤增强3D高斯溅射重建",
            "intro_zh": [
                "核心问题：高斯溅射在稀疏3D数据下难以捕捉细节和保持真实感",
                "方法要点：集成2D基础模型特征和几何过滤，指导高斯溅射的密度化和细化",
                "实验或效果：在多个数据集上优于现有高斯溅射方法，提升覆盖率和细节保留"
            ],
            "tags_zh": [
                "3D场景重建",
                "高斯溅射",
                "2D基础模型",
                "几何过滤",
                "语义特征监督"
            ],
            "_index": 122
        },
        {
            "title": "Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment",
            "authors": [
                "Miu Sumino",
                "Mayu Ishii",
                "Shun Kaizu",
                "Daisuke Hisano",
                "Yu Nakayama"
            ],
            "arxiv_id": "2510.14266v1",
            "summary": "We propose a robust demodulation scheme for optical camera communication\nsystems using an event-based vision sensor, combining OOK with toggle\ndemodulation and a digital phase-locked loop. This is the first report to\nachieve a $\\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor\nexperiments.",
            "headline_zh": "提出基于事件视觉传感器的稳健解调方案，在长距离户外环境中实现低误码率光相机通信。",
            "intro_zh": [
                "核心问题：长距离户外环境下的光相机通信系统稳健性不足。",
                "方法要点：结合OOK、切换解调和数字锁相环的稳健解调方案。",
                "实验或效果：在200米60kbps和400米30kbps下实现误码率低于10^{-3}。"
            ],
            "tags_zh": [
                "事件视觉传感器",
                "光相机通信",
                "稳健解调",
                "长距离通信",
                "户外实验",
                "误码率优化"
            ],
            "_index": 123
        },
        {
            "title": "MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching",
            "authors": [
                "Tingman Yan",
                "Tao Liu",
                "Xilian Yang",
                "Qunfei Zhao",
                "Zeyang Xia"
            ],
            "arxiv_id": "2510.14260v1",
            "summary": "Cross-view matching is fundamentally achieved through cross-attention\nmechanisms. However, matching of high-resolution images remains challenging due\nto the quadratic complexity and lack of explicit matching constraints in the\nexisting cross-attention. This paper proposes an attention mechanism,\nMatchAttention, that dynamically matches relative positions. The relative\nposition determines the attention sampling center of the key-value pairs given\na query. Continuous and differentiable sliding-window attention sampling is\nachieved by the proposed BilinearSoftmax. The relative positions are\niteratively updated through residual connections across layers by embedding\nthem into the feature channels. Since the relative position is exactly the\nlearning target for cross-view matching, an efficient hierarchical cross-view\ndecoder, MatchDecoder, is designed with MatchAttention as its core component.\nTo handle cross-view occlusions, gated cross-MatchAttention and a\nconsistency-constrained loss are proposed. These two components collectively\nmitigate the impact of occlusions in both forward and backward passes, allowing\nthe model to focus more on learning matching relationships. When applied to\nstereo matching, MatchStereo-B ranked 1st in average error on the public\nMiddlebury benchmark and requires only 29ms for KITTI-resolution inference.\nMatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU\nmemory. The proposed models also achieve state-of-the-art performance on KITTI\n2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high\naccuracy and low computational complexity makes real-time, high-resolution, and\nhigh-accuracy cross-view matching possible. Code is available at\nhttps://github.com/TingmanYan/MatchAttention.",
            "headline_zh": "提出MatchAttention机制以解决高分辨率跨视图匹配的效率和精度问题",
            "intro_zh": [
                "核心问题：高分辨率图像跨视图匹配存在二次复杂度和缺乏显式匹配约束的挑战",
                "方法要点：通过BilinearSoftmax实现连续可微滑动窗口注意力采样，迭代更新相对位置",
                "实验或效果：在多个基准数据集上达到SOTA，支持实时高分辨率处理，如4K图像0.1秒内推理"
            ],
            "tags_zh": [
                "跨视图匹配",
                "注意力机制",
                "立体匹配",
                "高分辨率图像处理",
                "实时推理"
            ],
            "_index": 124
        },
        {
            "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning",
            "authors": [
                "Xiangyu Meng",
                "Zixian Zhang",
                "Zhenghao Zhang",
                "Junchao Liao",
                "Long Qin",
                "Weizhi Wang"
            ],
            "arxiv_id": "2510.14256v1",
            "summary": "While advanced methods like VACE and Phantom have advanced video generation\nfor specific subjects in diverse scenarios, they struggle with multi-human\nidentity preservation in dynamic interactions, where consistent identities\nacross multiple characters are critical. To address this, we propose\nIdentity-GRPO, a human feedback-driven optimization pipeline for refining\nmulti-human identity-preserving video generation. First, we construct a video\nreward model trained on a large-scale preference dataset containing\nhuman-annotated and synthetic distortion data, with pairwise annotations\nfocused on maintaining human consistency throughout the video. We then employ a\nGRPO variant tailored for multi-human consistency, which greatly enhances both\nVACE and Phantom. Through extensive ablation studies, we evaluate the impact of\nannotation quality and design choices on policy optimization. Experiments show\nthat Identity-GRPO achieves up to 18.9% improvement in human consistency\nmetrics over baseline methods, offering actionable insights for aligning\nreinforcement learning with personalized video generation.",
            "headline_zh": "提出Identity-GRPO以优化多人类身份保持的视频生成",
            "intro_zh": [
                "多人类动态交互中身份一致性难以保持，影响视频生成质量",
                "基于人类反馈构建视频奖励模型，并采用GRPO变体优化生成策略",
                "实验显示身份一致性指标提升18.9%，优于基线方法"
            ],
            "tags_zh": [
                "视频生成",
                "身份保持",
                "强化学习",
                "人类反馈",
                "多人类交互"
            ],
            "_index": 125
        },
        {
            "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization",
            "authors": [
                "Liao Shen",
                "Wentao Jiang",
                "Yiran Zhu",
                "Tiezheng Ge",
                "Zhiguo Cao",
                "Bo Zheng"
            ],
            "arxiv_id": "2510.14255v1",
            "summary": "Recent advances in image-to-video (I2V) generation have achieved remarkable\nprogress in synthesizing high-quality, temporally coherent videos from static\nimages. Among all the applications of I2V, human-centric video generation\nincludes a large portion. However, existing I2V models encounter difficulties\nin maintaining identity consistency between the input human image and the\ngenerated video, especially when the person in the video exhibits significant\nexpression changes and movements. This issue becomes critical when the human\nface occupies merely a small fraction of the image. Since humans are highly\nsensitive to identity variations, this poses a critical yet under-explored\nchallenge in I2V generation. In this paper, we propose Identity-Preserving\nReward-guided Optimization (IPRO), a novel video diffusion framework based on\nreinforcement learning to enhance identity preservation. Instead of introducing\nauxiliary modules or altering model architectures, our approach introduces a\ndirect and effective tuning algorithm that optimizes diffusion models using a\nface identity scorer. To improve performance and accelerate convergence, our\nmethod backpropagates the reward signal through the last steps of the sampling\nchain, enabling richer gradient feedback. We also propose a novel facial\nscoring mechanism that treats faces in ground-truth videos as facial feature\npools, providing multi-angle facial information to enhance generalization. A\nKL-divergence regularization is further incorporated to stabilize training and\nprevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V\nmodel and our in-house I2V model demonstrate the effectiveness of our method.\nOur project and code are available at\n\\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.",
            "headline_zh": "提出身份保持奖励引导优化方法以解决图像到视频生成中身份一致性问题",
            "intro_zh": [
                "核心问题：现有图像到视频模型在人物表情和运动变化时难以保持输入图像与生成视频的身份一致性",
                "方法要点：基于强化学习优化扩散模型，使用面部身份评分器并通过采样链后向传播奖励信号",
                "实验或效果：在Wan 2.2和内部模型上验证有效性，结合KL散度正则化防止过拟合"
            ],
            "tags_zh": [
                "图像到视频生成",
                "身份保持",
                "奖励引导优化",
                "扩散模型",
                "强化学习",
                "面部评分机制"
            ],
            "_index": 126
        },
        {
            "title": "MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering",
            "authors": [
                "Mingkai Liu",
                "Dikai Fan",
                "Haohua Que",
                "Haojia Gao",
                "Xiao Liu",
                "Shuxue Peng",
                "Meixia Lin",
                "Shengyu Gu",
                "Ruicong Ye",
                "Wanli Qiu",
                "Handong Yao",
                "Ruopeng Zhang",
                "Xianliang Huang"
            ],
            "arxiv_id": "2510.14251v1",
            "summary": "Efficient localization and high-quality rendering in large-scale scenes\nremain a significant challenge due to the computational cost involved. While\nScene Coordinate Regression (SCR) methods perform well in small-scale\nlocalization, they are limited by the capacity of a single network when\nextended to large-scale scenes. To address these challenges, we propose the\nMixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables\nefficient localization and high-quality rendering in large-scale scenes.\nInspired by the remarkable capabilities of MOE in large model domains, we\nintroduce a gating network to implicitly classify and select sub-networks,\nensuring that only a single sub-network is activated during each inference.\nFurtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to\nenhance the localization accuracy on large-scale scene. Our framework provides\na significant reduction in costs while maintaining higher precision, offering\nan efficient solution for large-scale scene applications. Additional\nexperiments on the Cambridge test set demonstrate that our method achieves\nhigh-quality rendering results with merely 10 minutes of training.",
            "headline_zh": "提出MACE方法以解决大规模场景定位与渲染中的计算效率问题",
            "intro_zh": [
                "核心问题：大规模场景定位与渲染计算成本高，单网络容量不足。",
                "方法要点：引入门控网络选择子网络，仅激活单个子网络进行推理。",
                "实验效果：在剑桥测试集上，仅10分钟训练即可实现高质量渲染。"
            ],
            "tags_zh": [
                "场景坐标回归",
                "混合专家模型",
                "门控网络",
                "负载均衡",
                "大规模场景定位",
                "高效渲染"
            ],
            "_index": 127
        },
        {
            "title": "Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication",
            "authors": [
                "Miu Sumino",
                "Mayu Ishii",
                "Shun Kaizu",
                "Daisuke Hisano",
                "Yu Nakayama"
            ],
            "arxiv_id": "2510.14245v1",
            "summary": "Optical camera communication (OCC) represents a promising visible light\ncommunication technology. Nonetheless, typical OCC systems utilizing\nframe-based cameras are encumbered by limitations, including low bit rate and\nhigh processing load. To address these issues, OCC system utilizing an\nevent-based vision sensor (EVS) as receivers have been proposed. The EVS\nenables high-speed, low-latency, and robust communication due to its\nasynchronous operation and high dynamic range. In existing event-based OCC\nsystems, conventional modulation schemes such as on-off keying (OOK) and pulse\nposition modulation have been applied, however, to the best of our knowledge,\nno modulation method has been proposed that fully exploits the unique\ncharacteristics of the EVS. This paper proposes a novel modulation scheme,\ncalled the event interval modulation (EIM) scheme, specifically designed for\nevent-based OCC. EIM enables improvement in transmission speed by modulating\ninformation using the intervals between events. This paper proposes a\ntheoretical model of EIM and conducts a proof-of-concept experiment. First, the\nparameters of the EVS are tuned and customized to optimize the frequency\nresponse specifically for EIM. Then, the maximum modulation order usable in EIM\nis determined experimentally. We conduct transmission experiments based on the\nobtained parameters. Finally, we report successful transmission at 28 kbps over\n10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new\nbenchmark for bit rate in event-based OCC systems.",
            "headline_zh": "提出事件间隔调制方案以提升事件式光学相机通信的传输速度",
            "intro_zh": [
                "传统光学相机通信系统存在低比特率和高处理负载问题",
                "新方案利用事件间隔调制信息，优化传感器参数以提升频率响应",
                "实验实现室内28 kbps/10米和8.4 kbps/50米传输，创事件式OCC新纪录"
            ],
            "tags_zh": [
                "事件式视觉传感器",
                "光学相机通信",
                "事件间隔调制",
                "高比特率传输",
                "室内通信实验"
            ],
            "_index": 128
        },
        {
            "title": "Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation",
            "authors": [
                "Arnaud Judge",
                "Nicolas Duchateau",
                "Thierry Judge",
                "Roman A. Sandler",
                "Joseph Z. Sokol",
                "Christian Desrosiers",
                "Olivier Bernard",
                "Pierre-Marc Jodoin"
            ],
            "arxiv_id": "2510.14244v1",
            "summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling\nknowledge transfer across domains, reducing the need for additional expert\nannotations. However, many approaches struggle with reliability in the target\ndomain, an issue particularly critical in medical image segmentation, where\naccuracy and anatomical validity are essential. This challenge is further\nexacerbated in spatio-temporal data, where the lack of temporal consistency can\nsignificantly degrade segmentation quality, and particularly in\nechocardiography, where the presence of artifacts and noise can further hinder\nsegmentation performance. To address these issues, we present RL4Seg3D, an\nunsupervised domain adaptation framework for 2D + time echocardiography\nsegmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to\nenhance key landmark precision in its segmentations while processing full-sized\ninput videos. By leveraging reinforcement learning for image segmentation, our\napproach improves accuracy, anatomical validity, and temporal consistency while\nalso providing, as a beneficial side effect, a robust uncertainty estimator,\nwhich can be used at test time to further enhance segmentation performance. We\ndemonstrate the effectiveness of our framework on over 30,000 echocardiographic\nvideos, showing that it outperforms standard domain adaptation techniques\nwithout the need for any labels on the target domain. Code is available at\nhttps://github.com/arnaudjudge/RL4Seg3D.",
            "headline_zh": "提出RL4Seg3D框架，用于无监督领域自适应的心动图时空分割。",
            "intro_zh": [
                "核心问题：领域自适应方法在目标域可靠性低，心动图分割受噪声和时空不一致影响。",
                "方法要点：集成强化学习、新奖励函数和融合方案，提升分割精度和时空一致性。",
                "实验或效果：在3万+心动图视频上验证，优于标准方法，无需目标域标签。"
            ],
            "tags_zh": [
                "无监督领域自适应",
                "心动图分割",
                "强化学习",
                "时空一致性",
                "不确定性估计"
            ],
            "_index": 129
        },
        {
            "title": "PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis",
            "authors": [
                "Soumyya Kanti Datta",
                "Tanvi Ranga",
                "Chengzhe Sun",
                "Siwei Lyu"
            ],
            "arxiv_id": "2510.14241v1",
            "summary": "The rise of manipulated media has made deepfakes a particularly insidious\nthreat, involving various generative manipulations such as lip-sync\nmodifications, face-swaps, and avatar-driven facial synthesis. Conventional\ndetection methods, which predominantly depend on manually designed\nphoneme-viseme alignment thresholds, fundamental frame-level consistency\nchecks, or a unimodal detection strategy, inadequately identify modern-day\ndeepfakes generated by advanced generative models such as GANs, diffusion\nmodels, and neural rendering techniques. These advanced techniques generate\nnearly perfect individual frames yet inadvertently create minor temporal\ndiscrepancies frequently overlooked by traditional detectors. We present a\nnovel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic\nAnalysis(PIA), incorporating language, dynamic face motion, and facial\nidentification cues to address these limitations. We utilize phoneme sequences,\nlip geometry data, and advanced facial identity embeddings. This integrated\nmethod significantly improves the detection of subtle deepfake alterations by\nidentifying inconsistencies across multiple complementary modalities. Code is\navailable at https://github.com/skrantidatta/PIA",
            "headline_zh": "提出PIA框架，通过多模态分析检测高级生成模型产生的深度伪造视频。",
            "intro_zh": [
                "核心问题：传统检测方法难以识别基于GAN、扩散模型等生成的深度伪造视频中的细微时间不一致。",
                "方法要点：结合音素序列、唇部几何和面部身份嵌入，进行多模态不一致性分析。",
                "实验或效果：该方法显著提升了对细微深度伪造篡改的检测性能，代码已开源。"
            ],
            "tags_zh": [
                "深度伪造检测",
                "多模态分析",
                "音素-时间分析",
                "身份动态分析",
                "音频-视觉框架"
            ],
            "_index": 130
        },
        {
            "title": "Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space",
            "authors": [
                "Ning Han",
                "Gu Gong",
                "Bin Zhang",
                "Yuexuan Xu",
                "Bohan Yang",
                "Yunhui Liu",
                "David Navarro-Alarcon"
            ],
            "arxiv_id": "2510.14234v1",
            "summary": "Manipulating three-dimensional (3D) deformable objects presents significant\nchallenges for robotic systems due to their infinite-dimensional state space\nand complex deformable dynamics. This paper proposes a novel model-free\napproach for shape control with constraints imposed on key points. Unlike\nexisting methods that rely on feature dimensionality reduction, the proposed\ncontroller leverages the coordinates of key points as the feature vector, which\nare extracted from the deformable object's point cloud using deep learning\nmethods. This approach not only reduces the dimensionality of the feature space\nbut also retains the spatial information of the object. By extracting key\npoints, the manipulation of deformable objects is simplified into a visual\nservoing problem, where the shape dynamics are described using a deformation\nJacobian matrix. To enhance control accuracy, a prescribed performance control\nmethod is developed by integrating barrier Lyapunov functions (BLF) to enforce\nconstraints on the key points. The stability of the closed-loop system is\nrigorously analyzed and verified using the Lyapunov method. Experimental\nresults further demonstrate the effectiveness and robustness of the proposed\nmethod.",
            "headline_zh": "提出基于关键点与规定性能控制的模型无关方法，用于三维可变形物体形状控制。",
            "intro_zh": [
                "核心问题：三维可变形物体操纵因无限维状态空间和复杂变形动力学而具挑战性。",
                "方法要点：使用深度学习提取关键点坐标作为特征，结合变形雅可比矩阵和障碍李雅普诺夫函数进行控制。",
                "实验或效果：实验验证了方法的有效性和鲁棒性，闭环系统稳定性通过李雅普诺夫方法分析。"
            ],
            "tags_zh": [
                "可变形物体操纵",
                "规定性能控制",
                "关键点提取",
                "视觉伺服",
                "障碍李雅普诺夫函数",
                "变形雅可比矩阵"
            ],
            "_index": 131
        },
        {
            "title": "LOTA: Bit-Planes Guided AI-Generated Image Detection",
            "authors": [
                "Hongsong Wang",
                "Renxi Cheng",
                "Yang Zhang",
                "Chaolei Han",
                "Jie Gui"
            ],
            "arxiv_id": "2510.14230v1",
            "summary": "The rapid advancement of GAN and Diffusion models makes it more difficult to\ndistinguish AI-generated images from real ones. Recent studies often use\nimage-based reconstruction errors as an important feature for determining\nwhether an image is AI-generated. However, these approaches typically incur\nhigh computational costs and also fail to capture intrinsic noisy features\npresent in the raw images. To solve these problems, we innovatively refine\nerror extraction by using bit-plane-based image processing, as lower bit planes\nindeed represent noise patterns in images. We introduce an effective bit-planes\nguided noisy image generation and exploit various image normalization\nstrategies, including scaling and thresholding. Then, to amplify the noise\nsignal for easier AI-generated image detection, we design a maximum gradient\npatch selection that applies multi-directional gradients to compute the noise\nscore and selects the region with the highest score. Finally, we propose a\nlightweight and effective classification head and explore two different\nstructures: noise-based classifier and noise-guided classifier. Extensive\nexperiments on the GenImage benchmark demonstrate the outstanding performance\nof our method, which achieves an average accuracy of \\textbf{98.9\\%}\n(\\textbf{11.9}\\%~$\\uparrow$) and shows excellent cross-generator generalization\ncapability. Particularly, our method achieves an accuracy of over 98.2\\% from\nGAN to Diffusion and over 99.2\\% from Diffusion to GAN. Moreover, it performs\nerror extraction at the millisecond level, nearly a hundred times faster than\nexisting methods. The code is at https://github.com/hongsong-wang/LOTA.",
            "headline_zh": "提出基于位平面的AI生成图像检测方法，以解决高计算成本和噪声特征捕获不足问题。",
            "intro_zh": [
                "核心问题：GAN和扩散模型使AI生成图像难以区分，现有方法计算成本高且忽略原始噪声特征。",
                "方法要点：利用位平面提取噪声，设计最大梯度补丁选择和轻量分类头以增强检测。",
                "实验或效果：在GenImage基准上平均准确率达98.9%，速度快且跨生成器泛化能力强。"
            ],
            "tags_zh": [
                "AI生成图像检测",
                "位平面处理",
                "噪声特征提取",
                "轻量分类器",
                "跨生成器泛化"
            ],
            "_index": 132
        },
        {
            "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition",
            "authors": [
                "Ryo Masumura",
                "Shota Orihashi",
                "Mana Ihori",
                "Tomohiro Tanaka",
                "Naoki Makishima",
                "Taiga Yamane",
                "Naotaka Kawata",
                "Satoshi Suzuki",
                "Taichi Katayama"
            ],
            "arxiv_id": "2510.14203v1",
            "summary": "This paper proposes a joint modeling method of the Big Five, which has long\nbeen studied, and HEXACO, which has recently attracted attention in psychology,\nfor automatically recognizing apparent personality traits from multimodal human\nbehavior. Most previous studies have used the Big Five for multimodal apparent\npersonality-trait recognition. However, no study has focused on apparent HEXACO\nwhich can evaluate an Honesty-Humility trait related to displaced aggression\nand vengefulness, social-dominance orientation, etc. In addition, the\nrelationships between the Big Five and HEXACO when modeled by machine learning\nhave not been clarified. We expect awareness of multimodal human behavior to\nimprove by considering these relationships. The key advance of our proposed\nmethod is to optimize jointly recognizing the Big Five and HEXACO. Experiments\nusing a self-introduction video dataset demonstrate that the proposed method\ncan effectively recognize the Big Five and HEXACO.",
            "headline_zh": "提出联合建模Big Five和HEXACO的方法，用于多模态表观人格特质识别。",
            "intro_zh": [
                "核心问题：现有研究未关注HEXACO模型中的Honesty-Humility特质及其与Big Five的关系。",
                "方法要点：通过联合优化同时识别Big Five和HEXACO，以改进多模态人类行为建模。",
                "实验或效果：在自我介绍视频数据集上验证，能有效识别Big Five和HEXACO。"
            ],
            "tags_zh": [
                "多模态人格识别",
                "Big Five模型",
                "HEXACO模型",
                "联合建模",
                "视频行为分析"
            ],
            "_index": 133
        },
        {
            "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures",
            "authors": [
                "Yuancheng Xu",
                "Wenqi Xian",
                "Li Ma",
                "Julien Philip",
                "Ahmet Levent Taşel",
                "Yiwei Zhao",
                "Ryan Burgert",
                "Mingming He",
                "Oliver Hermann",
                "Oliver Pilarski",
                "Rahul Garg",
                "Paul Debevec",
                "Ning Yu"
            ],
            "arxiv_id": "2510.14179v1",
            "summary": "We introduce a framework that enables both multi-view character consistency\nand 3D camera control in video diffusion models through a novel customization\ndata pipeline. We train the character consistency component with recorded\nvolumetric capture performances re-rendered with diverse camera trajectories\nvia 4D Gaussian Splatting (4DGS), lighting variability obtained with a video\nrelighting model. We fine-tune state-of-the-art open-source video diffusion\nmodels on this data to provide strong multi-view identity preservation, precise\ncamera control, and lighting adaptability. Our framework also supports core\ncapabilities for virtual production, including multi-subject generation using\ntwo approaches: joint training and noise blending, the latter enabling\nefficient composition of independently customized models at inference time; it\nalso achieves scene and real-life video customization as well as control over\nmotion and spatial layout during customization. Extensive experiments show\nimproved video quality, higher personalization accuracy, and enhanced camera\ncontrol and lighting adaptability, advancing the integration of video\ngeneration into virtual production. Our project page is available at:\nhttps://eyeline-labs.github.io/Virtually-Being.",
            "headline_zh": "提出定制化视频扩散模型框架，实现多视角角色一致性和3D相机控制，用于虚拟制作。",
            "intro_zh": [
                "核心问题：视频扩散模型在多视角角色一致性和3D相机控制方面存在不足。",
                "方法要点：使用4D高斯溅射和视频重光照模型构建定制数据管道，微调模型。",
                "实验或效果：实验显示视频质量、个性化精度、相机控制和光照适应性提升。"
            ],
            "tags_zh": [
                "视频扩散模型",
                "多视角一致性",
                "3D相机控制",
                "虚拟制作",
                "4D高斯溅射",
                "视频重光照"
            ],
            "_index": 134
        }
    ]
}