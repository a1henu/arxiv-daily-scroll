{
    "papers": [
        {
            "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams",
            "authors": [
                "Takuya Nakabayashi",
                "Navami Kairanda",
                "Hideo Saito",
                "Vladislav Golyanik"
            ],
            "arxiv_id": "2510.11717v1",
            "summary": "Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/.",
            "headline_zh": "提出Ev4DGS方法，从单目事件流渲染非刚性物体的新视角",
            "intro_zh": [
                "核心问题：现有方法需额外RGB输入，限制了非刚性物体新视角渲染的实用性",
                "方法要点：通过损失函数和粗3D变形模型，回归可变形3D高斯泼溅表示",
                "实验或效果：在合成和真实数据集上验证有效性，优于多个基线方法"
            ],
            "tags_zh": [
                "事件相机",
                "新视角渲染",
                "非刚性物体",
                "3D高斯泼溅",
                "单目事件流"
            ],
            "_index": 0
        },
        {
            "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images",
            "authors": [
                "Chengqi Duan",
                "Kaiyue Sun",
                "Rongyao Fang",
                "Manyuan Zhang",
                "Yan Feng",
                "Ying Luo",
                "Yufang Liu",
                "Ke Wang",
                "Peng Pei",
                "Xunliang Cai",
                "Hongsheng Li",
                "Yi Ma",
                "Xihui Liu"
            ],
            "arxiv_id": "2510.11718v1",
            "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
            "headline_zh": "提出CodePlot-CoT方法，通过代码驱动图像解决数学视觉推理问题",
            "intro_zh": [
                "核心问题：LLMs和VLMs在需要视觉辅助的数学推理中缺乏精确性和可控性。",
                "方法要点：利用VLM生成文本推理和可执行绘图代码，渲染图像作为视觉思维。",
                "实验或效果：在Math-VR基准上，模型性能比基线提升高达21%。"
            ],
            "tags_zh": [
                "数学视觉推理",
                "代码驱动推理",
                "多模态学习",
                "数据集构建",
                "图像到代码转换"
            ],
            "_index": 1
        },
        {
            "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
            "authors": [
                "Ayush Shrivastava",
                "Sanyam Mehta",
                "Daniel Geng",
                "Andrew Owens"
            ],
            "arxiv_id": "2510.11715v1",
            "summary": "Trackers and video generators solve closely related problems: the former\nanalyze motion, while the latter synthesize it. We show that this connection\nenables pretrained video diffusion models to perform zero-shot point tracking\nby simply prompting them to visually mark points as they move over time. We\nplace a distinctively colored marker at the query point, then regenerate the\nrest of the video from an intermediate noise level. This propagates the marker\nacross frames, tracing the point's trajectory. To ensure that the marker\nremains visible in this counterfactual generation, despite such markers being\nunlikely in natural videos, we use the unedited initial frame as a negative\nprompt. Through experiments with multiple image-conditioned video diffusion\nmodels, we find that these \"emergent\" tracks outperform those of prior\nzero-shot methods and persist through occlusions, often obtaining performance\nthat is competitive with specialized self-supervised models.",
            "headline_zh": "提出点提示方法，利用视频扩散模型实现零样本点跟踪。",
            "intro_zh": [
                "核心问题：如何利用视频生成模型进行零样本点跟踪，分析物体运动轨迹。",
                "方法要点：通过添加彩色标记点并反事实生成视频，传播标记以追踪轨迹。",
                "实验效果：在多个模型上测试，性能优于现有零样本方法，对遮挡鲁棒。"
            ],
            "tags_zh": [
                "点跟踪",
                "视频扩散模型",
                "零样本学习",
                "反事实生成",
                "运动分析"
            ],
            "_index": 2
        },
        {
            "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
            "authors": [
                "Haoran Feng",
                "Dizhe Zhang",
                "Xiangtai Li",
                "Bo Du",
                "Lu Qi"
            ],
            "arxiv_id": "2510.11712v1",
            "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.",
            "headline_zh": "提出DiT360框架，通过混合训练解决全景图像生成中的几何保真度和真实感问题。",
            "intro_zh": [
                "核心问题：缺乏大规模高质量真实全景数据，影响生成图像的几何保真度和真实感。",
                "方法要点：在图像和token级别应用跨域变换和域内增强，包括视角图像引导和全景优化。",
                "实验效果：在文本到全景、修复和扩展任务中，边界一致性和图像保真度优于现有方法。"
            ],
            "tags_zh": [
                "全景图像生成",
                "混合训练",
                "扩散变换器",
                "几何保真度",
                "边界连续性",
                "图像修复"
            ],
            "_index": 3
        },
        {
            "title": "Adversarial Attacks Leverage Interference Between Features in Superposition",
            "authors": [
                "Edward Stevinson",
                "Lucas Prieto",
                "Melih Barsbey",
                "Tolga Birdal"
            ],
            "arxiv_id": "2510.11709v1",
            "summary": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs.",
            "headline_zh": "提出利用特征叠加干扰解释对抗性攻击，揭示网络压缩导致漏洞",
            "intro_zh": [
                "核心问题：对抗性漏洞源于神经网络高效信息编码中的特征叠加干扰",
                "方法要点：通过特征叠加机制预测攻击模式，解释模型间攻击可转移性",
                "实验或效果：在合成和ViT-CIFAR-10实验中验证叠加足以引发对抗性漏洞"
            ],
            "tags_zh": [
                "对抗性攻击",
                "特征叠加",
                "神经网络压缩",
                "攻击可转移性",
                "ViT模型"
            ],
            "_index": 4
        },
        {
            "title": "Bayesian Topological Convolutional Neural Nets",
            "authors": [
                "Sarah Harkins Dayton",
                "Hayden Everett",
                "Ioannis Schizas",
                "David L. Boothe Jr.",
                "Vasileios Maroulas"
            ],
            "arxiv_id": "2510.11704v1",
            "summary": "Convolutional neural networks (CNNs) have been established as the main\nworkhorse in image data processing; nonetheless, they require large amounts of\ndata to train, often produce overconfident predictions, and frequently lack the\nability to quantify the uncertainty of their predictions. To address these\nconcerns, we propose a new Bayesian topological CNN that promotes a novel\ninterplay between topology-aware learning and Bayesian sampling. Specifically,\nit utilizes information from important manifolds to accelerate training while\nreducing calibration error by placing prior distributions on network parameters\nand properly learning appropriate posteriors. One important contribution of our\nwork is the inclusion of a consistency condition in the learning cost, which\ncan effectively modify the prior distributions to improve the performance of\nour novel network architecture. We evaluate the model on benchmark image\nclassification datasets and demonstrate its superiority over conventional CNNs,\nBayesian neural networks (BNNs), and topological CNNs. In particular, we supply\nevidence that our method provides an advantage in situations where training\ndata is limited or corrupted. Furthermore, we show that the new model allows\nfor better uncertainty quantification than standard BNNs since it can more\nreadily identify examples of out-of-distribution data on which it has not been\ntrained. Our results highlight the potential of our novel hybrid approach for\nmore efficient and robust image classification.",
            "headline_zh": "提出贝叶斯拓扑卷积神经网络以解决图像分类中数据不足和不确定性量化问题",
            "intro_zh": [
                "核心问题：传统CNN需要大量数据训练、预测过度自信且缺乏不确定性量化能力",
                "方法要点：结合拓扑感知学习和贝叶斯采样，通过先验分布和一致性条件优化网络参数",
                "实验或效果：在基准数据集上优于传统CNN、BNN和拓扑CNN，尤其在数据有限或损坏时表现优越"
            ],
            "tags_zh": [
                "贝叶斯神经网络",
                "拓扑卷积网络",
                "不确定性量化",
                "图像分类",
                "数据效率"
            ],
            "_index": 5
        },
        {
            "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs",
            "authors": [
                "Wei Huang",
                "Yi Ge",
                "Shuai Yang",
                "Yicheng Xiao",
                "Huizi Mao",
                "Yujun Lin",
                "Hanrong Ye",
                "Sifei Liu",
                "Ka Chun Cheung",
                "Hongxu Yin",
                "Yao Lu",
                "Xiaojuan Qi",
                "Song Han",
                "Yukang Chen"
            ],
            "arxiv_id": "2510.11696v1",
            "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
            "headline_zh": "提出QeRL框架，结合量化与强化学习，提升大语言模型训练效率与探索能力。",
            "intro_zh": [
                "核心问题：强化学习在大语言模型中资源密集，需高GPU内存和长训练时间。",
                "方法要点：结合NVFP4量化和LoRA，引入自适应量化噪声机制，加速训练并增强探索。",
                "实验效果：在7B模型上实现1.5倍加速，单GPU训练32B模型，数学基准性能媲美全参数微调。"
            ],
            "tags_zh": [
                "量化强化学习",
                "大语言模型训练",
                "低秩适应",
                "自适应噪声",
                "GPU效率优化",
                "数学推理基准"
            ],
            "_index": 6
        },
        {
            "title": "Scaling Language-Centric Omnimodal Representation Learning",
            "authors": [
                "Chenghao Xiao",
                "Hou Pong Chan",
                "Hao Zhang",
                "Weiwen Xu",
                "Mahani Aljunied",
                "Yu Rong"
            ],
            "arxiv_id": "2510.11693v1",
            "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
            "headline_zh": "提出语言中心全模态嵌入框架以增强多模态表示学习",
            "intro_zh": [
                "核心问题：基于MLLM的多模态嵌入方法优势原因未明，需探索隐式跨模态对齐机制。",
                "方法要点：利用生成预训练中的隐式对齐，结合对比学习轻量化优化表示空间。",
                "实验或效果：在多种基准测试中实现SOTA，验证生成-表示缩放定律的有效性。"
            ],
            "tags_zh": [
                "多模态表示学习",
                "语言中心嵌入",
                "对比学习",
                "生成预训练",
                "跨模态对齐",
                "缩放定律"
            ],
            "_index": 7
        },
        {
            "title": "Diffusion Transformers with Representation Autoencoders",
            "authors": [
                "Boyang Zheng",
                "Nanye Ma",
                "Shengbang Tong",
                "Saining Xie"
            ],
            "arxiv_id": "2510.11690v1",
            "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
            "headline_zh": "提出表示自编码器以改进扩散变换器的潜在生成建模",
            "intro_zh": [
                "核心问题：传统VAE编码器在扩散变换器中存在架构过时、潜在空间低维和表示弱的问题。",
                "方法要点：用预训练表示编码器（如DINO）与解码器构建表示自编码器，提供高质量重构和语义丰富潜在空间。",
                "实验或效果：在ImageNet上实现低FID分数，如1.51（无引导），并实现更快收敛。"
            ],
            "tags_zh": [
                "扩散变换器",
                "表示自编码器",
                "潜在生成建模",
                "图像生成",
                "自监督学习"
            ],
            "_index": 8
        },
        {
            "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation",
            "authors": [
                "Maggie Wang",
                "Stephen Tian",
                "Aiden Swann",
                "Ola Shorinwa",
                "Jiajun Wu",
                "Mac Schwager"
            ],
            "arxiv_id": "2510.11689v1",
            "summary": "Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ .",
            "headline_zh": "提出Phys2Real方法，融合VLM先验与在线适应以解决仿真到真实机器人操作中的不确定性挑战",
            "intro_zh": [
                "核心问题：仿真到真实机器人操作转移困难，尤其涉及精确动力学任务",
                "方法要点：结合VLM推断物理参数先验与在线交互数据，通过不确定性融合优化策略",
                "实验效果：在T块和锤子推动任务中，成功率显著提升，优于领域随机化基线"
            ],
            "tags_zh": [
                "仿真到真实转移",
                "机器人操作",
                "不确定性融合",
                "视觉语言模型",
                "在线适应",
                "物理参数估计"
            ],
            "_index": 9
        },
        {
            "title": "Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View",
            "authors": [
                "Jinyu Zhang",
                "Haitao Lin",
                "Jiashu Hou",
                "Xiangyang Xue",
                "Yanwei Fu"
            ],
            "arxiv_id": "2510.11687v1",
            "summary": "Estimating an object's 6D pose, size, and shape from visual input is a\nfundamental problem in computer vision, with critical applications in robotic\ngrasping and manipulation. Existing methods either rely on object-specific\npriors such as CAD models or templates, or suffer from limited generalization\nacross categories due to pose-shape entanglement and multi-stage pipelines. In\nthis work, we propose a unified, category-agnostic framework that\nsimultaneously predicts 6D pose, size, and dense shape from a single RGB-D\nimage, without requiring templates, CAD models, or category labels at test\ntime. Our model fuses dense 2D features from vision foundation models with\npartial 3D point clouds using a Transformer encoder enhanced by a\nMixture-of-Experts, and employs parallel decoders for pose-size estimation and\nshape reconstruction, achieving real-time inference at 28 FPS. Trained solely\non synthetic data from 149 categories in the SOPE dataset, our framework is\nevaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,\nspanning over 300 categories. It achieves state-of-the-art accuracy on seen\ncategories while demonstrating remarkably strong zero-shot generalization to\nunseen real-world objects, establishing a new standard for open-set 6D\nunderstanding in robotics and embodied AI.",
            "headline_zh": "提出类别无关框架，从单视图同时估计物体6D位姿、尺寸和形状，无需模板或CAD模型。",
            "intro_zh": [
                "核心问题：从视觉输入估计物体6D位姿、尺寸和形状，现有方法依赖先验或泛化差。",
                "方法要点：融合2D特征与3D点云，使用Transformer和专家混合，并行解码实现实时推理。",
                "实验或效果：在合成数据训练，多基准测试中实现SOTA，零样本泛化强于未知物体。"
            ],
            "tags_zh": [
                "6D位姿估计",
                "类别无关学习",
                "形状重建",
                "Transformer模型",
                "零样本泛化",
                "实时推理"
            ],
            "_index": 10
        },
        {
            "title": "Ego-Vision World Model for Humanoid Contact Planning",
            "authors": [
                "Hang Liu",
                "Yuman Gao",
                "Sangli Teng",
                "Yufeng Chi",
                "Yakun Sophia Shao",
                "Zhongyu Li",
                "Maani Ghaffari",
                "Koushil Sreenath"
            ],
            "arxiv_id": "2510.11682v1",
            "summary": "Enabling humanoid robots to exploit physical contact, rather than simply\navoid collisions, is crucial for autonomy in unstructured environments.\nTraditional optimization-based planners struggle with contact complexity, while\non-policy reinforcement learning (RL) is sample-inefficient and has limited\nmulti-task ability. We propose a framework combining a learned world model with\nsampling-based Model Predictive Control (MPC), trained on a demonstration-free\noffline dataset to predict future outcomes in a compressed latent space. To\naddress sparse contact rewards and sensor noise, the MPC uses a learned\nsurrogate value function for dense, robust planning. Our single, scalable model\nsupports contact-aware tasks, including wall support after perturbation,\nblocking incoming objects, and traversing height-limited arches, with improved\ndata efficiency and multi-task capability over on-policy RL. Deployed on a\nphysical humanoid, our system achieves robust, real-time contact planning from\nproprioception and ego-centric depth images. Website:\nhttps://ego-vcp.github.io/",
            "headline_zh": "提出结合世界模型与模型预测控制的人形机器人接触规划框架，以提升非结构化环境中的自主性。",
            "intro_zh": [
                "核心问题：传统优化规划器难以处理复杂接触，在线强化学习样本效率低且多任务能力有限。",
                "方法要点：使用离线数据集训练世界模型，在压缩潜在空间预测未来，结合采样MPC和代理价值函数。",
                "实验或效果：在物理人形机器人上实现实时接触规划，支持扰动后支撑、阻挡物体和穿越限高拱门等任务。"
            ],
            "tags_zh": [
                "人形机器人",
                "接触规划",
                "世界模型",
                "模型预测控制",
                "离线强化学习",
                "自我中心视觉"
            ],
            "_index": 11
        },
        {
            "title": "FACE: Faithful Automatic Concept Extraction",
            "authors": [
                "Dipkamal Bhusal",
                "Michael Clifford",
                "Sara Rampazzi",
                "Nidhi Rastogi"
            ],
            "arxiv_id": "2510.11675v1",
            "summary": "Interpreting deep neural networks through concept-based explanations offers a\nbridge between low-level features and high-level human-understandable\nsemantics. However, existing automatic concept discovery methods often fail to\nalign these extracted concepts with the model's true decision-making process,\nthereby compromising explanation faithfulness. In this work, we propose FACE\n(Faithful Automatic Concept Extraction), a novel framework that augments\nNon-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence\nregularization term to ensure alignment between the model's original and\nconcept-based predictions. Unlike prior methods that operate solely on encoder\nactivations, FACE incorporates classifier supervision during concept learning,\nenforcing predictive consistency and enabling faithful explanations. We provide\ntheoretical guarantees showing that minimizing the KL divergence bounds the\ndeviation in predictive distributions, thereby promoting faithful local\nlinearity in the learned concept space. Systematic evaluations on ImageNet,\nCOCO, and CelebA datasets demonstrate that FACE outperforms existing methods\nacross faithfulness and sparsity metrics.",
            "headline_zh": "提出FACE框架以解决自动概念提取中解释忠实性问题",
            "intro_zh": [
                "现有自动概念发现方法常与模型决策过程不一致，影响解释忠实性。",
                "FACE结合NMF与KL散度正则化，确保概念预测与原预测对齐。",
                "在ImageNet等数据集上评估，FACE在忠实性和稀疏性上优于现有方法。"
            ],
            "tags_zh": [
                "概念提取",
                "解释忠实性",
                "非负矩阵分解",
                "KL散度正则化",
                "深度学习解释"
            ],
            "_index": 12
        },
        {
            "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
            "authors": [
                "Yi Yang",
                "Kefan Gu",
                "Yuqing Wen",
                "Hebei Li",
                "Yucheng Zhao",
                "Tiancai Wang",
                "Xudong Liu"
            ],
            "arxiv_id": "2510.11660v1",
            "summary": "While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets.The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/.",
            "headline_zh": "提出ManiAgent代理框架以解决机器人操作中复杂推理和长时任务规划问题",
            "intro_zh": [
                "核心问题：视觉-语言-动作模型在复杂推理和长时任务规划中受限于数据稀缺和模型能力",
                "方法要点：采用多代理架构，通过代理间通信实现环境感知、子任务分解和动作生成",
                "实验或效果：在SimplerEnv基准上成功率86.8%，真实世界拾放任务成功率95.8%"
            ],
            "tags_zh": [
                "机器人操作",
                "代理框架",
                "视觉-语言-动作模型",
                "任务规划",
                "多代理系统"
            ],
            "_index": 13
        },
        {
            "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
            "authors": [
                "Yuxuan Xue",
                "Xianghui Xie",
                "Margaret Kostyrko",
                "Gerard Pons-Moll"
            ],
            "arxiv_id": "2510.11650v1",
            "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
            "headline_zh": "提出InfiniHuman框架以解决3D人体生成的可控性与多样性问题",
            "intro_zh": [
                "核心问题：如何利用基础模型生成无限多样且可控的3D人体数据",
                "方法要点：通过蒸馏视觉语言和图像生成模型，构建自动数据生成管道",
                "实验或效果：用户研究表明生成身份与扫描渲染无法区分，提升视觉质量和可控性"
            ],
            "tags_zh": [
                "3D人体生成",
                "数据蒸馏",
                "扩散模型",
                "多模态数据集",
                "可控生成"
            ],
            "_index": 14
        },
        {
            "title": "PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image",
            "authors": [
                "Pradyumna Yalandur Muralidhar",
                "Yuxuan Xue",
                "Xianghui Xie",
                "Margaret Kostyrko",
                "Gerard Pons-Moll"
            ],
            "arxiv_id": "2510.11649v1",
            "summary": "Reconstructing metrically accurate humans and their surrounding scenes from a\nsingle image is crucial for virtual reality, robotics, and comprehensive 3D\nscene understanding. However, existing methods struggle with depth ambiguity,\nocclusions, and physically inconsistent contacts. To address these challenges,\nwe introduce PhySIC, a framework for physically plausible Human-Scene\nInteraction and Contact reconstruction. PhySIC recovers metrically consistent\nSMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within\na shared coordinate frame from a single RGB image. Starting from coarse\nmonocular depth and body estimates, PhySIC performs occlusion-aware inpainting,\nfuses visible depth with unscaled geometry for a robust metric scaffold, and\nsynthesizes missing support surfaces like floors. A confidence-weighted\noptimization refines body pose, camera parameters, and global scale by jointly\nenforcing depth alignment, contact priors, interpenetration avoidance, and 2D\nreprojection consistency. Explicit occlusion masking safeguards invisible\nregions against implausible configurations. PhySIC is efficient, requiring only\n9 seconds for joint human-scene optimization and under 27 seconds end-to-end.\nIt naturally handles multiple humans, enabling reconstruction of diverse\ninteractions. Empirically, PhySIC outperforms single-image baselines, reducing\nmean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,\nand improving contact F1 from 0.09 to 0.51. Qualitative results show realistic\nfoot-floor interactions, natural seating, and plausible reconstructions of\nheavily occluded furniture. By converting a single image into a physically\nplausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.\nOur implementation is publicly available at https://yuxuan-xue.com/physic.",
            "headline_zh": "提出PhySIC框架，从单张图像重建物理合理的人类-场景交互与接触",
            "intro_zh": [
                "核心问题：单图像重建存在深度模糊、遮挡和物理不一致接触，导致3D人类与场景不准确",
                "方法要点：通过遮挡感知修复、深度融合和置信优化，恢复SMPL-X人体网格与场景表面",
                "实验或效果：在基准测试中显著降低场景误差和姿态误差，提升接触检测F1分数"
            ],
            "tags_zh": [
                "3D人体重建",
                "场景理解",
                "物理合理性",
                "单图像重建",
                "接触检测"
            ],
            "_index": 15
        },
        {
            "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment",
            "authors": [
                "Yinan Chen",
                "Jiangning Zhang",
                "Teng Hu",
                "Yuxiang Zeng",
                "Zhucun Xue",
                "Qingdong He",
                "Chengjie Wang",
                "Yong Liu",
                "Xiaobin Hu",
                "Shuicheng Yan"
            ],
            "arxiv_id": "2510.11647v1",
            "summary": "Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.",
            "headline_zh": "提出IVEBench基准套件以解决指令引导视频编辑评估不足的问题",
            "intro_zh": [
                "现有视频编辑基准在指令引导评估上不足，存在源视频多样性低、任务覆盖窄和指标不完整问题",
                "构建包含600个高质量源视频、8类编辑任务和三维评估协议的基准套件",
                "实验显示IVEBench能全面评估先进方法，提供与人类对齐的评价结果"
            ],
            "tags_zh": [
                "指令引导视频编辑",
                "视频编辑基准",
                "多模态评估",
                "视频质量评估",
                "指令合规性"
            ],
            "_index": 16
        },
        {
            "title": "NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection",
            "authors": [
                "Krittin Chaowakarn",
                "Paramin Sangwongngam",
                "Nang Htet Htet Aung",
                "Chalie Charoenlarpnopparut"
            ],
            "arxiv_id": "2510.11632v1",
            "summary": "Recent studies in 3D object detection for autonomous vehicles aim to enrich\nfeatures through the utilization of multi-modal setups or the extraction of\nlocal patterns within LiDAR point clouds. However, multi-modal methods face\nsignificant challenges in feature alignment, and gaining features locally can\nbe oversimplified for complex 3D object detection tasks. In this paper, we\npropose a novel model, NV3D, which utilizes local features acquired from voxel\nneighbors, as normal vectors computed per voxel basis using K-nearest neighbors\n(KNN) and principal component analysis (PCA). This informative feature enables\nNV3D to determine the relationship between the surface and pertinent target\nentities, including cars, pedestrians, or cyclists. During the normal vector\nextraction process, NV3D offers two distinct sampling strategies: normal vector\ndensity-based sampling and FOV-aware bin-based sampling, allowing elimination\nof up to 55% of data while maintaining performance. In addition, we applied\nelement-wise attention fusion, which accepts voxel features as the query and\nvalue and normal vector features as the key, similar to the attention\nmechanism. Our method is trained on the KITTI dataset and has demonstrated\nsuperior performance in car and cyclist detection owing to their spatial\nshapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%\nmean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%\nand 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in\ncar detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of\nvoxels being filtered out.",
            "headline_zh": "提出NV3D模型，利用法向量增强3D物体检测，适用于自动驾驶场景。",
            "intro_zh": [
                "核心问题：多模态方法特征对齐困难，局部特征提取在复杂3D检测中过于简化。",
                "方法要点：基于KNN和PCA计算体素法向量，采用密度和FOV感知采样减少数据。",
                "实验效果：在KITTI数据集上，mAP优于基线，采样后性能保持且数据减少55%。"
            ],
            "tags_zh": [
                "3D物体检测",
                "法向量特征",
                "体素采样",
                "自动驾驶",
                "KITTI数据集"
            ],
            "_index": 17
        },
        {
            "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
            "authors": [
                "Tobias Preintner",
                "Weixuan Yuan",
                "Adrian König",
                "Thomas Bäck",
                "Elena Raponi",
                "Niki van Stein"
            ],
            "arxiv_id": "2510.11631v1",
            "summary": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.",
            "headline_zh": "提出EvoCAD方法，结合视觉语言模型与进化算法生成CAD对象。",
            "intro_zh": [
                "核心问题：如何生成符号表示的CAD对象，并确保拓扑正确性。",
                "方法要点：使用视觉语言模型采样CAD对象，并通过进化算法优化。",
                "实验或效果：在CADPrompt基准上评估，新拓扑指标显示优于先前方法。"
            ],
            "tags_zh": [
                "CAD生成",
                "进化算法",
                "视觉语言模型",
                "拓扑指标",
                "符号表示"
            ],
            "_index": 18
        },
        {
            "title": "High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid Network",
            "authors": [
                "Feng Zhang",
                "Haoyou Deng",
                "Zhiqiang Li",
                "Lida Li",
                "Bin Xu",
                "Qingbo Lu",
                "Zisheng Cao",
                "Minchen Wei",
                "Changxin Gao",
                "Nong Sang",
                "Xiang Bai"
            ],
            "arxiv_id": "2510.11613v1",
            "summary": "Photo enhancement plays a crucial role in augmenting the visual aesthetics of\na photograph. In recent years, photo enhancement methods have either focused on\nenhancement performance, producing powerful models that cannot be deployed on\nedge devices, or prioritized computational efficiency, resulting in inadequate\nperformance for real-world applications. To this end, this paper introduces a\npyramid network called LLF-LUT++, which integrates global and local operators\nthrough closed-form Laplacian pyramid decomposition and reconstruction. This\napproach enables fast processing of high-resolution images while also achieving\nexcellent performance. Specifically, we utilize an image-adaptive 3D LUT that\ncapitalizes on the global tonal characteristics of downsampled images, while\nincorporating two distinct weight fusion strategies to achieve coarse global\nimage enhancement. To implement this strategy, we designed a spatial-frequency\ntransformer weight predictor that effectively extracts the desired distinct\nweights by leveraging frequency features. Additionally, we apply local\nLaplacian filters to adaptively refine edge details in high-frequency\ncomponents. After meticulously redesigning the network structure and\ntransformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on\nthe HDR+ dataset, but also further reduces runtime, with 4K resolution images\nprocessed in just 13 ms on a single GPU. Extensive experimental results on two\nbenchmark datasets further show that the proposed approach performs favorably\ncompared to state-of-the-art methods. The source code will be made publicly\navailable at https://github.com/fengzhang427/LLF-LUT.",
            "headline_zh": "提出LLF-LUT++网络，实现高分辨率图像实时增强与高性能平衡",
            "intro_zh": [
                "核心问题：现有方法在增强性能与计算效率间难以兼顾，无法在边缘设备部署",
                "方法要点：结合全局3D LUT与局部拉普拉斯滤波，通过拉普拉斯金字塔分解重构",
                "实验效果：在HDR+数据集PSNR提升2.64 dB，4K图像处理仅需13 ms"
            ],
            "tags_zh": [
                "图像增强",
                "拉普拉斯金字塔",
                "3D LUT",
                "实时处理",
                "高分辨率图像",
                "空间频率变换器"
            ],
            "_index": 19
        },
        {
            "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
            "authors": [
                "Yicheng Xu",
                "Yue Wu",
                "Jiashuo Yu",
                "Ziang Yan",
                "Tianxiang Jiang",
                "Yinan He",
                "Qingsong Zhao",
                "Kai Chen",
                "Yu Qiao",
                "Limin Wang",
                "Manabu Okumura",
                "Yi Wang"
            ],
            "arxiv_id": "2510.11606v1",
            "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.",
            "headline_zh": "提出ExpVid基准以评估多模态大模型在科学实验视频理解中的能力",
            "intro_zh": [
                "现有基准忽略湿实验室工作的细粒度和长程特性，难以评估MLLMs的真实能力",
                "引入三级任务层次：细粒度感知、过程理解和科学推理，确保视觉基础",
                "评估19个MLLMs，发现其在细粒度细节和推理方面表现不佳，开源模型差距显著"
            ],
            "tags_zh": [
                "多模态大模型",
                "科学实验视频",
                "基准评估",
                "细粒度感知",
                "过程理解",
                "科学推理"
            ],
            "_index": 20
        },
        {
            "title": "ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training",
            "authors": [
                "Leonard Bruns",
                "Axel Barroso-Laguna",
                "Tommaso Cavallari",
                "Áron Monszpart",
                "Sowmya Munukutla",
                "Victor Adrian Prisacariu",
                "Eric Brachmann"
            ],
            "arxiv_id": "2510.11605v1",
            "summary": "Scene coordinate regression (SCR) has established itself as a promising\nlearning-based approach to visual relocalization. After mere minutes of\nscene-specific training, SCR models estimate camera poses of query images with\nhigh accuracy. Still, SCR methods fall short of the generalization capabilities\nof more classical feature-matching approaches. When imaging conditions of query\nimages, such as lighting or viewpoint, are too different from the training\nviews, SCR models fail. Failing to generalize is an inherent limitation of\nprevious SCR frameworks, since their training objective is to encode the\ntraining views in the weights of the coordinate regressor itself. The regressor\nessentially overfits to the training views, by design. We propose to separate\nthe coordinate regressor and the map representation into a generic transformer\nand a scene-specific map code. This separation allows us to pre-train the\ntransformer on tens of thousands of scenes. More importantly, it allows us to\ntrain the transformer to generalize from mapping images to unseen query images\nduring pre-training. We demonstrate on multiple challenging relocalization\ndatasets that our method, ACE-G, leads to significantly increased robustness\nwhile keeping the computational footprint attractive.",
            "headline_zh": "提出ACE-G方法，通过查询预训练提升场景坐标回归的泛化能力",
            "intro_zh": [
                "核心问题：传统场景坐标回归方法在查询图像条件差异大时泛化能力不足，易过拟合训练视图",
                "方法要点：分离坐标回归器和地图表示，使用通用Transformer预训练于多场景，增强泛化性",
                "实验或效果：在多个挑战性重定位数据集上验证，ACE-G显著提升鲁棒性，保持计算效率"
            ],
            "tags_zh": [
                "场景坐标回归",
                "视觉重定位",
                "Transformer预训练",
                "泛化能力",
                "地图表示分离"
            ],
            "_index": 21
        },
        {
            "title": "Smooth Spatiotemporal Tube Synthesis for Prescribed-Time Reach-Avoid-Stay Control",
            "authors": [
                "Siddhartha Upadhyay",
                "Ratnangshu Das",
                "Pushpak Jagtap"
            ],
            "arxiv_id": "2510.11583v1",
            "summary": "In this work, we address the issue of controller synthesis for a\ncontrol-affine nonlinear system to meet prescribed time reach-avoid-stay\nspecifications. Our goal is to improve upon previous methods based on\nspatiotemporal tubes (STTs) by eliminating the need for circumvent functions,\nwhich often lead to abrupt tube modifications and high control effort. We\npropose an adaptive framework that constructs smooth STTs around static unsafe\nsets, enabling continuous avoidance while guiding the system toward the target\nwithin the prescribed time. A closed-form, approximation-free control law is\nderived to ensure the system trajectory remains within the tube and satisfies\nthe RAS task. The effectiveness of the proposed approach is demonstrated\nthrough a case study, showing a significant reduction in control effort\ncompared to prior methods.",
            "headline_zh": "提出平滑时空管框架以解决规定时间到达-避障-停留控制问题",
            "intro_zh": [
                "核心问题：控制仿射非线性系统在规定时间内满足到达-避障-停留规范，避免绕行函数导致的突变和高控制成本。",
                "方法要点：构建平滑时空管，实现连续避障和目标引导，并推导无近似闭环控制律。",
                "实验或效果：案例研究表明控制成本显著降低，优于先前方法。"
            ],
            "tags_zh": [
                "控制合成",
                "时空管",
                "非线性系统",
                "规定时间控制",
                "避障控制"
            ],
            "_index": 22
        },
        {
            "title": "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis",
            "authors": [
                "Hongyu Zhu",
                "Lin Chen",
                "Mounim A. El-Yacoubi",
                "Mingsheng Shang"
            ],
            "arxiv_id": "2510.11579v1",
            "summary": "Multimodal Sentiment Analysis (MSA) aims to identify and interpret human\nemotions by integrating information from heterogeneous data sources such as\ntext, video, and audio. While deep learning models have advanced in network\narchitecture design, they remain heavily limited by scarce multimodal annotated\ndata. Although Mixup-based augmentation improves generalization in unimodal\ntasks, its direct application to MSA introduces critical challenges: random\nmixing often amplifies label ambiguity and semantic inconsistency due to the\nlack of emotion-aware mixing mechanisms. To overcome these issues, we propose\nMS-Mix, an adaptive, emotion-sensitive augmentation framework that\nautomatically optimizes sample mixing in multimodal settings. The key\ncomponents of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)\nstrategy that effectively prevents semantic confusion caused by mixing samples\nwith contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module\nusing multi-head self-attention to compute modality-specific mixing ratios\ndynamically based on their respective emotional intensities. (3) a Sentiment\nAlignment Loss (SAL) that aligns the prediction distributions across\nmodalities, and incorporates the Kullback-Leibler-based loss as an additional\nregularization term to train the emotion intensity predictor and the backbone\nnetwork jointly. Extensive experiments on three benchmark datasets with six\nstate-of-the-art backbones confirm that MS-Mix consistently outperforms\nexisting methods, establishing a new standard for robust multimodal sentiment\naugmentation. The source code is available at:\nhttps://github.com/HongyuZhu-s/MS-Mix.",
            "headline_zh": "提出MS-Mix以解决多模态情感分析中数据稀缺和混合增强的语义不一致问题",
            "intro_zh": [
                "核心问题：多模态情感分析中数据稀缺，且直接应用Mixup增强导致标签模糊和语义不一致",
                "方法要点：引入情感感知样本选择、强度引导混合比和情感对齐损失，优化多模态混合",
                "实验或效果：在三个基准数据集和六个先进骨干网络上验证，MS-Mix一致优于现有方法"
            ],
            "tags_zh": [
                "多模态情感分析",
                "数据增强",
                "情感感知混合",
                "模态对齐",
                "注意力机制",
                "KL损失"
            ],
            "_index": 23
        },
        {
            "title": "Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping",
            "authors": [
                "Walid Elbarz",
                "Mohamed Bourriz",
                "Hicham Hajji",
                "Hamd Ait Abdelali",
                "François Bourzeix"
            ],
            "arxiv_id": "2510.11576v1",
            "summary": "Foundation models are transforming Earth observation, but their potential for\nhyperspectral crop mapping remains underexplored. This study benchmarks three\nfoundation models for cereal crop mapping using hyperspectral imagery:\nHyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth\ndataset (a large multitemporal hyperspectral archive). Models were fine-tuned\non manually labeled data from a training region and evaluated on an independent\ntest region. Performance was measured with overall accuracy (OA), average\naccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),\nDOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of\n93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved\n91%, highlighting the importance of model architecture for strong\ngeneralization across geographic regions and sensor platforms. These results\nprovide a systematic evaluation of foundation models for operational\nhyperspectral crop mapping and outline directions for future model development.",
            "headline_zh": "基准测试基础模型用于高光谱图像分类，应用于谷物作物类型制图",
            "intro_zh": [
                "核心问题：基础模型在高光谱作物制图中的应用潜力未被充分探索。",
                "方法要点：对HyperSigma、DOFA和SpectralEarth预训练模型进行微调与评估。",
                "实验或效果：SpectralEarth模型总体准确率达93.5%，优于其他模型。"
            ],
            "tags_zh": [
                "高光谱图像分类",
                "基础模型基准测试",
                "谷物作物制图",
                "模型微调",
                "遥感应用"
            ],
            "_index": 24
        },
        {
            "title": "Calibrated Dynamic Modeling for Force and Payload Estimation in Hydraulic Machinery",
            "authors": [
                "Lennart Werner",
                "Pol Eyschen",
                "Sean Costello",
                "Pierluigi Micarelli",
                "Marco Hutter"
            ],
            "arxiv_id": "2510.11574v1",
            "summary": "Accurate real-time estimation of end effector interaction forces in hydraulic\nexcavators is a key enabler for advanced automation in heavy machinery.\nAccurate knowledge of these forces allows improved, precise grading and digging\nmaneuvers. To address these challenges, we introduce a high-accuracy,\nretrofittable 2D force- and payload estimation algorithm that does not impose\nadditional requirements on the operator regarding trajectory, acceleration or\nthe use of the slew joint. The approach is designed for retrofittability,\nrequires minimal calibration and no prior knowledge of machine-specific dynamic\ncharacteristics. Specifically, we propose a method for identifying a dynamic\nmodel, necessary to estimate both end effector interaction forces and bucket\npayload during normal operation. Our optimization-based payload estimation\nachieves a full-scale payload accuracy of 1%. On a standard 25 t excavator, the\nonline force measurement from pressure and inertial measurements achieves a\ndirection accuracy of 13 degree and a magnitude accuracy of 383 N. The method's\naccuracy and generalization capability are validated on two excavator platforms\nof different type and weight classes. We benchmark our payload estimation\nagainst a classical quasistatic method and a commercially available system. Our\nsystem outperforms both in accuracy and precision.",
            "headline_zh": "提出高精度动态建模方法，用于液压机械的力和载荷估计，实现自动化操作。",
            "intro_zh": [
                "核心问题：液压挖掘机末端执行器交互力的实时精确估计是重型机械自动化的关键。",
                "方法要点：基于优化的动态模型识别，无需额外轨迹要求或机器特定动态知识。",
                "实验或效果：在25吨挖掘机上，载荷估计精度达1%，力测量方向精度13度。"
            ],
            "tags_zh": [
                "液压机械",
                "动态建模",
                "力估计",
                "载荷估计",
                "实时估计",
                "自动化控制"
            ],
            "_index": 25
        },
        {
            "title": "A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation",
            "authors": [
                "Denis Zavadski",
                "Damjan Kalšan",
                "Tim Küchler",
                "Haebom Lee",
                "Stefan Roth",
                "Carsten Rother"
            ],
            "arxiv_id": "2510.11567v1",
            "summary": "Synthetic datasets are widely used for training urban scene recognition\nmodels, but even highly realistic renderings show a noticeable gap to real\nimagery. This gap is particularly pronounced when adapting to a specific target\ndomain, such as Cityscapes, where differences in architecture, vegetation,\nobject appearance, and camera characteristics limit downstream performance.\nClosing this gap with more detailed 3D modelling would require expensive asset\nand scene design, defeating the purpose of low-cost labelled data. To address\nthis, we present a new framework that adapts an off-the-shelf diffusion model\nto a target domain using only imperfect pseudo-labels. Once trained, it\ngenerates high-fidelity, target-aligned images from semantic maps of any\nsynthetic dataset, including low-effort sources created in hours rather than\nmonths. The method filters suboptimal generations, rectifies image-label\nmisalignments, and standardises semantics across datasets, transforming weak\nsynthetic data into competitive real-domain training sets. Experiments on five\nsynthetic datasets and two real target datasets show segmentation gains of up\nto +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly\nconstructed synthetic datasets as effective as high-effort, time-intensive\nsynthetic datasets requiring extensive manual design. This work highlights a\nvaluable collaborative paradigm where fast semantic prototyping, combined with\ngenerative models, enables scalable, high-quality training data creation for\nurban scene understanding.",
            "headline_zh": "提出基于扩散模型的框架，以低代价生成高保真城市语义分割训练数据",
            "intro_zh": [
                "核心问题：合成数据与真实图像存在域差距，影响城市语义分割模型性能",
                "方法要点：使用不完美伪标签适配扩散模型，生成目标域对齐图像并过滤优化",
                "实验或效果：在多个数据集上，分割性能提升达8.0% mIoU，超越现有翻译方法"
            ],
            "tags_zh": [
                "城市语义分割",
                "扩散模型",
                "域适应",
                "合成数据生成",
                "训练数据增强"
            ],
            "_index": 26
        },
        {
            "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy",
            "authors": [
                "Kuanning Wang",
                "Yongchong Gu",
                "Yuqian Fu",
                "Zeyu Shangguan",
                "Sicheng He",
                "Xiangyang Xue",
                "Yanwei Fu",
                "Daniel Seita"
            ],
            "arxiv_id": "2510.11566v1",
            "summary": "Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/.",
            "headline_zh": "提出SCOOP'D方法，通过仿真到真实生成策略学习混合液体固体舀取。",
            "intro_zh": [
                "核心问题：机器人舀取需处理复杂工具-物体交互和可变形物体动态。",
                "方法要点：利用仿真收集演示，通过扩散生成策略模仿观察输入。",
                "实验效果：零样本部署在465次试验中表现优异，优于基线方法。"
            ],
            "tags_zh": [
                "机器人舀取",
                "仿真到真实",
                "生成策略",
                "扩散模型",
                "可变形物体操作"
            ],
            "_index": 27
        },
        {
            "title": "SNAP: Towards Segmenting Anything in Any Point Cloud",
            "authors": [
                "Aniket Gupta",
                "Hanhui Wang",
                "Charles Saunders",
                "Aruni RoyChowdhury",
                "Hanumant Singh",
                "Huaizu Jiang"
            ],
            "arxiv_id": "2510.11565v1",
            "summary": "Interactive 3D point cloud segmentation enables efficient annotation of\ncomplex 3D scenes through user-guided prompts. However, current approaches are\ntypically restricted in scope to a single domain (indoor or outdoor), and to a\nsingle form of user interaction (either spatial clicks or textual prompts).\nMoreover, training on multiple datasets often leads to negative transfer,\nresulting in domain-specific tools that lack generalizability. To address these\nlimitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in\n\\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D\nsegmentation that supports both point-based and text-based prompts across\ndiverse domains. Our approach achieves cross-domain generalizability by\ntraining on 7 datasets spanning indoor, outdoor, and aerial environments, while\nemploying domain-adaptive normalization to prevent negative transfer. For\ntext-prompted segmentation, we automatically generate mask proposals without\nhuman intervention and match them against CLIP embeddings of textual queries,\nenabling both panoptic and open-vocabulary segmentation. Extensive experiments\ndemonstrate that SNAP consistently delivers high-quality segmentation results.\nWe achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for\nspatial-prompted segmentation and demonstrate competitive results on all 5\ntext-prompted benchmarks. These results show that a unified model can match or\nexceed specialized domain-specific approaches, providing a practical tool for\nscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/",
            "headline_zh": "提出SNAP模型，实现跨域交互式3D点云分割，支持点和文本提示。",
            "intro_zh": [
                "当前交互式3D点云分割方法局限于单一域和交互形式，且多数据集训练易导致负迁移。",
                "SNAP采用域自适应归一化，在7个数据集上训练，支持点和文本提示，实现开放词汇分割。",
                "实验显示SNAP在零样本基准测试中表现优异，超越多个领域特定方法。"
            ],
            "tags_zh": [
                "3D点云分割",
                "交互式分割",
                "跨域泛化",
                "开放词汇分割",
                "域自适应归一化"
            ],
            "_index": 28
        },
        {
            "title": "How many samples to label for an application given a foundation model? Chest X-ray classification study",
            "authors": [
                "Nikolay Nechaev",
                "Evgenia Przhezdzetskaya",
                "Viktor Gombolevskiy",
                "Dmitry Umerenkov",
                "Dmitry Dylov"
            ],
            "arxiv_id": "2510.11553v1",
            "summary": "Chest X-ray classification is vital yet resource-intensive, typically\ndemanding extensive annotated data for accurate diagnosis. Foundation models\nmitigate this reliance, but how many labeled samples are required remains\nunclear. We systematically evaluate the use of power-law fits to predict the\ntraining size necessary for specific ROC-AUC thresholds. Testing multiple\npathologies and foundation models, we find XrayCLIP and XraySigLIP achieve\nstrong performance with significantly fewer labeled examples than a ResNet-50\nbaseline. Importantly, learning curve slopes from just 50 labeled cases\naccurately forecast final performance plateaus. Our results enable\npractitioners to minimize annotation costs by labeling only the essential\nsamples for targeted performance.",
            "headline_zh": "提出基于幂律拟合的方法，预测胸片分类中基础模型所需标注样本数，以最小化标注成本。",
            "intro_zh": [
                "核心问题：胸片分类依赖大量标注数据，基础模型下所需样本数未知。",
                "方法要点：使用幂律拟合预测训练集大小，以达成特定ROC-AUC阈值。",
                "实验或效果：XrayCLIP和XraySigLIP在少量样本下优于ResNet-50，50例即可预测性能。"
            ],
            "tags_zh": [
                "胸片分类",
                "基础模型",
                "样本需求预测",
                "幂律拟合",
                "ROC-AUC",
                "标注成本优化"
            ],
            "_index": 29
        },
        {
            "title": "Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education",
            "authors": [
                "Gregoire Passault",
                "Clement Gaspard",
                "Olivier Ly"
            ],
            "arxiv_id": "2510.11552v1",
            "summary": "Recent developments of low cost off-the-shelf programmable components, their\nmodularity, and also rapid prototyping made educational robotics flourish, as\nit is accessible in most schools today. They allow to illustrate and embody\ntheoretical problems in practical and tangible applications, and gather\nmultidisciplinary skills. They also give a rich natural context for\nproject-oriented pedagogy. However, most current robot kits all are limited to\negocentric aspect of the robots perception. This makes it difficult to access\nmore high-level problems involving e.g. coordinates or navigation. In this\npaper we introduce an educational holonomous robot kit that comes with an\nexternal tracking system, which lightens the constraint on embedded systems,\nbut allows in the same time to discover high-level aspects of robotics,\notherwise unreachable.",
            "headline_zh": "提出全向轮足球机器人套件，结合外部追踪系统以解决教育机器人感知局限问题。",
            "intro_zh": [
                "当前教育机器人套件感知局限于自我中心，难以处理坐标和导航等高阶问题。",
                "方法采用全向轮机器人套件，集成外部追踪系统，减轻嵌入式系统负担。",
                "效果是使学生在教育环境中探索机器人高阶方面，提升多学科技能。"
            ],
            "tags_zh": [
                "教育机器人",
                "全向轮机器人",
                "外部追踪系统",
                "机器人套件",
                "项目导向教学"
            ],
            "_index": 30
        },
        {
            "title": "ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?",
            "authors": [
                "Liu Yang",
                "Huiyu Duan",
                "Ran Tao",
                "Juntao Cheng",
                "Sijing Wu",
                "Yunhao Li",
                "Jing Liu",
                "Xiongkuo Min",
                "Guangtao Zhai"
            ],
            "arxiv_id": "2510.11549v1",
            "summary": "Omnidirectional images (ODIs) provide full 360x180 view which are widely\nadopted in VR, AR and embodied intelligence applications. While multi-modal\nlarge language models (MLLMs) have demonstrated remarkable performance on\nconventional 2D image and video understanding benchmarks, their ability to\ncomprehend the immersive environments captured by ODIs remains largely\nunexplored. To address this gap, we first present ODI-Bench, a novel\ncomprehensive benchmark specifically designed for omnidirectional image\nunderstanding. ODI-Bench contains 2,000 high-quality omnidirectional images and\nover 4,000 manually annotated question-answering (QA) pairs across 10\nfine-grained tasks, covering both general-level and spatial-level ODI\nunderstanding. Extensive experiments are conducted to benchmark 20\nrepresentative MLLMs, including proprietary and open-source models, under both\nclose-ended and open-ended settings. Experimental results reveal that current\nMLLMs still struggle to capture the immersive context provided by ODIs. To this\nend, we further introduce Omni-CoT, a training-free method which significantly\nenhances MLLMs' comprehension ability in the omnidirectional environment\nthrough chain-of-thought reasoning across both textual information and visual\ncues. Both the benchmark and the code will be released upon the publication.",
            "headline_zh": "提出ODI-Bench基准和Omni-CoT方法以评估和增强MLLMs在全景图像理解中的能力",
            "intro_zh": [
                "核心问题：多模态大语言模型在全景图像理解中表现不佳，缺乏沉浸式上下文捕捉能力",
                "方法要点：引入Omni-CoT，一种无需训练的方法，通过思维链推理结合文本和视觉线索提升理解",
                "实验或效果：在ODI-Bench上测试20个MLLMs，显示当前模型仍存在困难，Omni-CoT显著改善性能"
            ],
            "tags_zh": [
                "全景图像理解",
                "多模态大语言模型",
                "基准评估",
                "思维链推理",
                "沉浸式环境",
                "问答对数据集"
            ],
            "_index": 31
        },
        {
            "title": "NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning",
            "authors": [
                "Neil C. Janwani",
                "Varun Madabushi",
                "Maegan Tucker"
            ],
            "arxiv_id": "2510.11542v1",
            "summary": "Reinforcement learning (RL) has emerged as a powerful method to learn robust\ncontrol policies for bipedal locomotion. Yet, it can be difficult to tune\ndesired robot behaviors due to unintuitive and complex reward design. In\ncomparison, offline trajectory optimization methods, like Hybrid Zero Dynamics,\noffer more tuneable, interpretable, and mathematically grounded motion plans\nfor high-dimensional legged systems. However, these methods often remain\nbrittle to real-world disturbances like external perturbations.\n  In this work, we present NaviGait, a hierarchical framework that combines the\nstructure of trajectory optimization with the adaptability of RL for robust and\nintuitive locomotion control. NaviGait leverages a library of offline-optimized\ngaits and smoothly interpolates between them to produce continuous reference\nmotions in response to high-level commands. The policy provides both\njoint-level and velocity command residual corrections to modulate and stabilize\nthe reference trajectories in the gait library. One notable advantage of\nNaviGait is that it dramatically simplifies reward design by encoding rich\nmotion priors from trajectory optimization, reducing the need for finely tuned\nshaping terms and enabling more stable and interpretable learning. Our\nexperimental results demonstrate that NaviGait enables faster training compared\nto conventional and imitation-based RL, and produces motions that remain\nclosest to the original reference. Overall, by decoupling high-level motion\ngeneration from low-level correction, NaviGait offers a more scalable and\ngeneralizable approach for achieving dynamic and robust locomotion.",
            "headline_zh": "提出NaviGait框架，结合轨迹优化与强化学习，实现稳健双足运动控制。",
            "intro_zh": [
                "核心问题：强化学习奖励设计复杂，轨迹优化方法对扰动脆弱。",
                "方法要点：利用离线优化步态库，插值生成参考运动并添加残差修正。",
                "实验或效果：训练更快，运动更接近参考，提升稳健性和可解释性。"
            ],
            "tags_zh": [
                "双足运动控制",
                "强化学习",
                "轨迹优化",
                "步态库",
                "残差修正"
            ],
            "_index": 32
        },
        {
            "title": "Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization",
            "authors": [
                "Denglin Cheng",
                "Jiarong Kang",
                "Xiaobin Xiong"
            ],
            "arxiv_id": "2510.11539v1",
            "summary": "Accurate state estimation is critical for legged and aerial robots operating\nin dynamic, uncertain environments. A key challenge lies in specifying process\nand measurement noise covariances, which are typically unknown or manually\ntuned. In this work, we introduce a bi-level optimization framework that\njointly calibrates covariance matrices and kinematic parameters in an\nestimator-in-the-loop manner. The upper level treats noise covariances and\nmodel parameters as optimization variables, while the lower level executes a\nfull-information estimator. Differentiating through the estimator allows direct\noptimization of trajectory-level objectives, resulting in accurate and\nconsistent state estimates. We validate our approach on quadrupedal and\nhumanoid robots, demonstrating significantly improved estimation accuracy and\nuncertainty calibration compared to hand-tuned baselines. Our method unifies\nstate estimation, sensor, and kinematics calibration into a principled,\ndata-driven framework applicable across diverse robotic platforms.",
            "headline_zh": "提出双级优化框架以联合校准噪声协方差和运动学参数，提升腿式机器人状态估计精度",
            "intro_zh": [
                "核心问题：腿式和空中机器人在动态环境中状态估计需准确噪声协方差，但通常未知或手动调整",
                "方法要点：采用双级优化，上层优化噪声协方差和模型参数，下层执行全信息估计器",
                "实验或效果：在四足和人形机器人上验证，估计精度和不确定性校准显著优于手动基线"
            ],
            "tags_zh": [
                "状态估计",
                "噪声协方差校准",
                "双级优化",
                "腿式机器人",
                "运动学参数校准",
                "数据驱动框架"
            ],
            "_index": 33
        },
        {
            "title": "Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers",
            "authors": [
                "Chaofan Gan",
                "Zicheng Zhao",
                "Yuanpeng Tu",
                "Xi Chen",
                "Ziran Qin",
                "Tieyuan Chen",
                "Mehrtash Harandi",
                "Weiyao Lin"
            ],
            "arxiv_id": "2510.11538v1",
            "summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor visual generation. Recent observations reveal \\emph{Massive Activations}\n(MAs) in their internal feature maps, yet their function remains poorly\nunderstood. In this work, we systematically investigate these activations to\nelucidate their role in visual generation. We found that these massive\nactivations occur across all spatial tokens, and their distribution is\nmodulated by the input timestep embeddings. Importantly, our investigations\nfurther demonstrate that these massive activations play a key role in local\ndetail synthesis, while having minimal impact on the overall semantic content\nof output. Building on these insights, we propose \\textbf{D}etail\n\\textbf{G}uidance (\\textbf{DG}), a MAs-driven, training-free self-guidance\nstrategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG\nconstructs a degraded ``detail-deficient'' model by disrupting MAs and\nleverages it to guide the original network toward higher-quality detail\nsynthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG),\nenabling further refinements of fine-grained details. Extensive experiments\ndemonstrate that our DG consistently improves fine-grained detail quality\nacross various pre-trained DiTs (\\eg, SD3, SD3.5, and Flux).",
            "headline_zh": "提出细节引导策略以增强扩散变换器中的局部细节合成",
            "intro_zh": [
                "核心问题：扩散变换器内部特征图中的大规模激活功能未明，影响局部细节合成。",
                "方法要点：基于大规模激活构建训练免费自引导策略，通过破坏激活生成细节缺陷模型进行指导。",
                "实验或效果：在多种预训练扩散变换器上验证，细节质量一致提升，可与分类器自由引导结合。"
            ],
            "tags_zh": [
                "扩散变换器",
                "大规模激活",
                "局部细节合成",
                "自引导策略",
                "训练免费方法"
            ],
            "_index": 34
        },
        {
            "title": "IntersectioNDE: Learning Complex Urban Traffic Dynamics based on Interaction Decoupling Strategy",
            "authors": [
                "Enli Lin",
                "Ziyuan Yang",
                "Qiujing Lu",
                "Jianming Hu",
                "Shuo Feng"
            ],
            "arxiv_id": "2510.11534v1",
            "summary": "Realistic traffic simulation is critical for ensuring the safety and\nreliability of autonomous vehicles (AVs), especially in complex and diverse\nurban traffic environments. However, existing data-driven simulators face two\nkey challenges: a limited focus on modeling dense, heterogeneous interactions\nat urban intersections - which are prevalent, crucial, and practically\nsignificant in countries like China, featuring diverse agents including\nmotorized vehicles (MVs), non-motorized vehicles (NMVs), and pedestrians - and\nthe inherent difficulty in robustly learning high-dimensional joint\ndistributions for such high-density scenes, often leading to mode collapse and\nlong-term simulation instability. We introduce City Crossings Dataset\n(CiCross), a large-scale dataset collected from a real-world urban\nintersection, uniquely capturing dense, heterogeneous multi-agent interactions,\nparticularly with a substantial proportion of MVs, NMVs and pedestrians. Based\non this dataset, we propose IntersectioNDE (Intersection Naturalistic Driving\nEnvironment), a data-driven simulator tailored for complex urban intersection\nscenarios. Its core component is the Interaction Decoupling Strategy (IDS), a\ntraining paradigm that learns compositional dynamics from agent subsets,\nenabling the marginal-to-joint simulation. Integrated into a scene-aware\nTransformer network with specialized training techniques, IDS significantly\nenhances simulation robustness and long-term stability for modeling\nheterogeneous interactions. Experiments on CiCross show that IntersectioNDE\noutperforms baseline methods in simulation fidelity, stability, and its ability\nto replicate complex, distribution-level urban traffic dynamics.",
            "headline_zh": "提出IntersectioNDE模拟器，基于交互解耦策略学习复杂城市交叉口交通动态",
            "intro_zh": [
                "核心问题：现有模拟器难以建模密集异构交互和高维联合分布，导致模式崩溃和长期不稳定",
                "方法要点：引入交互解耦策略，从代理子集学习组合动态，集成场景感知Transformer网络",
                "实验或效果：在CiCross数据集上验证，模拟保真度、稳定性和复杂动态复现能力优于基线"
            ],
            "tags_zh": [
                "交通模拟",
                "交互解耦",
                "城市交叉口",
                "多智能体交互",
                "数据驱动模拟",
                "Transformer网络"
            ],
            "_index": 35
        },
        {
            "title": "DQ-NMPC: Dual-Quaternion NMPC for Quadrotor Flight",
            "authors": [
                "Luis F. Recalde",
                "Dhruv Agrawal",
                "Jon Arrizabalaga",
                "Guanrui Li"
            ],
            "arxiv_id": "2510.11525v1",
            "summary": "MAVs have great potential to assist humans in complex tasks, with\napplications ranging from logistics to emergency response. Their agility makes\nthem ideal for operations in complex and dynamic environments. However,\nachieving precise control in agile flights remains a significant challenge,\nparticularly due to the underactuated nature of quadrotors and the strong\ncoupling between their translational and rotational dynamics. In this work, we\npropose a novel NMPC framework based on dual-quaternions (DQ-NMPC) for\nquadrotor flight. By representing both quadrotor dynamics and the pose error\ndirectly on the dual-quaternion manifold, our approach enables a compact and\nglobally non-singular formulation that captures the quadrotor coupled dynamics.\nWe validate our approach through simulations and real-world experiments,\ndemonstrating better numerical conditioning and significantly improved tracking\nperformance, with reductions in position and orientation errors of up to 56.11%\nand 56.77%, compared to a conventional baseline NMPC method. Furthermore, our\ncontroller successfully handles aggressive trajectories, reaching maximum\nspeeds up to 13.66 m/s and accelerations reaching 4.2 g within confined space\nconditions of dimensions 11m x 4.5m x 3.65m under which the baseline controller\nfails.",
            "headline_zh": "提出基于对偶四元数的NMPC框架，以提升四旋翼飞行器在复杂环境中的精确控制性能。",
            "intro_zh": [
                "四旋翼飞行器在敏捷飞行中面临欠驱动和动力学耦合的精确控制挑战。",
                "使用对偶四元数流形表示动力学和姿态误差，实现紧凑且全局非奇异的控制公式。",
                "实验显示跟踪误差降低超56%，并在高速高加速场景中优于基线控制器。"
            ],
            "tags_zh": [
                "四旋翼控制",
                "非线性模型预测控制",
                "对偶四元数",
                "姿态跟踪",
                "敏捷飞行",
                "动力学耦合"
            ],
            "_index": 36
        },
        {
            "title": "mmWalk: Towards Multi-modal Multi-view Walking Assistance",
            "authors": [
                "Kedi Ying",
                "Ruiping Liu",
                "Chongyan Chen",
                "Mingzhe Tao",
                "Hao Shi",
                "Kailun Yang",
                "Jiaming Zhang",
                "Rainer Stiefelhagen"
            ],
            "arxiv_id": "2510.11520v1",
            "summary": "Walking assistance in extreme or complex environments remains a significant\nchallenge for people with blindness or low vision (BLV), largely due to the\nlack of a holistic scene understanding. Motivated by the real-world needs of\nthe BLV community, we build mmWalk, a simulated multi-modal dataset that\nintegrates multi-view sensor and accessibility-oriented features for outdoor\nsafe navigation. Our dataset comprises 120 manually controlled,\nscenario-categorized walking trajectories with 62k synchronized frames. It\ncontains over 559k panoramic images across RGB, depth, and semantic modalities.\nFurthermore, to emphasize real-world relevance, each trajectory involves\noutdoor corner cases and accessibility-specific landmarks for BLV users.\nAdditionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual\nquestion-answer triplets across 9 categories tailored for safe and informed\nwalking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)\nusing zero- and few-shot settings and found they struggle with our risk\nassessment and navigational tasks. We validate our mmWalk-finetuned model on\nreal-world datasets and show the effectiveness of our dataset for advancing\nmulti-modal walking assistance.",
            "headline_zh": "提出mmWalk多模态多视图数据集以解决盲人或低视力者户外安全导航问题",
            "intro_zh": [
                "核心问题：盲人或低视力者在复杂环境中行走缺乏整体场景理解，导致导航困难。",
                "方法要点：构建模拟多模态数据集，集成多视图传感器和可访问性特征，包含12万帧同步数据。",
                "实验或效果：评估视觉语言模型在风险任务中表现不佳，微调模型在真实数据集上验证有效性。"
            ],
            "tags_zh": [
                "多模态数据集",
                "户外导航",
                "视觉语言模型",
                "可访问性特征",
                "风险评估"
            ],
            "_index": 37
        },
        {
            "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference",
            "authors": [
                "Jianhao Yuan",
                "Fabio Pizzati",
                "Francesco Pinto",
                "Lars Kunze",
                "Ivan Laptev",
                "Paul Newman",
                "Philip Torr",
                "Daniele De Martini"
            ],
            "arxiv_id": "2510.11512v1",
            "summary": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.",
            "headline_zh": "提出LikePhys方法以评估视频扩散模型的直观物理理解能力",
            "intro_zh": [
                "核心问题：难以在生成中分离物理正确性与视觉外观，导致评估困难",
                "方法要点：使用去噪目标作为ELBO似然代理，区分物理有效与不可能视频",
                "实验或效果：在12个场景基准上，PPE指标与人类偏好强对齐，优于基线"
            ],
            "tags_zh": [
                "直观物理理解",
                "视频扩散模型",
                "似然评估",
                "物理基准",
                "训练免费方法"
            ],
            "_index": 38
        },
        {
            "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model",
            "authors": [
                "Ruiping Liu",
                "Junwei Zheng",
                "Yufan Chen",
                "Zirui Wang",
                "Kunyu Peng",
                "Kailun Yang",
                "Jiaming Zhang",
                "Marc Pollefeys",
                "Rainer Stiefelhagen"
            ],
            "arxiv_id": "2510.11509v1",
            "summary": "Physical environments and circumstances are fundamentally dynamic, yet\ncurrent 3D datasets and evaluation benchmarks tend to concentrate on either\ndynamic scenarios or dynamic situations in isolation, resulting in incomplete\ncomprehension. To overcome these constraints, we introduce Situat3DChange, an\nextensive dataset supporting three situation-aware change understanding tasks\nfollowing the perception-action model: 121K question-answer pairs, 36K change\ndescriptions for perception tasks, and 17K rearrangement instructions for the\naction task. To construct this large-scale dataset, Situat3DChange leverages\n11K human observations of environmental changes to establish shared mental\nmodels and shared situational awareness for human-AI collaboration. These\nobservations, enriched with egocentric and allocentric perspectives as well as\ncategorical and coordinate spatial relations, are integrated using an LLM to\nsupport understanding of situated changes. To address the challenge of\ncomparing pairs of point clouds from the same scene with minor changes, we\npropose SCReasoner, an efficient 3D MLLM approach that enables effective point\ncloud comparison with minimal parameter overhead and no additional tokens\nrequired for the language decoder. Comprehensive evaluation on Situat3DChange\ntasks highlights both the progress and limitations of MLLMs in dynamic scene\nand situation understanding. Additional experiments on data scaling and\ncross-domain transfer demonstrate the task-agnostic effectiveness of using\nSituat3DChange as a training dataset for MLLMs.",
            "headline_zh": "提出Situat3DChange数据集和SCReasoner方法以解决3D动态场景理解问题",
            "intro_zh": [
                "核心问题：现有3D数据集对动态环境和情境理解不完整，限制人机协作。",
                "方法要点：构建大规模数据集，集成多视角观察，并开发高效3D MLLM进行点云比较。",
                "实验或效果：评估显示MLLM在动态场景理解有进展，数据集具任务无关有效性。"
            ],
            "tags_zh": [
                "3D变化理解",
                "多模态大语言模型",
                "点云比较",
                "情境感知",
                "人机协作",
                "数据集构建"
            ],
            "_index": 39
        },
        {
            "title": "Towards Fast and Scalable Normal Integration using Continuous Components",
            "authors": [
                "Francesco Milano",
                "Jen Jen Chung",
                "Lionel Ott",
                "Roland Siegwart"
            ],
            "arxiv_id": "2510.11508v1",
            "summary": "Surface normal integration is a fundamental problem in computer vision,\ndealing with the objective of reconstructing a surface from its corresponding\nnormal map. Existing approaches require an iterative global optimization to\njointly estimate the depth of each pixel, which scales poorly to larger normal\nmaps. In this paper, we address this problem by recasting normal integration as\nthe estimation of relative scales of continuous components. By constraining\npixels belonging to the same component to jointly vary their scale, we\ndrastically reduce the number of optimization variables. Our framework includes\na heuristic to accurately estimate continuous components from the start, a\nstrategy to rebalance optimization terms, and a technique to iteratively merge\ncomponents to further reduce the size of the problem. Our method achieves\nstate-of-the-art results on the standard normal integration benchmark in as\nlittle as a few seconds and achieves one-order-of-magnitude speedup over\npixel-level approaches on large-resolution normal maps.",
            "headline_zh": "提出基于连续组件的法向积分方法以解决大规模法向图重建效率问题",
            "intro_zh": [
                "核心问题：现有法向积分方法依赖迭代全局优化，在大规模法向图上扩展性差。",
                "方法要点：将法向积分重构为连续组件相对尺度估计，减少优化变量数量。",
                "实验效果：在标准基准上实现最优结果，大分辨率法向图速度提升一个数量级。"
            ],
            "tags_zh": [
                "法向积分",
                "表面重建",
                "优化加速",
                "连续组件",
                "计算机视觉"
            ],
            "_index": 40
        },
        {
            "title": "Context-Aware Model-Based Reinforcement Learning for Autonomous Racing",
            "authors": [
                "Emran Yasser Moustafa",
                "Ivana Dusparic"
            ],
            "arxiv_id": "2510.11501v1",
            "summary": "Autonomous vehicles have shown promising potential to be a groundbreaking\ntechnology for improving the safety of road users. For these vehicles, as well\nas many other safety-critical robotic technologies, to be deployed in\nreal-world applications, we require algorithms that can generalize well to\nunseen scenarios and data. Model-based reinforcement learning algorithms (MBRL)\nhave demonstrated state-of-the-art performance and data efficiency across a\ndiverse set of domains. However, these algorithms have also shown\nsusceptibility to changes in the environment and its transition dynamics.\n  In this work, we explore the performance and generalization capabilities of\nMBRL algorithms for autonomous driving, specifically in the simulated\nautonomous racing environment, Roboracer (formerly F1Tenth). We frame the\nhead-to-head racing task as a learning problem using contextual Markov decision\nprocesses and parameterize the driving behavior of the adversaries using the\ncontext of the episode, thereby also parameterizing the transition and reward\ndynamics. We benchmark the behavior of MBRL algorithms in this environment and\npropose a novel context-aware extension of the existing literature, cMask. We\ndemonstrate that context-aware MBRL algorithms generalize better to\nout-of-distribution adversary behaviors relative to context-free approaches. We\nalso demonstrate that cMask displays strong generalization capabilities, as\nwell as further performance improvement relative to other context-aware MBRL\napproaches when racing against adversaries with in-distribution behaviors.",
            "headline_zh": "提出上下文感知模型强化学习cMask，提升自动驾驶赛车在未知对手行为下的泛化能力。",
            "intro_zh": [
                "核心问题：模型强化学习在环境动态变化时泛化能力不足，影响自动驾驶赛车安全。",
                "方法要点：使用上下文马尔可夫决策过程参数化对手行为，扩展为上下文感知MBRL算法。",
                "实验或效果：cMask在分布内外对手行为下均优于上下文无关及其他上下文感知方法。"
            ],
            "tags_zh": [
                "模型强化学习",
                "自动驾驶赛车",
                "上下文感知",
                "泛化能力",
                "马尔可夫决策过程"
            ],
            "_index": 41
        },
        {
            "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
            "authors": [
                "Zhiwei Jin",
                "Xiaohui Song",
                "Nan Wang",
                "Yafei Liu",
                "Chao Li",
                "Xin Li",
                "Ruichen Wang",
                "Zhihao Li",
                "Qi Qi",
                "Long Cheng",
                "Dongze Hao",
                "Quanlong Zheng",
                "Yanhao Zhang",
                "Haobo Ji",
                "Jian Ma",
                "Zhitong Zheng",
                "Zhenyi Lin",
                "Haolin Deng",
                "Xin Zou",
                "Xiaojie Yin",
                "Ruilin Wang",
                "Liankai Cai",
                "Haijing Liu",
                "Yuqing Qiu",
                "Ke Chen",
                "Zixian Li",
                "Chi Xie",
                "Huafei Li",
                "Chenxing Li",
                "Chuangchuang Wang",
                "Kai Tang",
                "Zhiguang Zhu",
                "Kai Tang",
                "Wenmei Gao",
                "Rui Wang",
                "Jun Wu",
                "Chao Liu",
                "Qin Xie",
                "Chen Chen",
                "Haonan Lu"
            ],
            "arxiv_id": "2510.11496v1",
            "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoR",
            "headline_zh": "提出AndesVL移动端多模态大模型，以解决边缘设备资源限制问题",
            "intro_zh": [
                "核心问题：云端大模型参数庞大，无法在移动设备上高效运行",
                "方法要点：基于Qwen3 LLM和视觉编码器，构建0.6B至4B参数模型",
                "实验或效果：在多项开源基准测试中达到同类模型领先水平"
            ],
            "tags_zh": [
                "移动端多模态模型",
                "参数高效设计",
                "视觉语言理解",
                "边缘计算优化",
                "多任务基准测试"
            ],
            "_index": 42
        },
        {
            "title": "Constraint-Aware Reinforcement Learning via Adaptive Action Scaling",
            "authors": [
                "Murad Dawood",
                "Usama Ahmed Siddiquie",
                "Shahram Khorshidi",
                "Maren Bennewitz"
            ],
            "arxiv_id": "2510.11491v1",
            "summary": "Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that\narise from exploration during training by reducing constraint violations while\nmaintaining task performance. Existing approaches typically rely on a single\npolicy to jointly optimize reward and safety, which can cause instability due\nto conflicting objectives, or they use external safety filters that override\nactions and require prior system knowledge. In this paper, we propose a modular\ncost-aware regulator that scales the agent's actions based on predicted\nconstraint violations, preserving exploration through smooth action modulation\nrather than overriding the policy. The regulator is trained to minimize\nconstraint violations while avoiding degenerate suppression of actions. Our\napproach integrates seamlessly with off-policy RL methods such as SAC and TD3,\nand achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion\ntasks with sparse costs, reducing constraint violations by up to 126 times\nwhile increasing returns by over an order of magnitude compared to prior\nmethods.",
            "headline_zh": "提出自适应动作缩放方法以解决安全强化学习中的约束冲突问题",
            "intro_zh": [
                "核心问题：现有方法因目标冲突或需先验知识，导致训练不稳定或约束违反",
                "方法要点：使用模块化成本感知调节器，平滑缩放动作以最小化约束违反",
                "实验效果：在Safety Gym任务中，约束违反减少126倍，回报提升超10倍"
            ],
            "tags_zh": [
                "安全强化学习",
                "动作缩放",
                "约束优化",
                "模块化调节器",
                "稀疏成本"
            ],
            "_index": 43
        },
        {
            "title": "Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning",
            "authors": [
                "Ardian Selmonaj",
                "Giacomo Del Rio",
                "Adrian Schneider",
                "Alessandro Antonucci"
            ],
            "arxiv_id": "2510.11474v1",
            "summary": "Achieving mission objectives in a realistic simulation of aerial combat is\nhighly challenging due to imperfect situational awareness and nonlinear flight\ndynamics. In this work, we introduce a novel 3D multi-agent air combat\nenvironment and a Hierarchical Multi-Agent Reinforcement Learning framework to\ntackle these challenges. Our approach combines heterogeneous agent dynamics,\ncurriculum learning, league-play, and a newly adapted training algorithm. To\nthis end, the decision-making process is organized into two abstraction levels:\nlow-level policies learn precise control maneuvers, while high-level policies\nissue tactical commands based on mission objectives. Empirical results show\nthat our hierarchical approach improves both learning efficiency and combat\nperformance in complex dogfight scenarios.",
            "headline_zh": "提出分层多智能体强化学习框架以解决真实空战模拟中的复杂决策问题",
            "intro_zh": [
                "核心问题：真实空战模拟中，不完美态势感知和非线性飞行动力学使任务目标达成困难。",
                "方法要点：采用分层决策，低层学习精确控制，高层基于任务目标发布战术命令。",
                "实验或效果：实证显示，该方法在复杂狗斗场景中提升学习效率和战斗性能。"
            ],
            "tags_zh": [
                "分层强化学习",
                "多智能体系统",
                "空战模拟",
                "课程学习",
                "联盟训练"
            ],
            "_index": 44
        },
        {
            "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment",
            "authors": [
                "Qing Li",
                "Huifang Feng",
                "Xun Gong",
                "Yu-Shen Liu"
            ],
            "arxiv_id": "2510.11473v1",
            "summary": "3D Gaussian Splatting has recently emerged as an efficient solution for\nhigh-quality and real-time novel view synthesis. However, its capability for\naccurate surface reconstruction remains underexplored. Due to the discrete and\nunstructured nature of Gaussians, supervision based solely on image rendering\nloss often leads to inaccurate geometry and inconsistent multi-view alignment.\nIn this work, we propose a novel method that enhances the geometric\nrepresentation of 3D Gaussians through view alignment (VA). Specifically, we\nincorporate edge-aware image cues into the rendering loss to improve surface\nboundary delineation. To enforce geometric consistency across views, we\nintroduce a visibility-aware photometric alignment loss that models occlusions\nand encourages accurate spatial relationships among Gaussians. To further\nmitigate ambiguities caused by lighting variations, we incorporate normal-based\nconstraints to refine the spatial orientation of Gaussians and improve local\nsurface estimation. Additionally, we leverage deep image feature embeddings to\nenforce cross-view consistency, enhancing the robustness of the learned\ngeometry under varying viewpoints and illumination. Extensive experiments on\nstandard benchmarks demonstrate that our method achieves state-of-the-art\nperformance in both surface reconstruction and novel view synthesis. The source\ncode is available at https://github.com/LeoQLi/VA-GS.",
            "headline_zh": "提出VA-GS方法，通过视图对齐增强3D高斯泼溅的几何表示，以改进表面重建和新视角合成。",
            "intro_zh": [
                "核心问题：3D高斯泼溅在图像渲染损失下几何不准确且多视图对齐不一致。",
                "方法要点：引入边缘感知渲染损失、可见性感知光度对齐损失和法向约束。",
                "实验或效果：在标准基准测试中实现表面重建和新视角合成的最先进性能。"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "表面重建",
                "新视角合成",
                "视图对齐",
                "几何表示",
                "多视图一致性"
            ],
            "_index": 45
        },
        {
            "title": "Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion",
            "authors": [
                "Tianpei Zhang",
                "Jufeng Zhao",
                "Yiming Zhu",
                "Guangmang Cui"
            ],
            "arxiv_id": "2510.11456v1",
            "summary": "Existing Infrared and Visible Image Fusion (IVIF) methods typically assume\nhigh-quality inputs. However, when handing degraded images, these methods\nheavily rely on manually switching between different pre-processing techniques.\nThis decoupling of degradation handling and image fusion leads to significant\nperformance degradation. In this paper, we propose a novel VLM-Guided\nDegradation-Coupled Fusion network (VGDCFusion), which tightly couples\ndegradation modeling with the fusion process and leverages vision-language\nmodels (VLMs) for degradation-aware perception and guided suppression.\nSpecifically, the proposed Specific-Prompt Degradation-Coupled Extractor\n(SPDCE) enables modality-specific degradation awareness and establishes a joint\nmodeling of degradation suppression and intra-modal feature extraction. In\nparallel, the Joint-Prompt Degradation-Coupled Fusion (JPDCF) facilitates\ncross-modal degradation perception and couples residual degradation filtering\nwith complementary cross-modal feature fusion. Extensive experiments\ndemonstrate that our VGDCFusion significantly outperforms existing\nstate-of-the-art fusion approaches under various degraded image scenarios. Our\ncode is available at https://github.com/Lmmh058/VGDCFusion.",
            "headline_zh": "提出VGDCFusion网络，耦合退化建模与图像融合以处理退化红外和可见光图像",
            "intro_zh": [
                "现有红外和可见光图像融合方法假设高质量输入，处理退化图像时性能下降",
                "使用VLM引导退化感知，SPDCE和JPDCF模块耦合退化抑制与特征提取融合",
                "实验表明VGDCFusion在多种退化场景下优于现有方法，代码已开源"
            ],
            "tags_zh": [
                "红外和可见光图像融合",
                "退化建模",
                "视觉语言模型",
                "退化感知",
                "特征融合"
            ],
            "_index": 46
        },
        {
            "title": "Enhancing Maritime Domain Awareness on Inland Waterways: A YOLO-Based Fusion of Satellite and AIS for Vessel Characterization",
            "authors": [
                "Geoffery Agorku",
                "Sarah Hernandez",
                "Hayley Hames",
                "Cade Wagner"
            ],
            "arxiv_id": "2510.11449v1",
            "summary": "Maritime Domain Awareness (MDA) for inland waterways remains challenged by\ncooperative system vulnerabilities. This paper presents a novel framework that\nfuses high-resolution satellite imagery with vessel trajectory data from the\nAutomatic Identification System (AIS). This work addresses the limitations of\nAIS-based monitoring by leveraging non-cooperative satellite imagery and\nimplementing a fusion approach that links visual detections with AIS data to\nidentify dark vessels, validate cooperative traffic, and support advanced MDA.\nThe You Only Look Once (YOLO) v11 object detection model is used to detect and\ncharacterize vessels and barges by vessel type, barge cover, operational\nstatus, barge count, and direction of travel. An annotated data set of 4,550\ninstances was developed from $5{,}973~\\mathrm{mi}^2$ of Lower Mississippi River\nimagery. Evaluation on a held-out test set demonstrated vessel classification\n(tugboat, crane barge, bulk carrier, cargo ship, and hopper barge) with an F1\nscore of 95.8\\%; barge cover (covered or uncovered) detection yielded an F1\nscore of 91.6\\%; operational status (staged or in motion) classification\nreached an F1 score of 99.4\\%. Directionality (upstream, downstream) yielded\n93.8\\% accuracy. The barge count estimation resulted in a mean absolute error\n(MAE) of 2.4 barges. Spatial transferability analysis across geographically\ndisjoint river segments showed accuracy was maintained as high as 98\\%. These\nresults underscore the viability of integrating non-cooperative satellite\nsensing with AIS fusion. This approach enables near-real-time fleet\ninventories, supports anomaly detection, and generates high-quality data for\ninland waterway surveillance. Future work will expand annotated datasets,\nincorporate temporal tracking, and explore multi-modal deep learning to further\nenhance operational scalability.",
            "headline_zh": "提出基于YOLO的卫星与AIS融合框架，以增强内河海事感知能力。",
            "intro_zh": [
                "核心问题：内河海事感知受限于合作系统漏洞，难以监测非合作船只。",
                "方法要点：融合高分辨率卫星图像与AIS轨迹数据，使用YOLO v11检测和表征船只。",
                "实验或效果：在测试集上，船只分类F1达95.8%，空间可转移性高达98%。"
            ],
            "tags_zh": [
                "目标检测",
                "数据融合",
                "海事感知",
                "卫星图像",
                "AIS数据",
                "YOLO模型"
            ],
            "_index": 47
        },
        {
            "title": "A Faster and More Reliable Middleware for Autonomous Driving Systems",
            "authors": [
                "Yuankai He",
                "Hanlin Chen",
                "Weisong Shi"
            ],
            "arxiv_id": "2510.11448v1",
            "summary": "Ensuring safety in high-speed autonomous vehicles requires rapid control\nloops and tightly bounded delays from perception to actuation. Many open-source\nautonomy systems rely on ROS 2 middleware; when multiple sensor and control\nnodes share one compute unit, ROS 2 and its DDS transports add significant\n(de)serialization, copying, and discovery overheads, shrinking the available\ntime budget. We present Sensor-in-Memory (SIM), a shared-memory transport\ndesigned for intra-host pipelines in autonomous vehicles. SIM keeps sensor data\nin native memory layouts (e.g., cv::Mat, PCL), uses lock-free bounded double\nbuffers that overwrite old data to prioritize freshness, and integrates into\nROS 2 nodes with four lines of code. Unlike traditional middleware, SIM\noperates beside ROS 2 and is optimized for applications where data freshness\nand minimal latency outweigh guaranteed completeness. SIM provides sequence\nnumbers, a writer heartbeat, and optional checksums to ensure ordering,\nliveness, and basic integrity. On an NVIDIA Jetson Orin Nano, SIM reduces\ndata-transport latency by up to 98% compared to ROS 2 zero-copy transports such\nas FastRTPS and Zenoh, lowers mean latency by about 95%, and narrows\n95th/99th-percentile tail latencies by around 96%. In tests on a\nproduction-ready Level 4 vehicle running Autoware.Universe, SIM increased\nlocalization frequency from 7.5 Hz to 9.5 Hz. Applied across all\nlatency-critical modules, SIM cut average perception-to-decision latency from\n521.91 ms to 290.26 ms, reducing emergency braking distance at 40 mph (64 km/h)\non dry concrete by 13.6 ft (4.14 m).",
            "headline_zh": "提出Sensor-in-Memory共享内存传输以降低自动驾驶系统延迟",
            "intro_zh": [
                "ROS 2中间件在单计算单元多节点时引入高延迟，影响自动驾驶安全控制循环",
                "SIM使用共享内存、原生数据布局和锁自由双缓冲，优先数据新鲜度，集成ROS 2",
                "实验显示SIM显著降低延迟，提升定位频率，减少紧急制动距离"
            ],
            "tags_zh": [
                "自动驾驶系统",
                "共享内存传输",
                "ROS 2优化",
                "低延迟中间件",
                "数据新鲜度",
                "实时控制"
            ],
            "_index": 48
        },
        {
            "title": "A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation in Smart Cities",
            "authors": [
                "Shih-Chieh Sun",
                "Yun-Cheng Tsai"
            ],
            "arxiv_id": "2510.11421v1",
            "summary": "This paper presents an AI-driven IoT robotic teleoperation system designed\nfor real-time remote manipulation and intelligent visual monitoring, tailored\nfor smart city applications. The architecture integrates a Flutter-based\ncross-platform mobile interface with MQTT-based control signaling and WebRTC\nvideo streaming via the LiveKit framework. A YOLOv11-nano model is deployed for\nlightweight object detection, enabling real-time perception with annotated\nvisual overlays delivered to the user interface. Control commands are\ntransmitted via MQTT to an ESP8266-based actuator node, which coordinates\nmulti-axis robotic arm motion through an Arduino Mega2560 controller. The\nbackend infrastructure is hosted on DigitalOcean, ensuring scalable cloud\norchestration and stable global communication. Latency evaluations conducted\nunder both local and international VPN scenarios (including Hong Kong, Japan,\nand Belgium) demonstrate actuator response times as low as 0.2 seconds and\ntotal video latency under 1.2 seconds, even across high-latency networks. This\nlow-latency dual-protocol design ensures responsive closed-loop interaction and\nrobust performance in distributed environments. Unlike conventional\nteleoperation platforms, the proposed system emphasizes modular deployment,\nreal-time AI sensing, and adaptable communication strategies, making it\nwell-suited for smart city scenarios such as remote infrastructure inspection,\npublic equipment servicing, and urban automation. Future enhancements will\nfocus on edge-device deployment, adaptive routing, and integration with\ncity-scale IoT networks to enhance resilience and scalability.",
            "headline_zh": "提出模块化AIoT框架，实现低延迟实时机器人遥操作，适用于智慧城市场景。",
            "intro_zh": [
                "核心问题：智慧城市中远程机器人操作需低延迟实时交互与智能感知。",
                "方法要点：集成Flutter移动界面、MQTT控制、WebRTC视频流与YOLOv11-nano对象检测。",
                "实验或效果：在VPN场景下，执行器响应低至0.2秒，视频延迟低于1.2秒。"
            ],
            "tags_zh": [
                "机器人遥操作",
                "AIoT框架",
                "低延迟通信",
                "实时对象检测",
                "智慧城市应用",
                "模块化部署"
            ],
            "_index": 49
        },
        {
            "title": "Robust Ego-Exo Correspondence with Long-Term Memory",
            "authors": [
                "Yijun Hu",
                "Bing Fan",
                "Xin Gu",
                "Haiqing Ren",
                "Dongfang Liu",
                "Heng Fan",
                "Libo Zhang"
            ],
            "arxiv_id": "2510.11417v1",
            "summary": "Establishing object-level correspondence between egocentric and exocentric\nviews is essential for intelligent assistants to deliver precise and intuitive\nvisual guidance. However, this task faces numerous challenges, including\nextreme viewpoint variations, occlusions, and the presence of small objects.\nExisting approaches usually borrow solutions from video object segmentation\nmodels, but still suffer from the aforementioned challenges. Recently, the\nSegment Anything Model 2 (SAM 2) has shown strong generalization capabilities\nand excellent performance in video object segmentation. Yet, when simply\napplied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe\ndifficulties due to ineffective ego-exo feature fusion and limited long-term\nmemory capacity, especially for long videos. Addressing these problems, we\npropose a novel EEC framework based on SAM 2 with long-term memories by\npresenting a dual-memory architecture and an adaptive feature routing module\ninspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features\n(i) a Memory-View MoE module which consists of a dual-branch routing mechanism\nto adaptively assign contribution weights to each expert feature along both\nchannel and spatial dimensions, and (ii) a dual-memory bank system with a\nsimple yet effective compression strategy to retain critical long-term\ninformation while eliminating redundancy. In the extensive experiments on the\nchallenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new\nstate-of-the-art results and significantly outperforms existing methods and the\nSAM 2 baseline, showcasing its strong generalization across diverse scenarios.\nOur code and model are available at https://github.com/juneyeeHu/LM-EEC.",
            "headline_zh": "提出基于SAM 2的长记忆EEC框架，解决视角变化与长视频挑战",
            "intro_zh": [
                "核心问题：视角变化、遮挡和小物体导致ego-exo对应困难，现有方法性能不足",
                "方法要点：引入双记忆架构和自适应特征路由模块，提升特征融合与长期记忆能力",
                "实验效果：在EgoExo4D基准上实现SOTA，显著优于SAM 2和现有方法"
            ],
            "tags_zh": [
                "ego-exo对应",
                "长视频理解",
                "双记忆架构",
                "自适应特征路由",
                "SAM 2改进"
            ],
            "_index": 50
        },
        {
            "title": "Path and Motion Optimization for Efficient Multi-Location Inspection with Humanoid Robots",
            "authors": [
                "Jiayang Wu",
                "Jiongye Li",
                "Shibowen Zhang",
                "Zhicheng He",
                "Zaijin Wang",
                "Xiaokun Leng",
                "Hangxin Liu",
                "Jingwen Zhang",
                "Jiayi Wang",
                "Song-Chun Zhu",
                "Yao Su"
            ],
            "arxiv_id": "2510.11401v1",
            "summary": "This paper proposes a novel framework for humanoid robots to execute\ninspection tasks with high efficiency and millimeter-level precision. The\napproach combines hierarchical planning, time-optimal standing position\ngeneration, and integrated \\ac{mpc} to achieve high speed and precision. A\nhierarchical planning strategy, leveraging \\ac{ik} and \\ac{mip}, reduces\ncomputational complexity by decoupling the high-dimensional planning problem. A\nnovel MIP formulation optimizes standing position selection and trajectory\nlength, minimizing task completion time. Furthermore, an MPC system with\nsimplified kinematics and single-step position correction ensures\nmillimeter-level end-effector tracking accuracy. Validated through simulations\nand experiments on the Kuavo 4Pro humanoid platform, the framework demonstrates\nlow time cost and a high success rate in multi-location tasks, enabling\nefficient and precise execution of complex industrial operations.",
            "headline_zh": "提出人形机器人多位置检测框架，结合分层规划与MPC实现高效毫米级精度",
            "intro_zh": [
                "核心问题：人形机器人在多位置检测任务中需兼顾高效率和毫米级精度，计算复杂度高",
                "方法要点：采用分层规划、MIP优化站立位置和MPC系统，降低复杂度并提升跟踪精度",
                "实验或效果：在Kuavo 4Pro平台上验证，任务完成时间短且成功率高"
            ],
            "tags_zh": [
                "人形机器人",
                "分层规划",
                "模型预测控制",
                "多位置检测",
                "轨迹优化"
            ],
            "_index": 51
        },
        {
            "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
            "authors": [
                "Junpeng Liu",
                "Yuzhong Zhao",
                "Bowen Cao",
                "Jiayu Ding",
                "Yilin Jia",
                "Tengchao Lv",
                "Yupan Huang",
                "Shaohan Huang",
                "Nan Yang",
                "Li Dong",
                "Lei Cui",
                "Tao Ge",
                "Xun Wang",
                "Huitian Jiao",
                "Sun Mao",
                "FNU Kartik",
                "Si-Qing Chen",
                "Wai Lam",
                "Furu Wei"
            ],
            "arxiv_id": "2510.11391v1",
            "summary": "Recent advances in agentic workflows have enabled the automation of tasks\nsuch as professional document generation. However, they primarily focus on\ntextual quality, neglecting visual structure and style, which are crucial for\nreadability and engagement. This gap arises mainly from the absence of suitable\nreward models to guide agentic workflows toward producing documents with\nstronger structural and stylistic quality. To address this, we propose\nDocReward, a document reward model that evaluates documents based on their\nstructure and style. We construct a multi-domain dataset DocPair of 117K paired\ndocuments, covering 32 domains and 267 document types, each including a high-\nand low-professionalism document with identical content but different structure\nand style. This enables the model to evaluate professionalism comprehensively,\nand in a textual-quality-agnostic way. DocReward is trained using the\nBradley-Terry loss to score documents, penalizing predictions that contradict\nthe annotated ranking. To assess the performance of reward models, we create a\ntest dataset containing document bundles ranked by well-educated human\nevaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6\nand 19.4 percentage points, respectively, demonstrating its superiority over\nbaselines. In an extrinsic evaluation of document generation, DocReward\nachieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%\nwin rate, demonstrating its utility in guiding generation agents toward\nproducing human-preferred documents.",
            "headline_zh": "提出DocReward文档奖励模型，以解决代理工作流中视觉结构与风格评估的缺失问题。",
            "intro_zh": [
                "核心问题：代理工作流自动化文档生成时，缺乏对视觉结构与风格的评估，影响可读性与参与度。",
                "方法要点：构建多领域DocPair数据集，使用Bradley-Terry损失训练模型，实现文本质量无关的专业性评分。",
                "实验或效果：在测试集上，DocReward准确率超越GPT-4o和GPT-5，并在文档生成评估中赢得更高胜率。"
            ],
            "tags_zh": [
                "文档奖励模型",
                "代理工作流",
                "多领域数据集",
                "Bradley-Terry损失",
                "文档生成评估"
            ],
            "_index": 52
        },
        {
            "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference",
            "authors": [
                "Wenyuan Zhang",
                "Jimin Tang",
                "Weiqi Zhang",
                "Yi Fang",
                "Yu-Shen Liu",
                "Zhizhong Han"
            ],
            "arxiv_id": "2510.11387v1",
            "summary": "Modeling reflections from 2D images is essential for photorealistic rendering\nand novel view synthesis. Recent approaches enhance Gaussian primitives with\nreflection-related material attributes to enable physically based rendering\n(PBR) with Gaussian Splatting. However, the material inference often lacks\nsufficient constraints, especially under limited environment modeling,\nresulting in illumination aliasing and reduced generalization. In this work, we\nrevisit the problem from a multi-view perspective and show that multi-view\nconsistent material inference with more physically-based environment modeling\nis key to learning accurate reflections with Gaussian Splatting. To this end,\nwe enforce 2D Gaussians to produce multi-view consistent material maps during\ndeferred shading. We also track photometric variations across views to identify\nhighly reflective regions, which serve as strong priors for reflection strength\nterms. To handle indirect illumination caused by inter-object occlusions, we\nfurther introduce an environment modeling strategy through ray tracing with\n2DGS, enabling photorealistic rendering of indirect radiance. Experiments on\nwidely used benchmarks show that our method faithfully recovers both\nillumination and geometry, achieving state-of-the-art rendering quality in\nnovel views synthesis.",
            "headline_zh": "提出多视角一致材料推断方法以提升高斯溅射中的反射建模",
            "intro_zh": [
                "核心问题：高斯溅射中材料推断缺乏约束，导致光照锯齿和泛化性差。",
                "方法要点：强制多视角一致材料映射，并利用光变追踪高反射区域。",
                "实验效果：在基准测试中实现新视角合成的最先进渲染质量。"
            ],
            "tags_zh": [
                "高斯溅射",
                "材料推断",
                "多视角一致性",
                "反射建模",
                "新视角合成"
            ],
            "_index": 53
        },
        {
            "title": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment",
            "authors": [
                "Shijie Zhao",
                "Xuanyu Zhang",
                "Weiqi Li",
                "Junlin Li",
                "Li Zhang",
                "Tianfan Xue",
                "Jian Zhang"
            ],
            "arxiv_id": "2510.11369v1",
            "summary": "Reasoning-based image quality assessment (IQA) models trained through\nreinforcement learning (RL) exhibit exceptional generalization, yet the\nunderlying mechanisms and critical factors driving this capability remain\nunderexplored in current research. Moreover, despite their superior\nperformance, these models incur inference energy usage and latency orders of\nmagnitude higher than their earlier counterparts, restricting their deployment\nin specific scenarios. Through extensive experiments, this paper verifies and\nelaborates that through RL training, MLLMs leverage their reasoning capability\nto convert redundant visual representations into compact, cross-domain aligned\ntext representations. This conversion is precisely the source of the\ngeneralization exhibited by these reasoning-based IQA models. Building on this\nfundamental insight, we propose a novel algorithm, RALI, which employs\ncontrastive learning to directly align images with these generalizable text\nrepresentations learned by RL. This approach eliminates the reliance on\nreasoning processes and even obviates the need to load an LLM. For the quality\nscoring task, this framework achieves generalization performance comparable to\nreasoning-based models while requiring less than 5% of their model parameters\nand inference time.",
            "headline_zh": "提出RALI算法，通过对比学习对齐图像与文本表示，提升图像质量评估的泛化与效率。",
            "intro_zh": [
                "基于推理的IQA模型泛化强但机制不明，且推理能耗高限制部署。",
                "RL训练使MLLMs将冗余视觉表示转为紧凑文本表示，驱动泛化。",
                "RALI无需推理过程，参数与时间减少95%，泛化性能媲美推理模型。"
            ],
            "tags_zh": [
                "图像质量评估",
                "强化学习",
                "对比学习",
                "多模态表示对齐",
                "模型泛化",
                "推理效率优化"
            ],
            "_index": 54
        },
        {
            "title": "Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation",
            "authors": [
                "Joshua Niemeijer",
                "Jan Ehrhardt",
                "Heinz Handels",
                "Hristina Uzunova"
            ],
            "arxiv_id": "2510.11346v1",
            "summary": "Generative Models are a valuable tool for the controlled creation of\nhigh-quality image data. Controlled diffusion models like the ControlNet have\nallowed the creation of labeled distributions. Such synthetic datasets can\naugment the original training distribution when discriminative models, like\nsemantic segmentation, are trained. However, this augmentation effect is\nlimited since ControlNets tend to reproduce the original training distribution.\n  This work introduces a method to utilize data from unlabeled domains to train\nControlNets by introducing the concept of uncertainty into the control\nmechanism. The uncertainty indicates that a given image was not part of the\ntraining distribution of a downstream task, e.g., segmentation. Thus, two types\nof control are engaged in the final network: an uncertainty control from an\nunlabeled dataset and a semantic control from the labeled dataset. The\nresulting ControlNet allows us to create annotated data with high uncertainty\nfrom the target domain, i.e., synthetic data from the unlabeled distribution\nwith labels. In our scenario, we consider retinal OCTs, where typically\nhigh-quality Spectralis images are available with given ground truth\nsegmentations, enabling the training of segmentation networks. The recent\ndevelopment in Home-OCT devices, however, yields retinal OCTs with lower\nquality and a large domain shift, such that out-of-the-pocket segmentation\nnetworks cannot be applied for this type of data. Synthesizing annotated images\nfrom the Home-OCT domain using the proposed approach closes this gap and leads\nto significantly improved segmentation results without adding any further\nsupervision. The advantage of uncertainty-guidance becomes obvious when\ncompared to style transfer: it enables arbitrary domain shifts without any\nstrict learning of an image style. This is also demonstrated in a traffic scene\nexperiment.",
            "headline_zh": "提出不确定性感知ControlNet，利用未标注数据生成合成图像以弥合领域差距",
            "intro_zh": [
                "核心问题：ControlNet生成图像时易复制原训练分布，限制合成数据对下游任务（如分割）的增强效果",
                "方法要点：引入不确定性控制机制，结合未标注和标注数据训练，生成高不确定性目标域标注图像",
                "实验或效果：在视网膜OCT和交通场景实验中，显著提升分割性能，无需额外监督"
            ],
            "tags_zh": [
                "ControlNet",
                "不确定性控制",
                "合成图像生成",
                "领域适应",
                "语义分割",
                "视网膜OCT"
            ],
            "_index": 55
        },
        {
            "title": "MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression",
            "authors": [
                "Hai Dang Nguyen",
                "Nguyen Dang Huy Pham",
                "The Minh Duc Nguyen",
                "Dac Thai Nguyen",
                "Hang Thi Nguyen",
                "Duong M. Nguyen"
            ],
            "arxiv_id": "2510.11344v1",
            "summary": "Spatial Transcriptomics (ST) enables the measurement of gene expression while\npreserving spatial information, offering critical insights into tissue\narchitecture and disease pathology. Recent developments have explored the use\nof hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict\ntranscriptome-wide gene expression profiles through deep neural networks. This\ntask is commonly framed as a regression problem, where each input corresponds\nto a localized image patch extracted from the WSI. However, predicting spatial\ngene expression from histological images remains a challenging problem due to\nthe significant modality gap between visual features and molecular signals.\nRecent studies have attempted to incorporate both local and global information\ninto predictive models. Nevertheless, existing methods still suffer from two\nkey limitations: (1) insufficient granularity in local feature extraction, and\n(2) inadequate coverage of global spatial context. In this work, we propose a\nnovel framework, MMAP (Multi-MAgnification and Prototype-enhanced\narchitecture), that addresses both challenges simultaneously. To enhance local\nfeature granularity, MMAP leverages multi-magnification patch representations\nthat capture fine-grained histological details. To improve global contextual\nunderstanding, it learns a set of latent prototype embeddings that serve as\ncompact representations of slide-level information. Extensive experimental\nresults demonstrate that MMAP consistently outperforms all existing\nstate-of-the-art methods across multiple evaluation metrics, including Mean\nAbsolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation\nCoefficient (PCC).",
            "headline_zh": "提出MMAP架构，通过多放大率和原型增强预测空间基因表达。",
            "intro_zh": [
                "核心问题：组织学图像与分子信号间模态差距大，现有方法局部特征粒度不足、全局上下文覆盖不充分。",
                "方法要点：利用多放大率补丁增强局部特征，学习原型嵌入捕获全局空间上下文。",
                "实验或效果：在MAE、MSE和PCC等指标上优于现有方法，验证了有效性。"
            ],
            "tags_zh": [
                "空间转录组学",
                "基因表达预测",
                "多放大率学习",
                "原型嵌入",
                "深度学习",
                "组织学图像分析"
            ],
            "_index": 56
        },
        {
            "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models",
            "authors": [
                "Haomin Wang",
                "Jinhui Yin",
                "Qi Wei",
                "Wenguang Zeng",
                "Lixin Gu",
                "Shenglong Ye",
                "Zhangwei Gao",
                "Yaohui Wang",
                "Yanting Zhang",
                "Yuanqi Li",
                "Yanwen Guo",
                "Wenhai Wang",
                "Kai Chen",
                "Yu Qiao",
                "Hongjie Zhang"
            ],
            "arxiv_id": "2510.11341v1",
            "summary": "General SVG modeling remains challenging due to fragmented datasets, limited\ntransferability of methods across tasks, and the difficulty of handling\nstructural complexity. In response, we leverage the strong transfer and\ngeneralization capabilities of multimodal large language models (MLLMs) to\nachieve unified modeling for SVG understanding, editing, and generation. We\npresent the InternSVG family, an integrated data-benchmark-model suite. At its\ncore is SAgoge, the largest and most comprehensive multimodal dataset for SVG\ntasks, encompassing both static graphics and dynamic animations. It covers\nicons, long-sequence illustrations, scientific diagrams, and dynamic\nanimations, supporting tasks of varied difficulty levels and providing deeper\nhierarchies with richer attributes compared to previous datasets. Based on this\nresource, we introduce SArena, a companion benchmark with comprehensive task\ndefinitions and standardized evaluation that aligns with the domains and\ndifficulty spectrum covered by SAgoge. Building on these foundations, we\npropose InternSVG, a unified MLLM for SVG understanding, editing, and\ngeneration with SVG-specific special tokens, subword-based embedding\ninitialization, and a two-stage training strategy that progresses from short\nstatic SVGs to long-sequence illustrations and complex animations. This unified\nformulation induces positive transfer and improves overall performance.\nExperiments on SArena and prior benchmark confirm that InternSVG achieves\nsubstantial gains and consistently outperforms leading open and proprietary\ncounterparts.",
            "headline_zh": "提出InternSVG以统一SVG任务，利用MLLM实现理解、编辑和生成。",
            "intro_zh": [
                "核心问题：SVG建模因数据集碎片化、方法迁移性差和结构复杂而具挑战性。",
                "方法要点：构建SAgoge数据集和SArena基准，采用SVG专用令牌和两阶段训练策略。",
                "实验或效果：在SArena和先前基准上，InternSVG显著优于现有开源和专有模型。"
            ],
            "tags_zh": [
                "SVG统一建模",
                "多模态大语言模型",
                "SVG数据集",
                "SVG基准",
                "SVG生成",
                "SVG编辑"
            ],
            "_index": 57
        },
        {
            "title": "REACT3D: Recovering Articulations for Interactive Physical 3D Scenes",
            "authors": [
                "Zhao Huang",
                "Boyang Sun",
                "Alexandros Delitzas",
                "Jiaqi Chen",
                "Marc Pollefeys"
            ],
            "arxiv_id": "2510.11340v1",
            "summary": "Interactive 3D scenes are increasingly vital for embodied intelligence, yet\nexisting datasets remain limited due to the labor-intensive process of\nannotating part segmentation, kinematic types, and motion trajectories. We\npresent REACT3D, a scalable zero-shot framework that converts static 3D scenes\ninto simulation-ready interactive replicas with consistent geometry, enabling\ndirect use in diverse downstream tasks. Our contributions include: (i)\nopenable-object detection and segmentation to extract candidate movable parts\nfrom static scenes, (ii) articulation estimation that infers joint types and\nmotion parameters, (iii) hidden-geometry completion followed by interactive\nobject assembly, and (iv) interactive scene integration in widely supported\nformats to ensure compatibility with standard simulation platforms. We achieve\nstate-of-the-art performance on detection/segmentation and articulation metrics\nacross diverse indoor scenes, demonstrating the effectiveness of our framework\nand providing a practical foundation for scalable interactive scene generation,\nthereby lowering the barrier to large-scale research on articulated scene\nunderstanding. Our project page is\n\\textit{\\hypersetup{urlcolor=black}\\href{https://react3d.github.io/}{react3d.github.io}}.",
            "headline_zh": "提出REACT3D框架，将静态3D场景转换为交互式物理场景，以解决标注成本高的问题。",
            "intro_zh": [
                "核心问题：交互式3D场景标注成本高，限制数据集规模和下游任务应用。",
                "方法要点：包括可开启物体检测、关节估计、隐藏几何补全和交互场景集成。",
                "实验或效果：在检测/分割和关节指标上达到先进水平，支持大规模场景生成。"
            ],
            "tags_zh": [
                "3D场景交互",
                "关节估计",
                "零样本学习",
                "物理模拟",
                "物体分割"
            ],
            "_index": 58
        },
        {
            "title": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data",
            "authors": [
                "Ruizhe Liu",
                "Pei Zhou",
                "Qian Luo",
                "Li Sun",
                "Jun Cen",
                "Yibing Song",
                "Yanchao Yang"
            ],
            "arxiv_id": "2510.11321v1",
            "summary": "Effective generalization in robotic manipulation requires representations\nthat capture invariant patterns of interaction across environments and tasks.\nWe present a self-supervised framework for learning hierarchical manipulation\nconcepts that encode these invariant patterns through cross-modal sensory\ncorrelations and multi-level temporal abstractions without requiring human\nannotation. Our approach combines a cross-modal correlation network that\nidentifies persistent patterns across sensory modalities with a multi-horizon\npredictor that organizes representations hierarchically across temporal scales.\nManipulation concepts learned through this dual structure enable policies to\nfocus on transferable relational patterns while maintaining awareness of both\nimmediate actions and longer-term goals. Empirical evaluation across simulated\nbenchmarks and real-world deployments demonstrates significant performance\nimprovements with our concept-enhanced policies. Analysis reveals that the\nlearned concepts resemble human-interpretable manipulation primitives despite\nreceiving no semantic supervision. This work advances both the understanding of\nrepresentation learning for manipulation and provides a practical approach to\nenhancing robotic performance in complex scenarios.",
            "headline_zh": "提出自监督框架HiMaCon，从无标签多模态数据学习层次化操作概念以提升机器人泛化能力",
            "intro_zh": [
                "核心问题：机器人操作需捕捉跨环境和任务的不变交互模式以实现有效泛化",
                "方法要点：结合跨模态相关网络和多尺度时序预测器，无监督学习层次化操作概念",
                "实验或效果：在仿真和真实部署中，概念增强策略显著提升性能，概念类似人类可解释原语"
            ],
            "tags_zh": [
                "机器人操作",
                "自监督学习",
                "层次化表示",
                "多模态数据",
                "时序抽象",
                "泛化能力"
            ],
            "_index": 59
        },
        {
            "title": "Adap-RPF: Adaptive Trajectory Sampling for Robot Person Following in Dynamic Crowded Environments",
            "authors": [
                "Weixi Situ",
                "Hanjing Ye",
                "Jianwei Peng",
                "Yu Zhan",
                "Hong Zhang"
            ],
            "arxiv_id": "2510.11308v1",
            "summary": "Robot person following (RPF) is a core capability in human-robot interaction,\nenabling robots to assist users in daily activities, collaborative work, and\nother service scenarios. However, achieving practical RPF remains challenging\ndue to frequent occlusions, particularly in dynamic and crowded environments.\nExisting approaches often rely on fixed-point following or sparse\ncandidate-point selection with oversimplified heuristics, which cannot\nadequately handle complex occlusions caused by moving obstacles such as\npedestrians. To address these limitations, we propose an adaptive trajectory\nsampling method that generates dense candidate points within socially aware\nzones and evaluates them using a multi-objective cost function. Based on the\noptimal point, a person-following trajectory is estimated relative to the\npredicted motion of the target. We further design a prediction-aware model\npredictive path integral (MPPI) controller that simultaneously tracks this\ntrajectory and proactively avoids collisions using predicted pedestrian\nmotions. Extensive experiments show that our method outperforms\nstate-of-the-art baselines in smoothness, safety, robustness, and human\ncomfort, with its effectiveness further demonstrated on a mobile robot in\nreal-world scenarios.",
            "headline_zh": "提出自适应轨迹采样方法以解决动态拥挤环境中机器人跟随的遮挡问题",
            "intro_zh": [
                "核心问题：动态拥挤环境中频繁遮挡导致机器人跟随困难",
                "方法要点：生成密集候选点并评估，结合预测设计MPPI控制器",
                "实验或效果：在平滑性、安全性、鲁棒性和人类舒适度上优于基线"
            ],
            "tags_zh": [
                "机器人跟随",
                "轨迹采样",
                "遮挡处理",
                "模型预测控制",
                "动态环境"
            ],
            "_index": 60
        },
        {
            "title": "Rotor-Failure-Aware Quadrotors Flight in Unknown Environments",
            "authors": [
                "Xiaobin Zhou",
                "Miao Wang",
                "Chengao Li",
                "Can Cui",
                "Ruibin Zhang",
                "Yongchao Wang",
                "Chao Xu",
                "Fei Gao"
            ],
            "arxiv_id": "2510.11306v1",
            "summary": "Rotor failures in quadrotors may result in high-speed rotation and vibration\ndue to rotor imbalance, which introduces significant challenges for autonomous\nflight in unknown environments. The mainstream approaches against rotor\nfailures rely on fault-tolerant control (FTC) and predefined trajectory\ntracking. To the best of our knowledge, online failure detection and diagnosis\n(FDD), trajectory planning, and FTC of the post-failure quadrotors in unknown\nand complex environments have not yet been achieved. This paper presents a\nrotor-failure-aware quadrotor navigation system designed to mitigate the\nimpacts of rotor imbalance. First, a composite FDD-based nonlinear model\npredictive controller (NMPC), incorporating motor dynamics, is designed to\nensure fast failure detection and flight stability. Second, a\nrotor-failure-aware planner is designed to leverage FDD results and\nspatial-temporal joint optimization, while a LiDAR-based quadrotor platform\nwith four anti-torque plates is designed to enable reliable perception under\nhigh-speed rotation. Lastly, extensive benchmarks against state-of-the-art\nmethods highlight the superior performance of the proposed approach in\naddressing rotor failures, including propeller unloading and motor stoppage.\nThe experimental results demonstrate, for the first time, that our approach\nenables autonomous quadrotor flight with rotor failures in challenging\nenvironments, including cluttered rooms and unknown forests.",
            "headline_zh": "提出转子故障感知四旋翼导航系统，以在未知环境中实现自主飞行",
            "intro_zh": [
                "核心问题：转子故障导致高速旋转和振动，挑战未知环境自主飞行",
                "方法要点：结合故障检测诊断、非线性模型预测控制和时空联合优化规划",
                "实验效果：在杂乱房间和未知森林中首次实现转子故障下的自主飞行"
            ],
            "tags_zh": [
                "四旋翼导航",
                "转子故障检测",
                "非线性模型预测控制",
                "时空优化",
                "LiDAR感知",
                "故障容错控制"
            ],
            "_index": 61
        },
        {
            "title": "Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation",
            "authors": [
                "Jean-Paul Travert",
                "Cédric Goeury",
                "Sébastien Boyaval",
                "Vito Bacchi",
                "Fabrice Zaoui"
            ],
            "arxiv_id": "2510.11305v1",
            "summary": "Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)\nimagery are crucial for calibrating and validating hydraulic models. This study\nuses SAR imagery to evaluate various preprocessing (especially speckle noise\nreduction), flood mapping, and water depth estimation methods. The impact of\nthe choice of method at different steps and its hyperparameters is studied by\nconsidering an ensemble of preprocessed images, flood maps, and water depth\nfields. The evaluation is conducted for two flood events on the Garonne River\n(France) in 2019 and 2021, using hydrodynamic simulations and in-situ\nobservations as reference data. Results show that the choice of speckle filter\nalters flood extent estimations with variations of several square kilometers.\nFurthermore, the selection and tuning of flood mapping methods also affect\nperformance. While supervised methods outperformed unsupervised ones, tuned\nunsupervised approaches (such as local thresholding or change detection) can\nachieve comparable results. The compounded uncertainty from preprocessing and\nflood mapping steps also introduces high variability in the water depth field\nestimates. This study highlights the importance of considering the entire\nprocessing pipeline, encompassing preprocessing, flood mapping, and water depth\nestimation methods and their associated hyperparameters. Rather than relying on\na single configuration, adopting an ensemble approach and accounting for\nmethodological uncertainty should be privileged. For flood mapping, the method\nchoice has the most influence. For water depth estimation, the most influential\nprocessing step was the flood map input resulting from the flood mapping step\nand the hyperparameters of the methods.",
            "headline_zh": "评估预处理、方法选择与超参数调优对SAR洪水制图和水深估计的影响",
            "intro_zh": [
                "核心问题：SAR图像洪水制图和水深估计中方法选择与超参数调优的不确定性影响。",
                "方法要点：采用集成方法评估预处理、洪水制图和水深估计步骤及其超参数。",
                "实验或效果：基于法国加龙河洪水事件，结果显示方法选择对洪水范围和水深估计有显著影响。"
            ],
            "tags_zh": [
                "SAR图像处理",
                "洪水制图",
                "水深估计",
                "超参数调优",
                "集成方法",
                "不确定性分析"
            ],
            "_index": 62
        },
        {
            "title": "sketch2symm: Symmetry-aware sketch-to-shape generation via semantic bridging",
            "authors": [
                "Yan Zhou",
                "Mingji Li",
                "Xiantao Zeng",
                "Jie Lin",
                "Yuexia Zhou"
            ],
            "arxiv_id": "2510.11303v1",
            "summary": "Sketch-based 3D reconstruction remains a challenging task due to the abstract\nand sparse nature of sketch inputs, which often lack sufficient semantic and\ngeometric information. To address this, we propose Sketch2Symm, a two-stage\ngeneration method that produces geometrically consistent 3D shapes from\nsketches. Our approach introduces semantic bridging via sketch-to-image\ntranslation to enrich sparse sketch representations, and incorporates symmetry\nconstraints as geometric priors to leverage the structural regularity commonly\nfound in everyday objects. Experiments on mainstream sketch datasets\ndemonstrate that our method achieves superior performance compared to existing\nsketch-based reconstruction methods in terms of Chamfer Distance, Earth Mover's\nDistance, and F-Score, verifying the effectiveness of the proposed semantic\nbridging and symmetry-aware design.",
            "headline_zh": "提出Sketch2Symm方法，通过语义桥接和对称约束从草图生成几何一致的3D形状",
            "intro_zh": [
                "核心问题：草图输入抽象稀疏，缺乏足够语义和几何信息，导致3D重建困难。",
                "方法要点：采用两阶段生成，先通过草图到图像翻译进行语义桥接，再引入对称约束作为几何先验。",
                "实验效果：在主流数据集上，Chamfer距离、Earth Mover距离和F-Score指标优于现有方法。"
            ],
            "tags_zh": [
                "草图到3D重建",
                "语义桥接",
                "对称约束",
                "几何一致性",
                "生成方法"
            ],
            "_index": 63
        },
        {
            "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models",
            "authors": [
                "Samer Al-Hamadani"
            ],
            "arxiv_id": "2510.11302v1",
            "summary": "Object detection systems have traditionally relied on supervised learning\nwith manually annotated bounding boxes, achieving high accuracy at the cost of\nsubstantial annotation investment. The emergence of Vision-Language Models\n(VLMs) offers an alternative paradigm enabling zero-shot detection through\nnatural language queries, eliminating annotation requirements but operating\nwith reduced accuracy. This paper presents the first comprehensive\ncost-effectiveness analysis comparing supervised detection (YOLO) with\nzero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on\n1,000 stratified COCO images and 200 diverse product images spanning consumer\nelectronics and rare categories, combined with detailed Total Cost of Ownership\nmodeling, we establish quantitative break-even thresholds governing\narchitecture selection. Our findings reveal that supervised YOLO achieves 91.2%\naccuracy versus 68.5% for zero-shot Gemini on standard categories, representing\na 22.7 percentage point advantage that costs $10,800 in annotation for\n100-category systems. However, this advantage justifies investment only beyond\n55 million inferences, equivalent to 151,000 images daily for one year.\nZero-shot Gemini demonstrates 52.3% accuracy on diverse product categories\n(ranging from highly web-prevalent consumer electronics at 75-85% to rare\nspecialized equipment at 25-40%) where supervised YOLO achieves 0% due to\narchitectural constraints preventing detection of untrained classes. Cost per\nCorrect Detection analysis reveals substantially lower per-detection costs for\nGemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We\ndevelop decision frameworks demonstrating that optimal architecture selection\ndepends critically on deployment volume, category stability, budget\nconstraints, and accuracy requirements rather than purely technical performance\nmetrics.",
            "headline_zh": "提出监督与零-shot检测的成本效益分析框架，指导架构选择",
            "intro_zh": [
                "核心问题：监督学习与零-shot VLM在目标检测中的成本效益权衡",
                "方法要点：结合COCO和产品图像评估，建模总拥有成本",
                "实验或效果：监督YOLO准确率91.2%，零-shot Gemini 68.5%，成本优势取决于推理量"
            ],
            "tags_zh": [
                "目标检测",
                "视觉语言模型",
                "成本效益分析",
                "监督学习",
                "零-shot检测"
            ],
            "_index": 64
        },
        {
            "title": "$Δ\\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization",
            "authors": [
                "Lin Zhu",
                "Yifeng Yang",
                "Xinbing Wang",
                "Qinying Gu",
                "Nanyang Ye"
            ],
            "arxiv_id": "2510.11296v1",
            "summary": "Recent approaches for vision-language models (VLMs) have shown remarkable\nsuccess in achieving fast downstream adaptation. When applied to real-world\ndownstream tasks, VLMs inevitably encounter both the in-distribution (ID) data\nand out-of-distribution (OOD) data. The OOD datasets often include both\ncovariate shifts (e.g., known classes with changes in image styles) and\nsemantic shifts (e.g., test-time unseen classes). This highlights the\nimportance of improving VLMs' generalization ability to covariate-shifted OOD\ndata, while effectively detecting open-set semantic-shifted OOD classes. In\nthis paper, inspired by the substantial energy change observed in closed-set\ndata when re-aligning vision-language modalities (specifically by directly\nreducing the maximum cosine similarity to a low value), we introduce a novel\nOOD score, named {\\Delta}Energy. {\\Delta}Energy significantly outperforms the\nvanilla energy-based OOD score and provides a more reliable approach for OOD\ndetection. Furthermore, {\\Delta}Energy can simultaneously improve OOD\ngeneralization under covariate shifts, which is achieved by lower-bound\nmaximization for {\\Delta}Energy (termed EBM). EBM is theoretically proven to\nnot only enhance OOD detection but also yields a domain-consistent Hessian,\nwhich serves as a strong indicator for OOD generalization. Based on this\nfinding, we developed a unified fine-tuning framework that allows for improving\nVLMs' robustness in both OOD generalization and OOD detection. Extensive\nexperiments on challenging OOD detection and generalization benchmarks\ndemonstrate the superiority of our method, outperforming recent approaches by\n10% to 25% in AUROC.",
            "headline_zh": "提出ΔEnergy优化能量变化，提升视觉语言模型的OOD检测与泛化能力",
            "intro_zh": [
                "核心问题：视觉语言模型在真实任务中面临分布外数据，需改进泛化与检测能力",
                "方法要点：引入ΔEnergy作为OOD分数，通过能量变化优化和EBM框架统一微调",
                "实验或效果：在OOD检测和泛化基准上，AUROC提升10%至25%，优于现有方法"
            ],
            "tags_zh": [
                "视觉语言模型",
                "分布外检测",
                "能量优化",
                "泛化能力",
                "微调框架",
                "Hessian一致性"
            ],
            "_index": 65
        },
        {
            "title": "Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual Question Answering",
            "authors": [
                "Jian Lan",
                "Zhicheng Liu",
                "Udo Schlegel",
                "Raoyuan Zhao",
                "Yihong Liu",
                "Hinrich Schütze",
                "Michael A. Hedderich",
                "Thomas Seidl"
            ],
            "arxiv_id": "2510.11295v1",
            "summary": "Large vision-language models (VLMs) achieve strong performance in Visual\nQuestion Answering but still rely heavily on supervised fine-tuning (SFT) with\nmassive labeled datasets, which is costly due to human annotations. Crucially,\nreal-world datasets often exhibit human uncertainty (HU) -- variation in human\nconfidence across annotations -- but standard SFT simply optimizes toward the\nmost frequent label, disregarding HU distributions. This leaves two open\nquestions: How does HU affect SFT, and how can HU be effectively leveraged in\ntraining? In this work, we first conduct a systematic evaluation of VLMs across\nvarying HU levels. We have two key findings: (i) surprisingly, high-HU samples\ncontribute little or even degrade model performance, and (ii) naively training\non the full dataset yields under-calibrated models that fail to capture HU\ndistributions. Motivated by these findings, we introduce HaDola, a human\nuncertainty-aware data selection and automatic labeling framework. HaDola\noperates in four stages -- discriminate, self-annotate, error trigger, and\ntraining -- to iteratively identify harmful samples, prioritize informative\nones, and bootstrap from a small seed set (5\\% of data). Our approach\nsubstantially reduces reliance on costly HU annotations and makes VLMs more\naccurate and better calibrated. Extensive experiments on VQAv2 and VizWiz\ndatasets demonstrate that HaDola consistently matches or outperforms\nstate-of-the-art baselines with less training data. Our work highlights the\nimportance of explicitly modeling HU in SFT, suggesting that better utilization\nof HU is more effective than merely scaling up dataset size.",
            "headline_zh": "提出HaDola框架以利用人类不确定性优化视觉问答模型训练",
            "intro_zh": [
                "核心问题：人类不确定性影响监督微调，高不确定性样本可能降低模型性能。",
                "方法要点：HaDola通过四阶段迭代选择数据并自动标注，减少对昂贵标注的依赖。",
                "实验或效果：在VQAv2和VizWiz数据集上，HaDola以更少数据匹配或超越先进基线。"
            ],
            "tags_zh": [
                "视觉问答",
                "人类不确定性",
                "数据选择",
                "自动标注",
                "模型校准",
                "监督微调"
            ],
            "_index": 66
        },
        {
            "title": "EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism",
            "authors": [
                "Han Xia",
                "Quanjun Li",
                "Qian Li",
                "Zimeng Li",
                "Hongbin Ye",
                "Yupeng Liu",
                "Haolun Li",
                "Xuhang Chen"
            ],
            "arxiv_id": "2510.11287v1",
            "summary": "Medical image segmentation is vital for diagnosis, treatment planning, and\ndisease monitoring but is challenged by complex factors like ambiguous edges\nand background noise. We introduce EEMS, a new model for segmentation,\ncombining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt\nGeneration Unit (MSPGU). EAEU enhances edge perception via multi-frequency\nfeature extraction, accurately defining boundaries. MSPGU integrates high-level\nsemantic and low-level spatial features using a prompt-guided approach,\nensuring precise target localization. The Dual-Source Adaptive Gated Fusion\nUnit (DAGFU) merges edge features from EAEU with semantic features from MSPGU,\nenhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018\nconfirm EEMS's superior performance and reliability as a clinical tool.",
            "headline_zh": "提出EEMS模型以解决医学图像分割中边缘模糊和噪声问题",
            "intro_zh": [
                "医学图像分割面临边缘模糊和背景噪声等复杂挑战",
                "结合边缘感知增强和多尺度提示生成单元，提升边界定义和目标定位",
                "在ISIC2018等数据集测试中显示高精度和鲁棒性，适合临床应用"
            ],
            "tags_zh": [
                "医学图像分割",
                "边缘感知",
                "多尺度特征融合",
                "自适应门控机制",
                "深度学习模型"
            ],
            "_index": 67
        },
        {
            "title": "Exploring and Leveraging Class Vectors for Classifier Editing",
            "authors": [
                "Jaeik Kim",
                "Jaeyoung Do"
            ],
            "arxiv_id": "2510.11268v1",
            "summary": "Image classifiers play a critical role in detecting diseases in medical\nimaging and identifying anomalies in manufacturing processes. However, their\npredefined behaviors after extensive training make post hoc model editing\ndifficult, especially when it comes to forgetting specific classes or adapting\nto distribution shifts. Existing classifier editing methods either focus\nnarrowly on correcting errors or incur extensive retraining costs, creating a\nbottleneck for flexible editing. Moreover, such editing has seen limited\ninvestigation in image classification. To overcome these challenges, we\nintroduce Class Vectors, which capture class-specific representation\nadjustments during fine-tuning. Whereas task vectors encode task-level changes\nin weight space, Class Vectors disentangle each class's adaptation in the\nlatent space. We show that Class Vectors capture each class's semantic shift\nand that classifier editing can be achieved either by steering latent features\nalong these vectors or by mapping them into weight space to update the decision\nboundaries. We also demonstrate that the inherent linearity and orthogonality\nof Class Vectors support efficient, flexible, and high-level concept editing\nvia simple class arithmetic. Finally, we validate their utility in applications\nsuch as unlearning, environmental adaptation, adversarial defense, and\nadversarial trigger optimization.",
            "headline_zh": "提出类向量以支持图像分类器的灵活编辑，应用于医学影像和制造异常检测。",
            "intro_zh": [
                "核心问题：图像分类器训练后难以编辑，如遗忘特定类或适应分布变化，现有方法成本高或范围窄。",
                "方法要点：类向量在潜在空间捕获类特定表示调整，支持特征引导或权重更新实现编辑。",
                "实验或效果：验证在遗忘、环境适应、对抗防御和触发优化等应用中的有效性。"
            ],
            "tags_zh": [
                "图像分类器编辑",
                "类向量",
                "潜在空间调整",
                "遗忘学习",
                "对抗防御",
                "分布适应"
            ],
            "_index": 68
        },
        {
            "title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images",
            "authors": [
                "Yuxuan Chen",
                "Ruotong Yang",
                "Zhengyang Zhang",
                "Mehreen Ahmed",
                "Yanming Wang"
            ],
            "arxiv_id": "2510.11260v1",
            "summary": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM),\nare widely used in scientific research for visualizing and analyzing\nmicrostructures. Determining the scale bars is an important first step of\naccurate SEM analysis; however, currently, it mainly relies on manual\noperations, which is both time-consuming and prone to errors. To address this\nissue, we propose a multi-modal and automated scale bar detection and\nextraction framework that provides concurrent object detection, text detection\nand text recognition with a Large Language Model (LLM) agent. The proposed\nframework operates in four phases; i) Automatic Dataset Generation (Auto-DG)\nmodel to synthesize a diverse dataset of SEM images ensuring robust training\nand high generalizability of the model, ii) scale bar object detection, iii)\ninformation extraction using a hybrid Optical Character Recognition (OCR)\nsystem with DenseNet and Convolutional Recurrent Neural Network (CRNN) based\nalgorithms, iv) an LLM agent to analyze and verify accuracy of the results. The\nproposed model demonstrates a strong performance in object detection and\naccurate localization with a precision of 100%, recall of 95.8%, and a mean\nAverage Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The\nhybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the\nAuto-DG dataset, significantly outperforming several mainstream standalone\nengines, highlighting its reliability for scientific image analysis. The LLM is\nintroduced as a reasoning engine as well as an intelligent assistant that\nsuggests follow-up steps and verifies the results. This automated method\npowered by an LLM agent significantly enhances the efficiency and accuracy of\nscale bar detection and extraction in SEM images, providing a valuable tool for\nmicroscopic analysis and advancing the field of scientific imaging.",
            "headline_zh": "提出多模态自动化标尺检测提取框架，结合LLM提升SEM图像分析效率与准确性",
            "intro_zh": [
                "核心问题：SEM图像标尺检测依赖人工，耗时且易错。",
                "方法要点：四阶段框架，包括自动数据集生成、对象检测、混合OCR和LLM验证。",
                "实验或效果：对象检测mAP达99.2%，OCR F1分数75%，显著优于主流引擎。"
            ],
            "tags_zh": [
                "标尺检测",
                "多模态框架",
                "大型语言模型",
                "光学字符识别",
                "扫描电子显微镜图像",
                "自动化分析"
            ],
            "_index": 69
        },
        {
            "title": "DTEA: Dynamic Topology Weaving and Instability-Driven Entropic Attenuation for Medical Image Segmentation",
            "authors": [
                "Weixuan Li",
                "Quanjun Li",
                "Guang Yu",
                "Song Yang",
                "Zimeng Li",
                "Chi-Man Pun",
                "Yupeng Liu",
                "Xuhang Chen"
            ],
            "arxiv_id": "2510.11259v1",
            "summary": "In medical image segmentation, skip connections are used to merge global\ncontext and reduce the semantic gap between encoder and decoder. Current\nmethods often struggle with limited structural representation and insufficient\ncontextual modeling, affecting generalization in complex clinical scenarios. We\npropose the DTEA model, featuring a new skip connection framework with the\nSemantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG)\nmodules. STR reorganizes multi-scale semantic features into a dynamic\nhypergraph to better model cross-resolution anatomical dependencies, enhancing\nstructural and semantic representation. EPG assesses channel stability after\nperturbation and filters high-entropy channels to emphasize clinically\nimportant regions and improve spatial attention. Extensive experiments on three\nbenchmark datasets show our framework achieves superior segmentation accuracy\nand better generalization across various clinical settings. The code is\navailable at\n\\href{https://github.com/LWX-Research/DTEA}{https://github.com/LWX-Research/DTEA}.",
            "headline_zh": "提出DTEA模型以解决医学图像分割中结构表示不足和上下文建模弱的问题",
            "intro_zh": [
                "核心问题：现有方法在复杂临床场景中泛化能力差，结构表示和上下文建模不足",
                "方法要点：引入STR模块构建动态超图建模解剖依赖，EPG模块过滤高熵通道增强空间注意力",
                "实验或效果：在三个基准数据集上验证，实现更高分割精度和更好泛化性能"
            ],
            "tags_zh": [
                "医学图像分割",
                "动态拓扑编织",
                "熵扰动门控",
                "语义拓扑重构",
                "超图建模",
                "空间注意力"
            ],
            "_index": 70
        },
        {
            "title": "DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation",
            "authors": [
                "Yuhui Fu",
                "Feiyang Xie",
                "Chaoyi Xu",
                "Jing Xiong",
                "Haoqi Yuan",
                "Zongqing Lu"
            ],
            "arxiv_id": "2510.11258v1",
            "summary": "Loco-manipulation is a fundamental challenge for humanoid robots to achieve\nversatile interactions in human environments. Although recent studies have made\nsignificant progress in humanoid whole-body control, loco-manipulation remains\nunderexplored and often relies on hard-coded task definitions or costly\nreal-world data collection, which limits autonomy and generalization. We\npresent DemoHLM, a framework for humanoid loco-manipulation that enables\ngeneralizable loco-manipulation on a real humanoid robot from a single\ndemonstration in simulation. DemoHLM adopts a hierarchy that integrates a\nlow-level universal whole-body controller with high-level manipulation policies\nfor multiple tasks. The whole-body controller maps whole-body motion commands\nto joint torques and provides omnidirectional mobility for the humanoid robot.\nThe manipulation policies, learned in simulation via our data generation and\nimitation learning pipeline, command the whole-body controller with closed-loop\nvisual feedback to execute challenging loco-manipulation tasks. Experiments\nshow a positive correlation between the amount of synthetic data and policy\nperformance, underscoring the effectiveness of our data generation pipeline and\nthe data efficiency of our approach. Real-world experiments on a Unitree G1\nrobot equipped with an RGB-D camera validate the sim-to-real transferability of\nDemoHLM, demonstrating robust performance under spatial variations across ten\nloco-manipulation tasks.",
            "headline_zh": "提出DemoHLM框架，从单次仿真演示实现人形机器人通用化移动操作",
            "intro_zh": [
                "核心问题：人形机器人移动操作依赖硬编码或昂贵数据收集，限制自主性和泛化能力",
                "方法要点：采用分层结构，集成低层全身控制器与高层策略，通过仿真数据生成和模仿学习",
                "实验或效果：在真实机器人上验证了仿真到现实的迁移，在十个任务中表现鲁棒"
            ],
            "tags_zh": [
                "人形机器人",
                "移动操作",
                "仿真到现实迁移",
                "模仿学习",
                "全身控制"
            ],
            "_index": 71
        },
        {
            "title": "Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches",
            "authors": [
                "Birat Poudel",
                "Satyam Ghimire",
                "Sijan Bhattarai",
                "Saurav Bhandari",
                "Suramya Sharma Dahal"
            ],
            "arxiv_id": "2510.11243v1",
            "summary": "Sign languages serve as essential communication systems for individuals with\nhearing and speech impairments. However, digital linguistic dataset resources\nfor underrepresented sign languages, such as Nepali Sign Language (NSL), remain\nscarce. This study introduces the first benchmark dataset for NSL, consisting\nof 36 gesture classes with 1,500 samples per class, designed to capture the\nstructural and visual features of the language. To evaluate recognition\nperformance, we fine-tuned MobileNetV2 and ResNet50 architectures on the\ndataset, achieving classification accuracies of 90.45% and 88.78%,\nrespectively. These findings demonstrate the effectiveness of convolutional\nneural networks in sign recognition tasks, particularly within low-resource\nsettings. To the best of our knowledge, this work represents the first\nsystematic effort to construct a benchmark dataset and assess deep learning\napproaches for NSL recognition, highlighting the potential of transfer learning\nand fine-tuning for advancing research in underexplored sign languages.",
            "headline_zh": "提出首个尼泊尔手语基准数据集与深度学习模型，以解决低资源手语识别问题。",
            "intro_zh": [
                "核心问题：尼泊尔手语等低资源语言缺乏数字数据集，阻碍自动识别研究。",
                "方法要点：构建包含36类手势的基准数据集，并微调MobileNetV2和ResNet50模型。",
                "实验或效果：MobileNetV2和ResNet50分类准确率分别达90.45%和88.78%，验证CNN有效性。"
            ],
            "tags_zh": [
                "手语识别",
                "基准数据集",
                "卷积神经网络",
                "迁移学习",
                "低资源语言",
                "深度学习"
            ],
            "_index": 72
        },
        {
            "title": "LightPneumoNet: Lightweight Pneumonia Classifier",
            "authors": [
                "Neilansh Chauhan",
                "Piyush Kumar Gupta",
                "Faraz Doja"
            ],
            "arxiv_id": "2510.11232v1",
            "summary": "Effective pneumonia diagnosis is often challenged by the difficulty of\ndeploying large, computationally expensive deep learning models in\nresource-limited settings. This study introduces LightPneumoNet, an efficient,\nlightweight convolutional neural network (CNN) built from scratch to provide an\naccessible and accurate diagnostic solution for pneumonia detection from chest\nX-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.\nPreprocessing included image resizing to 224x224, grayscale conversion, and\npixel normalization, with data augmentation (rotation, zoom, shear) to prevent\noverfitting. The custom architecture features four blocks of stacked\nconvolutional layers and contains only 388,082 trainable parameters, resulting\nin a minimal 1.48 MB memory footprint. On the independent test set, our model\ndelivered exceptional performance, achieving an overall accuracy of 0.942,\nprecision of 0.92, and an F1-Score of 0.96. Critically, it obtained a\nsensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify\ntrue pneumonia cases and minimize clinically significant false negatives.\nNotably, LightPneumoNet achieves this high recall on the same dataset where\nexisting approaches typically require significantly heavier architectures or\nfail to reach comparable sensitivity levels. The model's efficiency enables\ndeployment on low-cost hardware, making advanced computer-aided diagnosis\naccessible in underserved clinics and serving as a reliable second-opinion tool\nto improve patient outcomes.",
            "headline_zh": "提出轻量级CNN LightPneumoNet以在资源受限环境中准确诊断肺炎",
            "intro_zh": [
                "资源受限环境下部署大型深度学习模型困难，影响肺炎诊断",
                "设计轻量CNN架构，仅38.8万参数，内存占用1.48MB",
                "在独立测试集上准确率0.942，敏感度0.99，减少假阴性"
            ],
            "tags_zh": [
                "肺炎分类",
                "轻量级CNN",
                "胸部X光",
                "计算机辅助诊断",
                "资源受限部署"
            ],
            "_index": 73
        },
        {
            "title": "Investigating Identity Signals in Conversational Facial Dynamics via Disentangled Expression Features",
            "authors": [
                "Masoumeh Chapariniya",
                "Pierre Vuillecard",
                "Jean-Marc Odobez",
                "Volker Dellwo",
                "Teodora Vukovic"
            ],
            "arxiv_id": "2510.11223v1",
            "summary": "This work investigates whether individuals can be identified solely through\nthe pure dynamical components of their facial expressions, independent of\nstatic facial appearance. We leverage the FLAME 3D morphable model to achieve\nexplicit disentanglement between facial shape and expression dynamics,\nextracting frame-by-frame parameters from conversational videos while retaining\nonly expression and jaw coefficients. On the CANDOR dataset of 1,429 speakers\nin naturalistic conversations, our Conformer model with supervised contrastive\nlearning achieves 61.14\\%accuracy on 1,429-way classification -- 458 times\nabove chance -- demonstrating that facial dynamics carry strong identity\nsignatures. We introduce a drift-to-noise ratio (DNR) that quantifies the\nreliability of shape expression separation by measuring across-session shape\nchanges relative to within-session variability. DNR strongly negatively\ncorrelates with recognition performance, confirming that unstable shape\nestimation compromises dynamic identification. Our findings reveal\nperson-specific signatures in conversational facial dynamics, with implications\nfor social perception and clinical assessment.",
            "headline_zh": "提出基于解耦表达特征的动态面部识别方法，在自然对话中验证身份信号。",
            "intro_zh": [
                "研究问题：个体能否仅通过面部表情动态成分识别，独立于静态外观。",
                "方法要点：使用FLAME模型解耦面部形状与表达动态，提取表达和下颌系数。",
                "实验效果：在CANDOR数据集上，模型准确率达61.14%，远高于随机水平。"
            ],
            "tags_zh": [
                "面部动态识别",
                "表达特征解耦",
                "对比学习",
                "身份信号分析",
                "3D形态模型"
            ],
            "_index": 74
        },
        {
            "title": "Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos",
            "authors": [
                "Rohit Gupta",
                "Anirban Roy",
                "Claire Christensen",
                "Sujeong Kim",
                "Sarah Gerard",
                "Madeline Cincebeaux",
                "Ajay Divakaran",
                "Todd Grindal",
                "Mubarak Shah"
            ],
            "arxiv_id": "2510.11204v1",
            "summary": "The recent growth in the consumption of online media by children during early\nchildhood necessitates data-driven tools enabling educators to filter out\nappropriate educational content for young learners. This paper presents an\napproach for detecting educational content in online videos. We focus on two\nwidely used educational content classes: literacy and math. For each class, we\nchoose prominent codes (sub-classes) based on the Common Core Standards. For\nexample, literacy codes include `letter names', `letter sounds', and math codes\ninclude `counting', `sorting'. We pose this as a fine-grained multilabel\nclassification problem as videos can contain multiple types of educational\ncontent and the content classes can get visually similar (e.g., `letter names'\nvs `letter sounds'). We propose a novel class prototypes based supervised\ncontrastive learning approach that can handle fine-grained samples associated\nwith multiple labels. We learn a class prototype for each class and a loss\nfunction is employed to minimize the distances between a class prototype and\nthe samples from the class. Similarly, distances between a class prototype and\nthe samples from other classes are maximized. As the alignment between visual\nand audio cues are crucial for effective comprehension, we consider a\nmultimodal transformer network to capture the interaction between visual and\naudio cues in videos while learning the embedding for videos. For evaluation,\nwe present a dataset, APPROVE, employing educational videos from YouTube\nlabeled with fine-grained education classes by education researchers. APPROVE\nconsists of 193 hours of expert-annotated videos with 19 classes. The proposed\napproach outperforms strong baselines on APPROVE and other benchmarks such as\nYoutube-8M, and COIN. The dataset is available at\nhttps://github.com/rohit-gupta/MMContrast/tree/main/APPROVE",
            "headline_zh": "提出基于类原型的对比学习方法，以分类多标签细粒度教育视频。",
            "intro_zh": [
                "核心问题：在线视频中检测儿童教育内容，如识字和数学的细粒度子类。",
                "方法要点：使用类原型监督对比学习，最小化类内距离并最大化类间距离。",
                "实验或效果：在APPROVE数据集上优于基线，并验证于Youtube-8M和COIN。"
            ],
            "tags_zh": [
                "多标签分类",
                "细粒度分类",
                "对比学习",
                "多模态Transformer",
                "教育视频分析",
                "类原型学习"
            ],
            "_index": 75
        },
        {
            "title": "Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations",
            "authors": [
                "Johannes Moll",
                "Markus Graf",
                "Tristan Lemke",
                "Nicolas Lenhart",
                "Daniel Truhn",
                "Jean-Benoit Delbrouck",
                "Jiazhen Pan",
                "Daniel Rueckert",
                "Lisa C. Adams",
                "Keno K. Bressem"
            ],
            "arxiv_id": "2510.11196v1",
            "summary": "Vision-language models (VLMs) often produce chain-of-thought (CoT)\nexplanations that sound plausible yet fail to reflect the underlying decision\nprocess, undermining trust in high-stakes clinical use. Existing evaluations\nrarely catch this misalignment, prioritizing answer accuracy or adherence to\nformats. We present a clinically grounded framework for chest X-ray visual\nquestion answering (VQA) that probes CoT faithfulness via controlled text and\nimage modifications across three axes: clinical fidelity, causal attribution,\nand confidence calibration. In a reader study (n=4), evaluator-radiologist\ncorrelations fall within the observed inter-radiologist range for all axes,\nwith strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate\nalignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone\n($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows\nthat answer accuracy and explanation quality are decoupled, acknowledging\ninjected cues does not ensure grounding, and text cues shift explanations more\nthan visual cues. While some open-source models match final answer accuracy,\nproprietary models score higher on attribution (25.0% vs. 1.4%) and often on\nfidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to\nevaluate beyond final answer accuracy.",
            "headline_zh": "提出基于多模态扰动的医学视觉语言模型推理忠实性评估框架，用于胸部X光视觉问答。",
            "intro_zh": [
                "核心问题：视觉语言模型的链式思维解释常与决策过程不符，影响临床高风险应用的可信度。",
                "方法要点：通过文本和图像修改，评估临床保真度、因果归因和置信度校准三个维度。",
                "实验或效果：基准测试显示答案准确性与解释质量脱钩，专有模型在归因和保真度上表现更优。"
            ],
            "tags_zh": [
                "医学视觉语言模型",
                "推理忠实性评估",
                "多模态扰动",
                "胸部X光视觉问答",
                "链式思维解释",
                "临床可信度"
            ],
            "_index": 76
        },
        {
            "title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models",
            "authors": [
                "Shengming Yuan",
                "Xinyu Lyu",
                "Shuailong Wang",
                "Beitao Chen",
                "Jingkuan Song",
                "Lianli Gao"
            ],
            "arxiv_id": "2510.11190v1",
            "summary": "Multimodal large language models (MLLMs) face an inherent trade-off between\nfaithfulness and creativity, as different tasks require varying degrees of\nassociative reasoning. However, existing methods lack the flexibility to\nmodulate this reasoning strength, limiting MLLMs' adaptability across factual\nand creative scenarios. To bridge this gap, we propose equipping MLLMs with\nmechanisms that enable flexible control over associative reasoning. We begin by\ninvestigating the internal mechanisms underlying associative behavior in MLLMs\nand find that: (1) middle layers play a pivotal role in shaping model's\nassociative tendencies, (2) modifying representations in these layers\neffectively regulates associative reasoning strength, and (3) hallucinations\ncan be exploited to derive steering vectors that guide this modulation.\nBuilding on these findings, we introduce Flexible Association Control (FlexAC),\na lightweight and training-free framework for modulating associative behavior\nin MLLMs. FlexAC first induces hallucination-guided intermediate\nrepresentations to encode associative directions. Then, it selects\nhigh-association instances to construct effective associative steering vectors,\nwhose strengths are adaptively calibrated to balance creative guidance with\noutput stability. Finally, recognizing the multi-dimensional nature of\nassociative reasoning, FlexAC incorporates task-specific associative vectors\nderived from a forward pass on a few target-domain samples, enabling models to\nfollow diverse associative directions and better adapt to creative tasks.\nNotably, our method achieves up to a 5.8x improvement in creativity on\nCreation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing\nexisting baselines and demonstrating its effectiveness in enabling flexible\ncontrol over associative reasoning in MLLMs. Our code is available at\nhttps://github.com/ylhz/FlexAC.",
            "headline_zh": "提出FlexAC框架以灵活控制多模态大语言模型中的联想推理强度",
            "intro_zh": [
                "多模态大语言模型在忠实性与创造性间存在固有权衡，现有方法缺乏灵活调节联想推理强度的能力。",
                "FlexAC通过修改中间层表示和利用幻觉引导向量，实现无需训练的联想推理强度调制。",
                "实验显示在Creation-MMBench上创造力提升5.8倍，CHAIR上幻觉率降低29%，优于基线方法。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "联想推理控制",
                "幻觉利用",
                "中间层调制",
                "训练无关框架",
                "创造性任务适应"
            ],
            "_index": 77
        },
        {
            "title": "Saudi Sign Language Translation Using T5",
            "authors": [
                "Ali Alhejab",
                "Tomas Zelezny",
                "Lamya Alkanhal",
                "Ivan Gruber",
                "Yazeed Alharbi",
                "Jakub Straka",
                "Vaclav Javorek",
                "Marek Hruz",
                "Badriah Alkalifah",
                "Ahmed Ali"
            ],
            "arxiv_id": "2510.11183v1",
            "summary": "This paper explores the application of T5 models for Saudi Sign Language\n(SSL) translation using a novel dataset. The SSL dataset includes three\nchallenging testing protocols, enabling comprehensive evaluation across\ndifferent scenarios. Additionally, it captures unique SSL characteristics, such\nas face coverings, which pose challenges for sign recognition and translation.\nIn our experiments, we investigate the impact of pre-training on American Sign\nLanguage (ASL) data by comparing T5 models pre-trained on the YouTubeASL\ndataset with models trained directly on the SSL dataset. Experimental results\ndemonstrate that pre-training on YouTubeASL significantly improves models'\nperformance (roughly $3\\times$ in BLEU-4), indicating cross-linguistic\ntransferability in sign language models. Our findings highlight the benefits of\nleveraging large-scale ASL data to improve SSL translation and provide insights\ninto the development of more effective sign language translation systems. Our\ncode is publicly available at our GitHub repository.",
            "headline_zh": "应用T5模型翻译沙特手语，利用ASL预训练提升性能",
            "intro_zh": [
                "核心问题：沙特手语翻译面临独特挑战，如面部覆盖影响识别。",
                "方法要点：比较T5模型在ASL预训练与直接SSL训练的效果。",
                "实验或效果：ASL预训练显著提升BLEU-4分数约3倍。"
            ],
            "tags_zh": [
                "沙特手语翻译",
                "T5模型",
                "跨语言迁移",
                "预训练优化",
                "手语数据集"
            ],
            "_index": 78
        },
        {
            "title": "Generalisation of automatic tumour segmentation in histopathological whole-slide images across multiple cancer types",
            "authors": [
                "Ole-Johan Skrede",
                "Manohar Pradhan",
                "Maria Xepapadakis Isaksen",
                "Tarjei Sveinsgjerd Hveem",
                "Ljiljana Vlatkovic",
                "Arild Nesbakken",
                "Kristina Lindemann",
                "Gunnar B Kristensen",
                "Jenneke Kasius",
                "Alain G Zeimet",
                "Odd Terje Brustugun",
                "Lill-Tove Rasmussen Busund",
                "Elin H Richardsen",
                "Erik Skaaheim Haug",
                "Bjørn Brennhovd",
                "Emma Rewcastle",
                "Melinda Lillesand",
                "Vebjørn Kvikstad",
                "Emiel Janssen",
                "David J Kerr",
                "Knut Liestøl",
                "Fritz Albregtsen",
                "Andreas Kleppe"
            ],
            "arxiv_id": "2510.11182v1",
            "summary": "Deep learning is expected to aid pathologists by automating tasks such as\ntumour segmentation. We aimed to develop one universal tumour segmentation\nmodel for histopathological images and examine its performance in different\ncancer types. The model was developed using over 20 000 whole-slide images from\nover 4 000 patients with colorectal, endometrial, lung, or prostate carcinoma.\nPerformance was validated in pre-planned analyses on external cohorts with over\n3 000 patients across six cancer types. Exploratory analyses included over 1\n500 additional patients from The Cancer Genome Atlas. Average Dice coefficient\nwas over 80% in all validation cohorts with en bloc resection specimens and in\nThe Cancer Genome Atlas cohorts. No loss of performance was observed when\ncomparing the universal model with models specialised on single cancer types.\nIn conclusion, extensive and rigorous evaluations demonstrate that generic\ntumour segmentation by a single model is possible across cancer types, patient\npopulations, sample preparations, and slide scanners.",
            "headline_zh": "提出通用肿瘤分割模型，在多种癌症类型中实现高精度分割。",
            "intro_zh": [
                "核心问题：开发单一模型在组织病理学全切片图像中通用肿瘤分割，适应不同癌症类型。",
                "方法要点：使用深度学习，基于超过20000张全切片图像训练模型，涵盖四种癌症类型。",
                "实验或效果：在外部验证队列中平均Dice系数超过80%，性能与专用模型相当。"
            ],
            "tags_zh": [
                "肿瘤分割",
                "深度学习",
                "组织病理学图像",
                "多癌症类型",
                "通用模型"
            ],
            "_index": 79
        },
        {
            "title": "BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models",
            "authors": [
                "Bryan Chen Zhengyu Tan",
                "Zheng Weihua",
                "Zhengyuan Liu",
                "Nancy F. Chen",
                "Hwaran Lee",
                "Kenny Tsu Wei Choo",
                "Roy Ka-Wei Lee"
            ],
            "arxiv_id": "2510.11178v1",
            "summary": "As vision-language models (VLMs) are deployed globally, their ability to\nunderstand culturally situated knowledge becomes essential. Yet, existing\nevaluations largely assess static recall or isolated visual grounding, leaving\nunanswered whether VLMs possess robust and transferable cultural understanding.\nWe introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to\nevaluate the robustness of everyday cultural knowledge in VLMs across\nlinguistic rephrasings and visual modalities. Building on the BLEnD dataset,\nBLEnD-Vis constructs 313 culturally grounded question templates spanning 16\nregions and generates three aligned multiple-choice formats: (i) a text-only\nbaseline querying from Region $\\to$ Entity, (ii) an inverted text-only variant\n(Entity $\\to$ Region), and (iii) a VQA-style version of (ii) with generated\nimages. The resulting benchmark comprises 4,916 images and over 21,000\nmultiple-choice question (MCQ) instances, validated through human annotation.\nBLEnD-Vis reveals significant fragility in current VLM cultural knowledge;\nmodels exhibit performance drops under linguistic rephrasing and, whilst visual\ncues often aid performance, low cross-modal consistency highlights challenges\nin robustly integrating textual and visual understanding, particularly for\nlower-resource regions. BLEnD-Vis thus provides a crucial testbed for\nsystematically analysing cultural robustness and multimodal grounding, exposing\nlimitations and guiding the development of more culturally competent VLMs.",
            "headline_zh": "提出BLEnD-Vis基准以评估视觉语言模型的多模态文化理解鲁棒性",
            "intro_zh": [
                "现有评估忽视视觉语言模型的文化理解鲁棒性和可迁移性",
                "构建多模态多文化基准，涵盖文本重述和视觉模态的313个问题模板",
                "实验显示模型在语言重述下性能下降，视觉线索帮助但跨模态一致性低"
            ],
            "tags_zh": [
                "视觉语言模型",
                "文化理解基准",
                "多模态评估",
                "鲁棒性分析",
                "跨模态一致性"
            ],
            "_index": 80
        },
        {
            "title": "G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation",
            "authors": [
                "Yesung Cho",
                "Sungmin Lee",
                "Geongyu Lee",
                "Minkyung Lee",
                "Jongbae Park",
                "Dongmyung Shin"
            ],
            "arxiv_id": "2510.11176v1",
            "summary": "Recent studies in pathology foundation models have shown that scaling\ntraining data, diversifying cancer types, and increasing model size\nconsistently improve their performance. However, giga-scale foundation models,\nwhich are trained on hundreds of thousands of slides covering tens of cancer\ntypes and contain billions of parameters, pose significant challenges for\npractical use due to their tremendous computational costs in both development\nand deployment. In this work, we present a novel strategy, named the G2L\nframework, to increase the performance of large-scale foundation models, which\nconsist of only $15\\%$ of the parameters of giga-scale models, to a comparable\nperformance level of giga-scale models in cancer-specific tasks. Our approach\napplies knowledge distillation, transferring the capabilities of a giga-scale\nmodel to a large-scale model, using just 1K pathology slides of a target cancer\n(e.g., breast, prostate, etc.). The resulting distilled model not only\noutperformed state-of-the-art models of the same size (i.e., large-scale)\nacross several benchmarks but also, interestingly, surpassed the giga-scale\nteacher and huge-scale models in some benchmarks. In addition, the distilled\nmodel exhibited a higher robustness index, indicating improved resilience to\nimage variations originating from multiple institutions. These findings suggest\nthat the proposed distillation approach for a large-scale model is a data- and\nparameter-efficient way to achieve giga-scale-level performance for\ncancer-specific applications without prohibitive computational burden.",
            "headline_zh": "提出G2L框架通过知识蒸馏提升病理大模型性能，降低计算成本",
            "intro_zh": [
                "核心问题：千亿级病理基础模型计算成本高，难以实用部署",
                "方法要点：用少量目标癌症数据蒸馏，将千亿模型能力转移至大型模型",
                "实验或效果：蒸馏模型在多项基准中超越同规模模型，部分超越千亿模型"
            ],
            "tags_zh": [
                "病理基础模型",
                "知识蒸馏",
                "癌症特异性",
                "计算效率",
                "模型压缩"
            ],
            "_index": 81
        },
        {
            "title": "Reliable Cross-modal Alignment via Prototype Iterative Construction",
            "authors": [
                "Xiang Ma",
                "Litian Xu",
                "Lexin Fang",
                "Caiming Zhang",
                "Lizhen Cui"
            ],
            "arxiv_id": "2510.11175v1",
            "summary": "Cross-modal alignment is an important multi-modal task, aiming to bridge the\nsemantic gap between different modalities. The most reliable fundamention for\nachieving this objective lies in the semantic consistency between matched\npairs. Conventional methods implicitly assume embeddings contain solely\nsemantic information, ignoring the impact of non-semantic information during\nalignment, which inevitably leads to information bias or even loss. These\nnon-semantic information primarily manifest as stylistic variations in the\ndata, which we formally define as style information. An intuitive approach is\nto separate style from semantics, aligning only the semantic information.\nHowever, most existing methods distinguish them based on feature columns, which\ncannot represent the complex coupling relationship between semantic and style\ninformation. In this paper, we propose PICO, a novel framework for suppressing\nstyle interference during embedding interaction. Specifically, we quantify the\nprobability of each feature column representing semantic information, and\nregard it as the weight during the embedding interaction. To ensure the\nreliability of the semantic probability, we propose a prototype iterative\nconstruction method. The key operation of this method is a performance\nfeedback-based weighting function, and we have theoretically proven that the\nfunction can assign higher weight to prototypes that bring higher performance\nimprovements. Extensive experiments on various benchmarks and model backbones\ndemonstrate the superiority of PICO, outperforming state-of-the-art methods by\n5.2\\%-14.1\\%.",
            "headline_zh": "提出PICO框架以抑制风格干扰，提升跨模态对齐可靠性",
            "intro_zh": [
                "核心问题：跨模态对齐中非语义风格信息导致语义偏差或损失",
                "方法要点：通过原型迭代构建量化语义概率，加权嵌入交互",
                "实验或效果：在多个基准上优于SOTA方法5.2%-14.1%"
            ],
            "tags_zh": [
                "跨模态对齐",
                "风格干扰抑制",
                "原型迭代构建",
                "语义概率量化",
                "嵌入交互加权"
            ],
            "_index": 82
        },
        {
            "title": "CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation",
            "authors": [
                "Zhenyu Lu",
                "Liupeng Li",
                "Jinpeng Wang",
                "Yan Feng",
                "Bin Chen",
                "Ke Chen",
                "Yaowei Wang"
            ],
            "arxiv_id": "2510.11173v1",
            "summary": "Existing works on reasoning segmentation either connect hidden features from\na language model directly to a mask decoder or represent positions in text,\nwhich limits interpretability and semantic detail. To solve this, we present\nCoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model\nthat bridges language reasoning to segmentation through a differentiable and\ninterpretable positional prior instantiated as a heatmap. By making the\nreasoning process clear via MCoT and expressing it as a dense, differentiable\nheatmap, this interface enhances interpretability and diagnostic analysis and\nyields more concentrated evidence on the target. A learnable concentration\ntoken aggregates features of the image and reasoning text to generate this\npositional prior, which is decoded to precise masks through a lightweight\ndecoder, providing a direct connection between reasoning and segmentation.\nAcross the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best\nreported metrics on each standard split under comparable protocols, with\nperformance at or above prior state of the art across both validation and test\npartitions. Extensive experiments reveal that the quality of the heatmap\nstrongly influences the resulting mask quality, supporting a consistent\nassociation between the reasoning output and downstream mask generation.\nCollectively, these findings support the utility of this paradigm in bridging\nreasoning and segmentation and show advantages in concentration driven by\nreasoning and predicting masks more precisely. Code, checkpoints and logs are\nreleased at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.",
            "headline_zh": "提出CoPRS模型，通过可微分热图桥接语言推理与分割，提升可解释性与精度。",
            "intro_zh": [
                "现有推理分割方法直接连接语言模型特征或文本位置，限制可解释性与语义细节。",
                "CoPRS基于多模态思维链生成可微分位置先验热图，通过轻量解码器输出精确掩码。",
                "在RefCOCO系列和ReasonSeg数据集上，性能达到或超越现有最佳结果，热图质量影响掩码质量。"
            ],
            "tags_zh": [
                "推理分割",
                "多模态思维链",
                "可微分热图",
                "位置先验",
                "掩码生成",
                "可解释性"
            ],
            "_index": 83
        },
        {
            "title": "Multiview Manifold Evidential Fusion for PolSAR Image Classification",
            "authors": [
                "Junfei Shi",
                "Haojia Zhang",
                "Haiyan Jin",
                "Junhuai Li",
                "Xiaogang Song",
                "Yuanfan Guo",
                "Haonan Su",
                "Weisi Lin"
            ],
            "arxiv_id": "2510.11171v1",
            "summary": "Polarimetric Synthetic Aperture Radar (PolSAR) covariance matrices and their\nextracted multi-features - such as scattering angle, entropy, texture, and\nboundary descriptors - provide complementary and physically interpretable\ninformation for image classification. Traditional fusion strategies typically\nconcatenate these features or employ deep learning networks to combine them.\nHowever, the covariance matrices and multi-features, as two complementary\nviews, lie on different manifolds with distinct geometric structures. Existing\nfusion methods also overlook the varying importance of different views and\nignore uncertainty, often leading to unreliable predictions. To address these\nissues, we propose a Multiview Manifold Evidential Fusion (MMEFnet) method to\neffectively fuse these two views. It gives a new framework to integrate PolSAR\nmanifold learning and evidence fusion into a unified architecture.\nSpecifically, covariance matrices are represented on the Hermitian Positive\nDefinite (HPD) manifold, while multi-features are modeled on the Grassmann\nmanifold. Two different kernel metric learning networks are constructed to\nlearn their manifold representations. Subsequently, a trusted multiview\nevidence fusion, replacing the conventional softmax classifier, estimates\nbelief mass and quantifies the uncertainty of each view from the learned deep\nfeatures. Finally, a Dempster-Shafer theory-based fusion strategy combines\nevidence, enabling a more reliable and interpretable classification. Extensive\nexperiments on three real-world PolSAR datasets demonstrate that the proposed\nmethod consistently outperforms existing approaches in accuracy, robustness,\nand interpretability.",
            "headline_zh": "提出多流形证据融合方法以提升PolSAR图像分类的可靠性和可解释性",
            "intro_zh": [
                "传统融合方法忽略多视图的几何差异和不确定性，导致分类不可靠",
                "构建HPD和Grassmann流形表示，结合证据融合估计置信度和不确定性",
                "在三个真实数据集上验证，方法在精度、鲁棒性和可解释性上优于现有方法"
            ],
            "tags_zh": [
                "PolSAR图像分类",
                "多视图融合",
                "流形学习",
                "证据理论",
                "不确定性量化",
                "深度学习"
            ],
            "_index": 84
        },
        {
            "title": "Validation of an Artificial Intelligence Tool for the Detection of Sperm DNA Fragmentation Using the TUNEL In Situ Hybridization Assay",
            "authors": [
                "Byron Alexander Jacobs",
                "Aqeel Morris",
                "Ifthakaar Shaik",
                "Frando Lin"
            ],
            "arxiv_id": "2510.11142v1",
            "summary": "Sperm DNA fragmentation (SDF) is a critical parameter in male fertility\nassessment that conventional semen analysis fails to evaluate. This study\npresents the validation of a novel artificial intelligence (AI) tool designed\nto detect SDF through digital analysis of phase contrast microscopy images,\nusing the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL)\nassay as the gold standard reference. Utilising the established link between\nsperm morphology and DNA integrity, the present work proposes a morphology\nassisted ensemble AI model that combines image processing techniques with\nstate-of-the-art transformer based machine learning models (GC-ViT) for the\nprediction of DNA fragmentation in sperm from phase contrast images. The\nensemble model is benchmarked against a pure transformer `vision' model as well\nas a `morphology-only` model. Promising results show the proposed framework is\nable to achieve sensitivity of 60\\% and specificity of 75\\%. This\nnon-destructive methodology represents a significant advancement in\nreproductive medicine by enabling real-time sperm selection based on DNA\nintegrity for clinical diagnostic and therapeutic applications.",
            "headline_zh": "提出基于形态辅助AI模型以检测精子DNA碎片，用于男性生育评估。",
            "intro_zh": [
                "核心问题：精子DNA碎片是男性生育评估关键参数，传统精液分析无法检测。",
                "方法要点：结合图像处理与Transformer模型，从相差显微镜图像预测DNA碎片。",
                "实验或效果：模型灵敏度60%、特异性75%，支持实时精子选择。"
            ],
            "tags_zh": [
                "精子DNA碎片检测",
                "AI模型验证",
                "相差显微镜图像分析",
                "Transformer机器学习",
                "生殖医学诊断",
                "非破坏性方法"
            ],
            "_index": 85
        },
        {
            "title": "video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory",
            "authors": [
                "Guangzhi Sun",
                "Yixuan Li",
                "Xiaodong Wu",
                "Yudong Yang",
                "Wei Li",
                "Zejun Ma",
                "Chao Zhang"
            ],
            "arxiv_id": "2510.11129v1",
            "summary": "Continuous, high-frame-rate, high-resolution processing of long video streams\nis critical for future AI agents, yet current video-understanding LLMs struggle\nto scale. Offline, fixed-frame-number methods require the stream length to\nadapt frame rates; streaming methods constrain memory by merging or discarding\ntokens, losing information. We propose video-SALMONN S, a streaming\naudio-visual LLM that, to our knowledge, is the first to process 3-hour videos\nat 1 FPS and 360p resolution under a fixed memory budget. Our model introduces\n(i) a test-time-training (TTT) memory module that continually updates token\nrepresentations to capture long-range dependencies by replacing token merging,\nand (ii) a prompt-dependent memory reader that selectively retrieves\ncontext-relevant content from fixed-size memory. The TTT module is optimised\nwith a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient\nadaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),\nvideo-SALMONN S sustains high-quality understanding on multi-hour videos with\n10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and\n67.8% on the Video-MME long split, outperforming both offline and streaming\nbaselines.",
            "headline_zh": "提出video-SALMONN S以在固定内存下处理长视频流，实现高质量音频-视觉理解",
            "intro_zh": [
                "当前视频理解LLM难以处理长视频流，存在内存限制和信息丢失问题",
                "引入测试时训练记忆模块和提示依赖记忆读取器，替代令牌合并以捕获长程依赖",
                "在长视频基准测试中，模型在3小时视频上保持高性能，优于离线与流式基线"
            ],
            "tags_zh": [
                "流式音频-视觉大语言模型",
                "长视频理解",
                "测试时训练",
                "记忆模块",
                "固定内存处理"
            ],
            "_index": 86
        },
        {
            "title": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level Cross-Modal Knowledge Transfer",
            "authors": [
                "Qiyi Tong",
                "Olivia Nocentini",
                "Marta Lagomarsino",
                "Kuanqi Cai",
                "Marta Lorenzini",
                "Arash Ajoudani"
            ],
            "arxiv_id": "2510.11128v1",
            "summary": "Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead.",
            "headline_zh": "提出多级跨模态知识蒸馏以解决热图像中轻量级面部关键点检测问题",
            "intro_zh": [
                "核心问题：热图像面部关键点检测缺乏丰富视觉线索，传统跨模态方法计算昂贵或引入结构伪影",
                "方法要点：引入双向双注入知识蒸馏，通过闭环监督学习模态不变特征，确保语义对齐",
                "实验或效果：在公开基准上实现新SOTA，显著降低计算开销并超越先前方法"
            ],
            "tags_zh": [
                "面部关键点检测",
                "热图像分析",
                "跨模态知识蒸馏",
                "轻量级模型",
                "模态不变特征"
            ],
            "_index": 87
        },
        {
            "title": "Demystifying Numerosity in Diffusion Models -- Limitations and Remedies",
            "authors": [
                "Yaqi Zhao",
                "Xiaochen Wang",
                "Li Dong",
                "Wentao Zhang",
                "Yuhui Yuan"
            ],
            "arxiv_id": "2510.11117v1",
            "summary": "Numerosity remains a challenge for state-of-the-art text-to-image generation\nmodels like FLUX and GPT-4o, which often fail to accurately follow counting\ninstructions in text prompts. In this paper, we aim to study a fundamental yet\noften overlooked question: Can diffusion models inherently generate the correct\nnumber of objects specified by a textual prompt simply by scaling up the\ndataset and model size? To enable rigorous and reproducible evaluation, we\nconstruct a clean synthetic numerosity benchmark comprising two complementary\ndatasets: GrayCount250 for controlled scaling studies, and NaturalCount6\nfeaturing complex naturalistic scenes. Second, we empirically show that the\nscaling hypothesis does not hold: larger models and datasets alone fail to\nimprove counting accuracy on our benchmark. Our analysis identifies a key\nreason: diffusion models tend to rely heavily on the noise initialization\nrather than the explicit numerosity specified in the prompt. We observe that\nnoise priors exhibit biases toward specific object counts. In addition, we\npropose an effective strategy for controlling numerosity by injecting\ncount-aware layout information into the noise prior. Our method achieves\nsignificant gains, improving accuracy on GrayCount250 from 20.0\\% to 85.3\\% and\non NaturalCount6 from 74.8\\% to 86.3\\%, demonstrating effective generalization\nacross settings.",
            "headline_zh": "提出注入计数感知布局信息的方法，以解决扩散模型在文本提示中计数准确性不足的问题。",
            "intro_zh": [
                "核心问题：扩散模型难以准确生成文本提示中指定的对象数量，仅靠扩大模型和数据集无效。",
                "方法要点：通过向噪声先验注入计数感知布局信息，控制对象数量生成。",
                "实验或效果：在基准测试中，准确率从20.0%提升至85.3%，并实现跨场景泛化。"
            ],
            "tags_zh": [
                "扩散模型",
                "计数准确性",
                "噪声先验",
                "布局信息注入",
                "文本到图像生成"
            ],
            "_index": 88
        },
        {
            "title": "Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning",
            "authors": [
                "Hao Tang",
                "Shengfeng He",
                "Jing Qin"
            ],
            "arxiv_id": "2510.11115v1",
            "summary": "Few-shot learning (FSL) addresses the challenge of classifying novel classes\nwith limited training samples. While some methods leverage semantic knowledge\nfrom smaller-scale models to mitigate data scarcity, these approaches often\nintroduce noise and bias due to the data's inherent simplicity. In this paper,\nwe propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which\neffectively transfers diverse and complementary knowledge from large multimodal\nmodels to empower the off-the-shelf few-shot learner. Specifically, SynTrans\nemploys CLIP as a robust teacher and uses a few-shot vision encoder as a weak\nstudent, distilling semantic-aligned visual knowledge via an unsupervised proxy\ntask. Subsequently, a training-free synergistic knowledge mining module\nfacilitates collaboration among large multimodal models to extract high-quality\nsemantic knowledge. Building upon this, a visual-semantic bridging module\nenables bi-directional knowledge transfer between visual and semantic spaces,\ntransforming explicit visual and implicit semantic knowledge into\ncategory-specific classifier weights. Finally, SynTrans introduces a visual\nweight generator and a semantic weight reconstructor to adaptively construct\noptimal multimodal FSL classifiers. Experimental results on four FSL datasets\ndemonstrate that SynTrans, even when paired with a simple few-shot vision\nencoder, significantly outperforms current state-of-the-art methods.",
            "headline_zh": "提出SynTrans框架，通过大模型知识转移增强少样本学习性能",
            "intro_zh": [
                "少样本学习面临数据稀缺和噪声问题，现有方法易引入偏差",
                "利用CLIP作为教师模型，通过无监督代理任务蒸馏视觉知识",
                "在四个数据集上实验，显著优于当前最优方法"
            ],
            "tags_zh": [
                "少样本学习",
                "知识蒸馏",
                "多模态模型",
                "视觉语义桥接",
                "分类器构建"
            ],
            "_index": 89
        },
        {
            "title": "Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment",
            "authors": [
                "Chen Liu",
                "Wenfang Yao",
                "Kejing Yin",
                "William K. Cheung",
                "Jing Qin"
            ],
            "arxiv_id": "2510.11112v1",
            "summary": "Longitudinal multimodal data, including electronic health records (EHR) and\nsequential chest X-rays (CXRs), is critical for modeling disease progression,\nyet remains underutilized due to two key challenges: (1) redundancy in\nconsecutive CXR sequences, where static anatomical regions dominate over\nclinically-meaningful dynamics, and (2) temporal misalignment between sparse,\nirregular imaging and continuous EHR data. We introduce $\\texttt{DiPro}$, a\nnovel framework that addresses these challenges through region-aware\ndisentanglement and multi-timescale alignment. First, we disentangle static\n(anatomy) and dynamic (pathology progression) features in sequential CXRs,\nprioritizing disease-relevant changes. Second, we hierarchically align these\nstatic and dynamic CXR features with asynchronous EHR data via local (pairwise\ninterval-level) and global (full-sequence) synchronization to model coherent\nprogression pathways. Extensive experiments on the MIMIC dataset demonstrate\nthat $\\texttt{DiPro}$ could effectively extract temporal clinical dynamics and\nachieve state-of-the-art performance on both disease progression identification\nand general ICU prediction tasks.",
            "headline_zh": "提出DiPro框架，通过时空解耦和多尺度对齐解决多模态疾病进展建模问题",
            "intro_zh": [
                "核心问题：连续CXR序列冗余和EHR与成像数据时间不对齐阻碍疾病进展建模",
                "方法要点：解耦CXR静态与动态特征，并分层对齐EHR数据以建模进展路径",
                "实验或效果：在MIMIC数据集上实现疾病识别和ICU预测的先进性能"
            ],
            "tags_zh": [
                "多模态学习",
                "疾病进展建模",
                "时空解耦",
                "多尺度对齐",
                "电子健康记录",
                "胸部X光序列"
            ],
            "_index": 90
        },
        {
            "title": "MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps",
            "authors": [
                "Jiahui Lei",
                "Kyle Genova",
                "George Kopanas",
                "Noah Snavely",
                "Leonidas Guibas"
            ],
            "arxiv_id": "2510.11107v1",
            "summary": "This paper addresses the challenge of learning semantically and functionally\nmeaningful 3D motion priors from real-world videos, in order to enable\nprediction of future 3D scene motion from a single input image. We propose a\nnovel pixel-aligned Motion Map (MoMap) representation for 3D scene motion,\nwhich can be generated from existing generative image models to facilitate\nefficient and effective motion prediction. To learn meaningful distributions\nover motion, we create a large-scale database of MoMaps from over 50,000 real\nvideos and train a diffusion model on these representations. Our motion\ngeneration not only synthesizes trajectories in 3D but also suggests a new\npipeline for 2D video synthesis: first generate a MoMap, then warp an image\naccordingly and complete the warped point-based renderings. Experimental\nresults demonstrate that our approach generates plausible and semantically\nconsistent 3D scene motion.",
            "headline_zh": "提出MoMaps表示以从单张图像预测语义一致的3D场景运动",
            "intro_zh": [
                "核心问题：从真实视频学习语义和功能有意义的3D运动先验，用于单图像预测未来运动。",
                "方法要点：引入像素对齐的MoMap表示，基于扩散模型从大规模视频数据库生成3D运动。",
                "实验或效果：生成合理且语义一致的3D场景运动，并支持2D视频合成新流程。"
            ],
            "tags_zh": [
                "3D场景运动生成",
                "运动地图表示",
                "扩散模型",
                "视频合成",
                "语义运动先验"
            ],
            "_index": 91
        },
        {
            "title": "Compositional Zero-Shot Learning: A Survey",
            "authors": [
                "Ans Munir",
                "Faisal Z. Qureshi",
                "Mohsen Ali",
                "Muhammad Haris Khan"
            ],
            "arxiv_id": "2510.11106v1",
            "summary": "Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision\nthat enables models to recognize unseen combinations of known attributes and\nobjects during inference, addressing the combinatorial challenge of requiring\ntraining data for every possible composition. This is particularly challenging\nbecause the visual appearance of primitives is highly contextual; for example,\n``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars\ndiffer significantly from ``wet'' cats. Effectively modeling this contextuality\nand the inherent compositionality is crucial for robust compositional zero-shot\nrecognition. This paper presents, to our knowledge, the first comprehensive\nsurvey specifically focused on Compositional Zero-Shot Learning. We\nsystematically review the state-of-the-art CZSL methods, introducing a taxonomy\ngrounded in disentanglement, with four families of approaches: no explicit\ndisentanglement, textual disentanglement, visual disentanglement, and\ncross-modal disentanglement. We provide a detailed comparative analysis of\nthese methods, highlighting their core advantages and limitations in different\nproblem settings, such as closed-world and open-world CZSL. Finally, we\nidentify the most significant open challenges and outline promising future\nresearch directions. This survey aims to serve as a foundational resource to\nguide and inspire further advancements in this fascinating and important field.\nPapers studied in this survey with their official code are available on our\ngithub: https://github.com/ans92/Compositional-Zero-Shot-Learning",
            "headline_zh": "综述组合零样本学习方法，解决未见属性-对象组合识别问题",
            "intro_zh": [
                "核心问题：模型需识别训练中未见的属性与对象组合，视觉外观高度依赖上下文",
                "方法要点：基于解缠分类法，涵盖无解缠、文本解缠、视觉解缠和跨模态解缠",
                "实验或效果：比较不同方法在封闭与开放世界设置下的优势与局限"
            ],
            "tags_zh": [
                "组合零样本学习",
                "解缠方法",
                "跨模态学习",
                "零样本识别",
                "计算机视觉综述"
            ],
            "_index": 92
        },
        {
            "title": "A Primer on SO(3) Action Representations in Deep Reinforcement Learning",
            "authors": [
                "Martin Schuck",
                "Sherif Samy",
                "Angela P. Schoellig"
            ],
            "arxiv_id": "2510.11103v1",
            "summary": "Many robotic control tasks require policies to act on orientations, yet the\ngeometry of SO(3) makes this nontrivial. Because SO(3) admits no global,\nsmooth, minimal parameterization, common representations such as Euler angles,\nquaternions, rotation matrices, and Lie algebra coordinates introduce distinct\nconstraints and failure modes. While these trade-offs are well studied for\nsupervised learning, their implications for actions in reinforcement learning\nremain unclear. We systematically evaluate SO(3) action representations across\nthree standard continuous control algorithms, PPO, SAC, and TD3, under dense\nand sparse rewards. We compare how representations shape exploration, interact\nwith entropy regularization, and affect training stability through empirical\nstudies and analyze the implications of different projections for obtaining\nvalid rotations from Euclidean network outputs. Across a suite of robotics\nbenchmarks, we quantify the practical impact of these choices and distill\nsimple, implementation-ready guidelines for selecting and using rotation\nactions. Our results highlight that representation-induced geometry strongly\ninfluences exploration and optimization and show that representing actions as\ntangent vectors in the local frame yields the most reliable results across\nalgorithms.",
            "headline_zh": "评估SO(3)动作表示在深度强化学习中的影响，提供机器人控制指南",
            "intro_zh": [
                "机器人控制中SO(3)几何使动作表示非平凡，常见参数化引入约束和失败模式",
                "系统评估Euler角、四元数等表示在PPO、SAC、TD3算法下的探索和稳定性",
                "实验显示局部切向量表示最可靠，量化几何对优化影响并给出实用选择指南"
            ],
            "tags_zh": [
                "SO(3)表示",
                "深度强化学习",
                "机器人控制",
                "动作几何",
                "探索优化",
                "实验评估"
            ],
            "_index": 93
        },
        {
            "title": "CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization",
            "authors": [
                "Fengling Zhu",
                "Boshi Liu",
                "Jingyu Hua",
                "Sheng Zhong"
            ],
            "arxiv_id": "2510.11096v1",
            "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\ntasks such as image captioning, visual question answering, and cross-modal\nreasoning by integrating visual and textual modalities. However, their\nmultimodal nature also exposes them to adversarial threats, where attackers can\nperturb either modality or both jointly to induce harmful, misleading, or\npolicy violating outputs. Existing defense strategies, such as adversarial\ntraining and input purification, face notable limitations: adversarial training\ntypically improves robustness only against known attacks while incurring high\ncomputational costs, whereas conventional purification approaches often suffer\nfrom degraded image quality and insufficient generalization to complex\nmultimodal tasks.\n  In this work, we focus on defending the visual modality, which frequently\nserves as the primary entry point for adversarial manipulation. We propose a\nsupervised diffusion based denoising framework that leverages paired\nadversarial clean image datasets to fine-tune diffusion models with\ndirectional, task specific guidance. Unlike prior unsupervised purification\nmethods such as DiffPure, our approach achieves higher quality reconstructions\nwhile significantly improving defense robustness in multimodal tasks.\nFurthermore, we incorporate prompt optimization as a complementary defense\nmechanism, enhancing resistance against diverse and unseen attack strategies.\n  Extensive experiments on image captioning and visual question answering\ndemonstrate that our method not only substantially improves robustness but also\nexhibits strong transferability to unknown adversarial attacks. These results\nhighlight the effectiveness of supervised diffusion based denoising for\nmultimodal defense, paving the way for more reliable and secure deployment of\nMLLMs in real world applications.",
            "headline_zh": "提出监督扩散去噪与提示优化以防御多模态大语言模型的视觉对抗攻击",
            "intro_zh": [
                "多模态大语言模型易受视觉对抗攻击，导致有害或误导输出",
                "使用监督扩散去噪框架，结合任务特定指导提升图像重建质量与防御鲁棒性",
                "在图像描述和视觉问答任务中验证，显著提升鲁棒性并具强迁移性"
            ],
            "tags_zh": [
                "多模态防御",
                "扩散去噪",
                "提示优化",
                "对抗攻击",
                "图像重建",
                "鲁棒性提升"
            ],
            "_index": 94
        },
        {
            "title": "Design and Koopman Model Predictive Control of A Soft Exoskeleton Based on Origami-Inspired Pneumatic Actuator for Knee Rehabilitation",
            "authors": [
                "Junxiang Wang",
                "Han Zhang",
                "Zehao Wang",
                "Huaiyuan Chen",
                "Pu Wang",
                "Weidong Chen"
            ],
            "arxiv_id": "2510.11094v1",
            "summary": "Effective rehabilitation methods are essential for the recovery of lower limb\ndysfunction caused by stroke. Nowadays, robotic exoskeletons have shown great\npotentials in rehabilitation. Nevertheless, traditional rigid exoskeletons are\nusually heavy and need a lot of work to help the patients to put them on.\nMoreover, it also requires extra compliance control to guarantee the safety. In\ncontrast, soft exoskeletons are easy and comfortable to wear and have intrinsic\ncompliance, but their complex nonlinear human-robot interaction dynamics would\npose significant challenges for control. In this work, based on the pneumatic\nactuators inspired by origami, we design a rehabilitation exoskeleton for knee\nthat is easy and comfortable to wear. To guarantee the control performance and\nenable a nice human-robot interaction, we first use Deep Koopman Network to\nmodel the human-robot interaction dynamics. In particular, by viewing the\nelectromyography (EMG) signals and the duty cycle of the PWM wave that controls\nthe pneumatic robot's valves and pump as the inputs, the linear Koopman model\naccurately captures the complex human-robot interaction dynamics. Next, based\non the obtained Koopman model, we further use Model Predictive Control (MPC) to\ncontrol the soft robot and help the user to do rehabilitation training in\nreal-time. The goal of the rehabilitation training is to track a given\nreference signal shown on the screen. Experiments show that by integrating the\nEMG signals into the Koopman model, we have improved the model accuracy to\ngreat extent. In addition, a personalized Koopman model trained from the\nindividual's own data performs better than the non-personalized model.\nConsequently, our control framework outperforms the traditional PID control in\nboth passive and active training modes. Hence the proposed method provides a\nnew control framework for soft rehabilitation robots.",
            "headline_zh": "提出基于折纸气动执行器的软外骨骼及Koopman模型预测控制，用于膝关节康复。",
            "intro_zh": [
                "核心问题：软外骨骼人机交互动态复杂非线性，控制困难。",
                "方法要点：使用深度Koopman网络建模动态，结合肌电信号和PWM占空比输入。",
                "实验或效果：个性化模型优于非个性化，控制框架在被动和主动训练中超越PID。"
            ],
            "tags_zh": [
                "软外骨骼",
                "Koopman模型",
                "模型预测控制",
                "膝关节康复",
                "气动执行器",
                "人机交互"
            ],
            "_index": 95
        },
        {
            "title": "Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution",
            "authors": [
                "Bozhou Zhang",
                "Nan Song",
                "Jingyu Li",
                "Xiatian Zhu",
                "Jiankang Deng",
                "Li Zhang"
            ],
            "arxiv_id": "2510.11092v1",
            "summary": "End-to-end autonomous driving methods aim to directly map raw sensor inputs\nto future driving actions such as planned trajectories, bypassing traditional\nmodular pipelines. While these approaches have shown promise, they often\noperate under a one-shot paradigm that relies heavily on the current scene\ncontext, potentially underestimating the importance of scene dynamics and their\ntemporal evolution. This limitation restricts the model's ability to make\ninformed and adaptive decisions in complex driving scenarios. We propose a new\nperspective: the future trajectory of an autonomous vehicle is closely\nintertwined with the evolving dynamics of its environment, and conversely, the\nvehicle's own future states can influence how the surrounding scene unfolds.\nMotivated by this bidirectional relationship, we introduce SeerDrive, a novel\nend-to-end framework that jointly models future scene evolution and trajectory\nplanning in a closed-loop manner. Our method first predicts future bird's-eye\nview (BEV) representations to anticipate the dynamics of the surrounding scene,\nthen leverages this foresight to generate future-context-aware trajectories.\nTwo key components enable this: (1) future-aware planning, which injects\npredicted BEV features into the trajectory planner, and (2) iterative scene\nmodeling and vehicle planning, which refines both future scene prediction and\ntrajectory generation through collaborative optimization. Extensive experiments\non the NAVSIM and nuScenes benchmarks show that SeerDrive significantly\noutperforms existing state-of-the-art methods.",
            "headline_zh": "提出SeerDrive框架，通过双向建模轨迹规划与场景演化以提升端到端自动驾驶性能",
            "intro_zh": [
                "核心问题：端到端自动驾驶方法依赖当前场景，低估动态演化，限制复杂场景决策能力",
                "方法要点：联合建模未来场景BEV预测与轨迹规划，实现闭环迭代优化",
                "实验或效果：在NAVSIM和nuScenes基准上显著优于现有先进方法"
            ],
            "tags_zh": [
                "端到端自动驾驶",
                "轨迹规划",
                "场景演化预测",
                "鸟瞰图表示",
                "闭环建模",
                "双向关系"
            ],
            "_index": 96
        },
        {
            "title": "Text-Enhanced Panoptic Symbol Spotting in CAD Drawings",
            "authors": [
                "Xianlin Liu",
                "Yan Gong",
                "Bohao Li",
                "Jiajing Huang",
                "Bowen Du",
                "Junchen Ye",
                "Liyan Xu"
            ],
            "arxiv_id": "2510.11091v1",
            "summary": "With the widespread adoption of Computer-Aided Design(CAD) drawings in\nengineering, architecture, and industrial design, the ability to accurately\ninterpret and analyze these drawings has become increasingly critical. Among\nvarious subtasks, panoptic symbol spotting plays a vital role in enabling\ndownstream applications such as CAD automation and design retrieval. Existing\nmethods primarily focus on geometric primitives within the CAD drawings to\naddress this task, but they face following major problems: they usually\noverlook the rich textual annotations present in CAD drawings and they lack\nexplicit modeling of relationships among primitives, resulting in\nincomprehensive understanding of the holistic drawings. To fill this gap, we\npropose a panoptic symbol spotting framework that incorporates textual\nannotations. The framework constructs unified representations by jointly\nmodeling geometric and textual primitives. Then, using visual features extract\nby pretrained CNN as the initial representations, a Transformer-based backbone\nis employed, enhanced with a type-aware attention mechanism to explicitly model\nthe different types of spatial dependencies between various primitives.\nExtensive experiments on the real-world dataset demonstrate that the proposed\nmethod outperforms existing approaches on symbol spotting tasks involving\ntextual annotations, and exhibits superior robustness when applied to complex\nCAD drawings.",
            "headline_zh": "提出融合文本的CAD图纸全景符号识别框架，以提升复杂图纸理解能力。",
            "intro_zh": [
                "现有方法忽视文本注释和空间关系建模，导致图纸理解不全面。",
                "联合建模几何与文本基元，使用Transformer和类型感知注意力机制。",
                "在真实数据集上优于现有方法，对复杂CAD图纸具有更强鲁棒性。"
            ],
            "tags_zh": [
                "CAD图纸分析",
                "全景符号识别",
                "文本增强",
                "Transformer模型",
                "类型感知注意力"
            ],
            "_index": 97
        },
        {
            "title": "Source-Free Object Detection with Detection Transformer",
            "authors": [
                "Huizai Yao",
                "Sicheng Zhao",
                "Shuo Lu",
                "Hui Chen",
                "Yangyang Li",
                "Guoping Liu",
                "Tengfei Xing",
                "Chenggang Yan",
                "Jianhua Tao",
                "Guiguang Ding"
            ],
            "arxiv_id": "2510.11090v1",
            "summary": "Source-Free Object Detection (SFOD) enables knowledge transfer from a source\ndomain to an unsupervised target domain for object detection without access to\nsource data. Most existing SFOD approaches are either confined to conventional\nobject detection (OD) models like Faster R-CNN or designed as general solutions\nwithout tailored adaptations for novel OD architectures, especially Detection\nTransformer (DETR). In this paper, we introduce Feature Reweighting ANd\nContrastive Learning NetworK (FRANCK), a novel SFOD framework specifically\ndesigned to perform query-centric feature enhancement for DETRs. FRANCK\ncomprises four key components: (1) an Objectness Score-based Sample Reweighting\n(OSSR) module that computes attention-based objectness scores on multi-scale\nencoder feature maps, reweighting the detection loss to emphasize\nless-recognized regions; (2) a Contrastive Learning with Matching-based Memory\nBank (CMMB) module that integrates multi-level features into memory banks,\nenhancing class-wise contrastive learning; (3) an Uncertainty-weighted\nQuery-fused Feature Distillation (UQFD) module that improves feature\ndistillation through prediction quality reweighting and query feature fusion;\nand (4) an improved self-training pipeline with a Dynamic Teacher Updating\nInterval (DTUI) that optimizes pseudo-label quality. By leveraging these\ncomponents, FRANCK effectively adapts a source-pre-trained DETR model to a\ntarget domain with enhanced robustness and generalization. Extensive\nexperiments on several widely used benchmarks demonstrate that our method\nachieves state-of-the-art performance, highlighting its effectiveness and\ncompatibility with DETR-based SFOD models.",
            "headline_zh": "提出FRANCK框架以解决源自由目标检测中DETR模型的适应问题",
            "intro_zh": [
                "核心问题：源自由目标检测中DETR模型缺乏专门适配，导致性能受限。",
                "方法要点：通过特征重加权、对比学习和查询融合蒸馏模块增强模型鲁棒性。",
                "实验或效果：在多个基准测试中实现最先进性能，验证了方法的有效性。"
            ],
            "tags_zh": [
                "源自由目标检测",
                "检测变换器",
                "特征重加权",
                "对比学习",
                "查询融合蒸馏",
                "自训练优化"
            ],
            "_index": 98
        },
        {
            "title": "Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling",
            "authors": [
                "Tianyi Tan",
                "Yinan Zheng",
                "Ruiming Liang",
                "Zexu Wang",
                "Kexin Zheng",
                "Jinliang Zheng",
                "Jianxiong Li",
                "Xianyuan Zhan",
                "Jingjing Liu"
            ],
            "arxiv_id": "2510.11083v1",
            "summary": "Modeling interactive driving behaviors in complex scenarios remains a\nfundamental challenge for autonomous driving planning. Learning-based\napproaches attempt to address this challenge with advanced generative models,\nremoving the dependency on over-engineered architectures for representation\nfusion. However, brute-force implementation by simply stacking transformer\nblocks lacks a dedicated mechanism for modeling interactive behaviors that are\ncommon in real driving scenarios. The scarcity of interactive driving data\nfurther exacerbates this problem, leaving conventional imitation learning\nmethods ill-equipped to capture high-value interactive behaviors. We propose\nFlow Planner, which tackles these problems through coordinated innovations in\ndata modeling, model architecture, and learning scheme. Specifically, we first\nintroduce fine-grained trajectory tokenization, which decomposes the trajectory\ninto overlapping segments to decrease the complexity of whole trajectory\nmodeling. With a sophisticatedly designed architecture, we achieve efficient\ntemporal and spatial fusion of planning and scene information, to better\ncapture interactive behaviors. In addition, the framework incorporates flow\nmatching with classifier-free guidance for multi-modal behavior generation,\nwhich dynamically reweights agent interactions during inference to maintain\ncoherent response strategies, providing a critical boost for interactive\nscenario understanding. Experimental results on the large-scale nuPlan dataset\nand challenging interactive interPlan dataset demonstrate that Flow Planner\nachieves state-of-the-art performance among learning-based approaches while\neffectively modeling interactive behaviors in complex driving scenarios.",
            "headline_zh": "提出Flow Planner以解决复杂驾驶场景中的交互行为建模问题",
            "intro_zh": [
                "核心问题：交互驾驶行为建模困难，数据稀缺导致传统模仿学习难以捕捉高价值行为",
                "方法要点：采用细粒度轨迹标记化和流匹配，结合分类器自由引导生成多模态行为",
                "实验或效果：在nuPlan和interPlan数据集上实现SOTA性能，有效建模交互行为"
            ],
            "tags_zh": [
                "自动驾驶规划",
                "交互行为建模",
                "流匹配",
                "轨迹标记化",
                "多模态生成",
                "学习型方法"
            ],
            "_index": 99
        },
        {
            "title": "ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer",
            "authors": [
                "Yuan Tian",
                "Min Zhou",
                "Yitong Chen",
                "Fang Li",
                "Lingzi Qi",
                "Shuo Wang",
                "Xieyang Xu",
                "Yu Yu",
                "Shiqiong Xu",
                "Chaoyu Lei",
                "Yankai Jiang",
                "Rongzhao Zhang",
                "Jia Tan",
                "Li Wu",
                "Hong Chen",
                "Xiaowei Liu",
                "Wei Lu",
                "Lin Li",
                "Huifang Zhou",
                "Xuefei Song",
                "Guangtao Zhai",
                "Xianqun Fan"
            ],
            "arxiv_id": "2510.11073v1",
            "summary": "Patient face images provide a convenient mean for evaluating eye diseases,\nwhile also raising privacy concerns. Here, we introduce ROFI, a deep\nlearning-based privacy protection framework for ophthalmology. Using weakly\nsupervised learning and neural identity translation, ROFI anonymizes facial\nfeatures while retaining disease features (over 98\\% accuracy, $\\kappa >\n0.90$). It achieves 100\\% diagnostic sensitivity and high agreement ($\\kappa >\n0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\\% of\nimages. ROFI works with AI systems, maintaining original diagnoses ($\\kappa >\n0.80$), and supports secure image reversal (over 98\\% similarity), enabling\naudits and long-term care. These results show ROFI's effectiveness of\nprotecting patient privacy in the digital medicine era.",
            "headline_zh": "提出ROFI框架以在眼科中匿名化患者面部同时保留疾病特征",
            "intro_zh": [
                "核心问题：患者面部图像在眼科诊断中易泄露隐私，需平衡隐私保护与疾病特征保留。",
                "方法要点：采用弱监督学习和神经身份转换技术，实现面部特征匿名化。",
                "实验或效果：在11种眼病中保持高诊断敏感性和一致性，支持图像安全反转。"
            ],
            "tags_zh": [
                "隐私保护",
                "眼科图像处理",
                "深度学习",
                "神经身份转换",
                "弱监督学习",
                "图像反转"
            ],
            "_index": 100
        },
        {
            "title": "PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System",
            "authors": [
                "Huayi Wang",
                "Wentao Zhang",
                "Runyi Yu",
                "Tao Huang",
                "Junli Ren",
                "Feiyu Jia",
                "Zirui Wang",
                "Xiaojie Niu",
                "Xiao Chen",
                "Jiahe Chen",
                "Qifeng Chen",
                "Jingbo Wang",
                "Jiangmiao Pang"
            ],
            "arxiv_id": "2510.11072v1",
            "summary": "Deploying humanoid robots to interact with real-world environments--such as\ncarrying objects or sitting on chairs--requires generalizable, lifelike motions\nand robust scene perception. Although prior approaches have advanced each\ncapability individually, combining them in a unified system is still an ongoing\nchallenge. In this work, we present a physical-world humanoid-scene interaction\nsystem, PhysHSI, that enables humanoids to autonomously perform diverse\ninteraction tasks while maintaining natural and lifelike behaviors. PhysHSI\ncomprises a simulation training pipeline and a real-world deployment system. In\nsimulation, we adopt adversarial motion prior-based policy learning to imitate\nnatural humanoid-scene interaction data across diverse scenarios, achieving\nboth generalization and lifelike behaviors. For real-world deployment, we\nintroduce a coarse-to-fine object localization module that combines LiDAR and\ncamera inputs to provide continuous and robust scene perception. We validate\nPhysHSI on four representative interactive tasks--box carrying, sitting, lying,\nand standing up--in both simulation and real-world settings, demonstrating\nconsistently high success rates, strong generalization across diverse task\ngoals, and natural motion patterns.",
            "headline_zh": "提出PhysHSI系统，实现人形机器人在真实世界中通用且自然的场景交互。",
            "intro_zh": [
                "核心问题：人形机器人需在真实环境中执行交互任务，但现有方法难以统一泛化运动和鲁棒感知。",
                "方法要点：采用模拟对抗运动先验学习，结合LiDAR与相机粗到精定位，提升交互自然性和鲁棒性。",
                "实验或效果：在模拟和真实世界验证四种交互任务，成功率高、泛化强、运动自然。"
            ],
            "tags_zh": [
                "人形机器人交互",
                "对抗运动先验",
                "场景感知",
                "模拟到真实部署",
                "多模态定位"
            ],
            "_index": 101
        },
        {
            "title": "LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation",
            "authors": [
                "Chang Liu",
                "Henghui Ding",
                "Kaining Ying",
                "Lingyi Hong",
                "Ning Xu",
                "Linjie Yang",
                "Yuchen Fan",
                "Mingqi Gao",
                "Jingkun Chen",
                "Yunqi Miao",
                "Gengshen Wu",
                "Zhijin Qin",
                "Jungong Han",
                "Zhixiong Zhang",
                "Shuangrui Ding",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuhang Cao",
                "Jiaqi Wang",
                "Chang Soo Lim",
                "Joonyoung Moon",
                "Donghyeon Cho",
                "Tingmin Li",
                "Yixuan Li",
                "Yang Yang",
                "An Yan",
                "Leilei Cao",
                "Feng Lu",
                "Ran Hong",
                "Youhai Jiang",
                "Fengjie Zhu",
                "Yujie Xie",
                "Hongyang Zhang",
                "Zhihui Liu",
                "Shihai Ruan",
                "Quanzhu Niu",
                "Dengxian Gong",
                "Shihao Chen",
                "Tao Zhang",
                "Yikang Zhou",
                "Haobo Yuan",
                "Lu Qi",
                "Xiangtai Li",
                "Shunping Ji",
                "Ran Hong",
                "Feng Lu",
                "Leilei Cao",
                "An Yan",
                "Alexey Nekrasov",
                "Ali Athar",
                "Daan de Geus",
                "Alexander Hermans",
                "Bastian Leibe"
            ],
            "arxiv_id": "2510.11063v1",
            "summary": "This report presents an overview of the 7th Large-scale Video Object\nSegmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the\ntwo traditional tracks of LSVOS that jointly target robustness in realistic\nvideo scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition\nfeatures a newly introduced track, Complex VOS (MOSEv2). Building upon prior\ninsights, MOSEv2 substantially increases difficulty, introducing more\nchallenging but realistic scenarios including denser small objects, frequent\ndisappear/reappear events, severe occlusions, adverse weather and lighting,\netc., pushing long-term consistency and generalization beyond curated\nbenchmarks. The challenge retains standard ${J}$, $F$, and ${J\\&F}$ metrics for\nVOS and RVOS, while MOSEv2 adopts ${J\\&\\dot{F}}$ as the primary ranking metric\nto better evaluate objects across scales and disappearance cases. We summarize\ndatasets and protocols, highlight top-performing solutions, and distill\nemerging trends, such as the growing role of LLM/MLLM components and\nmemory-aware propagation, aiming to chart future directions for resilient,\nlanguage-aware video segmentation in the wild.",
            "headline_zh": "报告LSVOS 2025挑战赛进展，引入复杂视频对象分割新赛道以提升真实场景鲁棒性。",
            "intro_zh": [
                "核心问题：视频对象分割在复杂场景中面临小对象密集、频繁消失重现、严重遮挡及恶劣天气等挑战。",
                "方法要点：新增MOSEv2赛道，采用J&Ḟ指标评估多尺度和消失情况，强调LLM/MLLM组件和记忆感知传播。",
                "实验或效果：总结数据集、协议和顶尖方案，推动语言感知视频分割在野外环境中的未来发展。"
            ],
            "tags_zh": [
                "视频对象分割",
                "复杂场景鲁棒性",
                "多模态语言模型",
                "记忆感知传播",
                "挑战赛报告",
                "长期一致性"
            ],
            "_index": 102
        },
        {
            "title": "Zero-shot Face Editing via ID-Attribute Decoupled Inversion",
            "authors": [
                "Yang Hou",
                "Minggu Wang",
                "Jianjun Zhao"
            ],
            "arxiv_id": "2510.11050v1",
            "summary": "Recent advancements in text-guided diffusion models have shown promise for\ngeneral image editing via inversion techniques, but often struggle to maintain\nID and structural consistency in real face editing tasks. To address this\nlimitation, we propose a zero-shot face editing method based on ID-Attribute\nDecoupled Inversion. Specifically, we decompose the face representation into ID\nand attribute features, using them as joint conditions to guide both the\ninversion and the reverse diffusion processes. This allows independent control\nover ID and attributes, ensuring strong ID preservation and structural\nconsistency while enabling precise facial attribute manipulation. Our method\nsupports a wide range of complex multi-attribute face editing tasks using only\ntext prompts, without requiring region-specific input, and operates at a speed\ncomparable to DDIM inversion. Comprehensive experiments demonstrate its\npracticality and effectiveness.",
            "headline_zh": "提出ID-属性解耦反演方法以解决零样本人脸编辑中的ID和结构一致性问题",
            "intro_zh": [
                "核心问题：文本引导扩散模型在真实人脸编辑中难以保持ID和结构一致性",
                "方法要点：将人脸表示分解为ID和属性特征，作为联合条件指导反演和扩散过程",
                "实验或效果：支持多属性编辑，仅需文本提示，保持ID并实现精确操控"
            ],
            "tags_zh": [
                "人脸编辑",
                "零样本学习",
                "扩散模型",
                "ID保持",
                "属性操控"
            ],
            "_index": 103
        },
        {
            "title": "Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset",
            "authors": [
                "Nivea Roy",
                "Son Tran",
                "Atul Sajjanhar",
                "K. Devaraja",
                "Prakashini Koteshwara",
                "Yong Xiang",
                "Divya Rao"
            ],
            "arxiv_id": "2510.11047v1",
            "summary": "Laryngeal cancer imaging research lacks standardised datasets to enable\nreproducible deep learning (DL) model development. We present LaryngealCT, a\ncurated benchmark of 1,029 computed tomography (CT) scans aggregated from six\ncollections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic\nvolumes of interest encompassing the larynx were extracted using a weakly\nsupervised parameter search framework validated by clinical experts. 3D DL\narchitectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i)\nearly (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification\ntasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892,\nF1-macro-0.646) respectively outperformed the other models in the two tasks.\nModel explainability assessed using 3D GradCAMs with thyroid cartilage overlays\nrevealed greater peri-cartilage attention in non-T4 cases and focal activations\nin T4 predictions. Through open-source data, pretrained models, and integrated\nexplainability tools, LaryngealCT offers a reproducible foundation for\nAI-driven research to support clinical decisions in laryngeal oncology.",
            "headline_zh": "提出LaryngealCT数据集和基准测试，以解决喉癌分期缺乏标准化数据的问题。",
            "intro_zh": [
                "核心问题：喉癌影像研究缺乏标准化数据集，影响深度学习模型的可复现性。",
                "方法要点：构建包含1029个CT扫描的LaryngealCT数据集，使用弱监督参数搜索提取感兴趣区域。",
                "实验或效果：在早期vs晚期和T4vs非T4分类任务中，3D CNN和ResNet18模型表现最佳。"
            ],
            "tags_zh": [
                "喉癌分期",
                "深度学习基准测试",
                "3D医学影像",
                "模型可解释性",
                "CT扫描数据集"
            ],
            "_index": 104
        },
        {
            "title": "Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy",
            "authors": [
                "Shiyao Zhang",
                "Liwei Deng",
                "Shuyu Zhang",
                "Weijie Yuan",
                "Hong Zhang"
            ],
            "arxiv_id": "2510.11041v1",
            "summary": "In future intelligent transportation systems, autonomous cooperative planning\n(ACP), becomes a promising technique to increase the effectiveness and security\nof multi-vehicle interactions. However, multiple uncertainties cannot be fully\naddressed for existing ACP strategies, e.g. perception, planning, and\ncommunication uncertainties. To address these, a novel deep reinforcement\nlearning-based autonomous cooperative planning (DRLACP) framework is proposed\nto tackle various uncertainties on cooperative motion planning schemes.\nSpecifically, the soft actor-critic (SAC) with the implementation of gate\nrecurrent units (GRUs) is adopted to learn the deterministic optimal\ntime-varying actions with imperfect state information occurred by planning,\ncommunication, and perception uncertainties. In addition, the real-time actions\nof autonomous vehicles (AVs) are demonstrated via the Car Learning to Act\n(CARLA) simulation platform. Evaluation results show that the proposed DRLACP\nlearns and performs cooperative planning effectively, which outperforms other\nbaseline methods under different scenarios with imperfect AV state information.",
            "headline_zh": "提出基于深度强化学习的自主协同规划框架以解决多车辆交互中的不确定性",
            "intro_zh": [
                "核心问题：现有自主协同规划策略无法充分处理感知、规划和通信中的多种不确定性",
                "方法要点：采用软演员-评论者算法结合门控循环单元学习时变动作，应对不完美状态信息",
                "实验或效果：在CARLA仿真平台验证，优于基线方法，有效提升协同规划性能"
            ],
            "tags_zh": [
                "自主协同规划",
                "深度强化学习",
                "不确定性处理",
                "软演员-评论者算法",
                "门控循环单元",
                "CARLA仿真"
            ],
            "_index": 105
        },
        {
            "title": "XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation",
            "authors": [
                "Yeonseo Lee",
                "Jungwook Mun",
                "Hyosup Shin",
                "Guebin Hwang",
                "Junhee Nam",
                "Taeyeop Lee",
                "Sungho Jo"
            ],
            "arxiv_id": "2510.11036v1",
            "summary": "Most robotic grasping methods are typically designed for single gripper\ntypes, which limits their applicability in real-world scenarios requiring\ndiverse end-effectors. We propose XGrasp, a real-time gripper-aware grasp\ndetection framework that efficiently handles multiple gripper configurations.\nThe proposed method addresses data scarcity by systematically augmenting\nexisting datasets with multi-gripper annotations. XGrasp employs a hierarchical\ntwo-stage architecture. In the first stage, a Grasp Point Predictor (GPP)\nidentifies optimal locations using global scene information and gripper\nspecifications. In the second stage, an Angle-Width Predictor (AWP) refines the\ngrasp angle and width using local features. Contrastive learning in the AWP\nmodule enables zero-shot generalization to unseen grippers by learning\nfundamental grasping characteristics. The modular framework integrates\nseamlessly with vision foundation models, providing pathways for future\nvision-language capabilities. The experimental results demonstrate competitive\ngrasp success rates across various gripper types, while achieving substantial\nimprovements in inference speed compared to existing gripper-aware methods.\nProject page: https://sites.google.com/view/xgrasp",
            "headline_zh": "提出XGrasp框架以解决多夹具场景下的抓取检测问题",
            "intro_zh": [
                "核心问题：现有抓取方法多针对单一夹具，限制实际应用多样性。",
                "方法要点：采用两阶段架构，结合全局场景和局部特征预测抓取点、角度与宽度。",
                "实验或效果：在多种夹具上实现高抓取成功率，并显著提升推理速度。"
            ],
            "tags_zh": [
                "机器人抓取检测",
                "多夹具处理",
                "对比学习",
                "实时推理",
                "数据增强"
            ],
            "_index": 106
        },
        {
            "title": "Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts",
            "authors": [
                "Yanning Hou",
                "Ke Xu",
                "Junfa Li",
                "Yanran Ruan",
                "Jianfeng Qiu"
            ],
            "arxiv_id": "2510.11028v1",
            "summary": "Recently, the powerful generalization ability exhibited by foundation models\nhas brought forth new solutions for zero-shot anomaly segmentation tasks.\nHowever, guiding these foundation models correctly to address downstream tasks\nremains a challenge. This paper proposes a novel two-stage framework, for\nzero-shot anomaly segmentation tasks in industrial anomaly detection. This\nframework excellently leverages the powerful anomaly localization capability of\nCLIP and the boundary perception ability of SAM.(1) To mitigate SAM's\ninclination towards object segmentation, we propose the Co-Feature Point Prompt\nGeneration (PPG) module. This module collaboratively utilizes CLIP and SAM to\ngenerate positive and negative point prompts, guiding SAM to focus on\nsegmenting anomalous regions rather than the entire object. (2) To further\noptimize SAM's segmentation results and mitigate rough boundaries and isolated\nnoise, we introduce the Cascaded Prompts for SAM (CPS) module. This module\nemploys hybrid prompts cascaded with a lightweight decoder of SAM, achieving\nprecise segmentation of anomalous regions. Across multiple datasets, consistent\nexperimental validation demonstrates that our approach achieves\nstate-of-the-art zero-shot anomaly segmentation results. Particularly\nnoteworthy is our performance on the Visa dataset, where we outperform the\nstate-of-the-art methods by 10.3\\% and 7.7\\% in terms of {$F_1$-max} and AP\nmetrics, respectively.",
            "headline_zh": "提出CLIP-SAM协作框架以解决工业零样本异常分割问题",
            "intro_zh": [
                "核心问题：基础模型在零样本异常分割中难以正确引导，导致对象分割倾向和边界粗糙。",
                "方法要点：设计两阶段框架，包括PPG模块生成点提示和CPS模块优化分割结果。",
                "实验或效果：在多个数据集上实现SOTA，Visa数据集F1-max和AP指标分别提升10.3%和7.7%。"
            ],
            "tags_zh": [
                "零样本异常检测",
                "异常分割",
                "CLIP模型",
                "SAM模型",
                "提示工程",
                "工业视觉"
            ],
            "_index": 107
        },
        {
            "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
            "authors": [
                "Ganlin Yang",
                "Tianyi Zhang",
                "Haoran Hao",
                "Weiyun Wang",
                "Yibin Liu",
                "Dehui Wang",
                "Guanzhou Chen",
                "Zijian Cai",
                "Junting Chen",
                "Weijie Su",
                "Wengang Zhou",
                "Yu Qiao",
                "Jifeng Dai",
                "Jiangmiao Pang",
                "Gen Luo",
                "Wenhai Wang",
                "Yao Mu",
                "Zhi Hou"
            ],
            "arxiv_id": "2510.11027v1",
            "summary": "While significant research has focused on developing embodied reasoning\ncapabilities using Vision-Language Models (VLMs) or integrating advanced VLMs\ninto Vision-Language-Action (VLA) models for end-to-end robot control, few\nstudies directly address the critical gap between upstream VLM-based reasoning\nand downstream VLA policy learning. In this work, we take an initial step\ntoward bridging embodied reasoning with VLA policy learning by introducing\nVlaser - a Vision-Language-Action Model with synergistic embodied reasoning\ncapability, which is a foundational vision-language model designed to integrate\nhigh-level reasoning with low-level control for embodied agents. Built upon the\nhigh-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance\nacross a range of embodied reasoning benchmarks - including spatial reasoning,\nembodied grounding, embodied QA, and task planning. Furthermore, we\nsystematically examine how different VLM initializations affect supervised VLA\nfine-tuning, offering novel insights into mitigating the domain shift between\ninternet-scale pre-training data and embodied-specific policy learning data.\nBased on these insights, our approach achieves state-of-the-art results on the\nWidowX benchmark and competitive performance on the Google Robot benchmark.",
            "headline_zh": "提出Vlaser模型以弥合视觉语言推理与具身策略学习间的差距",
            "intro_zh": [
                "核心问题：现有研究未直接解决上游视觉语言模型推理与下游视觉语言动作策略学习间的关键差距。",
                "方法要点：构建Vlaser模型，集成高层次推理与低层次控制，基于Vlaser-6M数据集实现协同具身推理。",
                "实验或效果：在多个具身推理基准上达到最优性能，并在WidowX和Google Robot基准上取得领先或竞争性结果。"
            ],
            "tags_zh": [
                "视觉语言动作模型",
                "具身推理",
                "策略学习",
                "数据集构建",
                "基准测试",
                "领域迁移"
            ],
            "_index": 108
        },
        {
            "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
            "authors": [
                "Hongxiang Li",
                "Yaowei Li",
                "Bin Lin",
                "Yuwei Niu",
                "Yuhang Yang",
                "Xiaoshuang Huang",
                "Jiayin Cai",
                "Xiaolong Jiang",
                "Yao Hu",
                "Long Chen"
            ],
            "arxiv_id": "2510.11026v1",
            "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\n\\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.",
            "headline_zh": "提出GIR-Bench基准以评估统一多模态模型在推理驱动图像生成中的能力",
            "intro_zh": [
                "核心问题：缺乏系统基准评估多模态模型理解与生成的一致性和泛化能力",
                "方法要点：设计三个子集评估理解-生成一致性、推理文本到图像生成和多步编辑",
                "实验或效果：统一模型在推理任务中表现更好，但理解与生成间仍存在差距"
            ],
            "tags_zh": [
                "多模态基准",
                "图像生成",
                "推理评估",
                "理解-生成一致性",
                "文本到图像生成",
                "多步编辑"
            ],
            "_index": 109
        },
        {
            "title": "GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation",
            "authors": [
                "Shasha Guo",
                "Liang Pang",
                "Xi Wang",
                "Yanling Wang",
                "Huawei Shen",
                "Jing Zhang"
            ],
            "arxiv_id": "2510.11020v1",
            "summary": "Auxiliary lines are essential for solving complex geometric problems but\nremain challenging for large vision-language models (LVLMs). Rather than\nediting diagrams to draw auxiliary lines, which current image editing models\nstruggle to render with geometric precision, we generate textual descriptions\nof auxiliary-line constructions to better align with the representational\nstrengths of LVLMs. To bridge the gap between textual descriptions and spatial\nstructure, we propose a reinforcement learning framework that enhances\ndiagram-text alignment. At the core of our approach is a cross-modal reward\nthat evaluates how well the generated auxiliary-line description for an\noriginal diagram matches a ground-truth auxiliary-line diagram. Built on this\nreward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line\nreasoning in solid geometry. This fine-grained signal drives a GRPO-based RL\nstage, yielding precise diagram-text alignment. To support training, we develop\na scalable data creation pipeline and construct AuxSolidMath, a dataset of\n3,018 real-exam geometry problems with paired diagrams and aligned textual\nfields. At the 3B and 7B scales, GeoVLMath achieves competitive and often\nsuperior performance compared with strong open-source and proprietary LVLMs on\nauxiliary-line reasoning benchmarks.",
            "headline_zh": "提出GeoVLMath以通过跨模态奖励增强视觉语言模型在几何辅助线推理中的性能",
            "intro_zh": [
                "核心问题：大型视觉语言模型在几何问题中绘制辅助线时面临几何精度不足的挑战",
                "方法要点：使用强化学习框架和跨模态奖励来优化辅助线文本描述与图表的对齐",
                "实验或效果：在3B和7B规模上，GeoVLMath在辅助线推理基准上表现优于开源和专有模型"
            ],
            "tags_zh": [
                "几何推理",
                "视觉语言模型",
                "强化学习",
                "跨模态对齐",
                "辅助线生成",
                "数据集构建"
            ],
            "_index": 110
        },
        {
            "title": "Refinery: Active Fine-tuning and Deployment-time Optimization for Contact-Rich Policies",
            "authors": [
                "Bingjie Tang",
                "Iretiayo Akinola",
                "Jie Xu",
                "Bowen Wen",
                "Dieter Fox",
                "Gaurav S. Sukhatme",
                "Fabio Ramos",
                "Abhishek Gupta",
                "Yashraj Narang"
            ],
            "arxiv_id": "2510.11019v1",
            "summary": "Simulation-based learning has enabled policies for precise, contact-rich\ntasks (e.g., robotic assembly) to reach high success rates (~80%) under high\nlevels of observation noise and control error. Although such performance may be\nsufficient for research applications, it falls short of industry standards and\nmakes policy chaining exceptionally brittle. A key limitation is the high\nvariance in individual policy performance across diverse initial conditions. We\nintroduce Refinery, an effective framework that bridges this performance gap,\nrobustifying policy performance across initial conditions. We propose Bayesian\nOptimization-guided fine-tuning to improve individual policies, and Gaussian\nMixture Model-based sampling during deployment to select initializations that\nmaximize execution success. Using Refinery, we improve mean success rates by\n10.98% over state-of-the-art methods in simulation-based learning for robotic\nassembly, reaching 91.51% in simulation and comparable performance in the real\nworld. Furthermore, we demonstrate that these fine-tuned policies can be\nchained to accomplish long-horizon, multi-part\nassembly$\\unicode{x2013}$successfully assembling up to 8 parts without\nrequiring explicit multi-step training.",
            "headline_zh": "提出Refinery框架以优化接触丰富策略的微调与部署",
            "intro_zh": [
                "核心问题：模拟学习策略在多样化初始条件下性能方差高，影响工业应用",
                "方法要点：结合贝叶斯优化微调策略和高斯混合模型采样部署初始化",
                "实验或效果：仿真成功率提升至91.51%，支持多部件链式组装"
            ],
            "tags_zh": [
                "机器人装配",
                "模拟学习",
                "贝叶斯优化",
                "高斯混合模型",
                "策略微调",
                "部署优化"
            ],
            "_index": 111
        },
        {
            "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness",
            "authors": [
                "Pranav Ramesh",
                "Arjun Roy",
                "Deepak Ravikumar",
                "Kaushik Roy",
                "Gopalakrishnan Srinivasan"
            ],
            "arxiv_id": "2510.11018v1",
            "summary": "Designing adversarially robust models from a data-centric perspective\nrequires understanding which input samples are most crucial for learning\nresilient features. While coreset selection provides a mechanism for efficient\ntraining on data subsets, current algorithms are designed for clean accuracy\nand fall short in preserving robustness. To address this, we propose a\nframework linking a sample's adversarial vulnerability to its\n\\textit{hardness}, which we quantify using the average input gradient norm\n(AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN)\nare less vulnerable and occupy regions further from the decision boundary.\nLeveraging this insight, we present EasyCore, a coreset selection algorithm\nthat retains only the samples with low AIGN for training. We empirically show\nthat models trained on EasyCore-selected data achieve significantly higher\nadversarial accuracy than those trained with competing coreset methods under\nboth standard and adversarial training. As AIGN is a model-agnostic dataset\nproperty, EasyCore is an efficient and widely applicable data-centric method\nfor improving adversarial robustness. We show that EasyCore achieves up to 7\\%\nand 5\\% improvement in adversarial accuracy under standard training and TRADES\nadversarial training, respectively, compared to existing coreset methods.",
            "headline_zh": "提出EasyCore核心集选择方法，通过样本硬度提升对抗鲁棒性",
            "intro_zh": [
                "核心问题：现有核心集选择方法注重干净准确率，但无法保持对抗鲁棒性",
                "方法要点：基于平均输入梯度范数量化样本硬度，选择低硬度样本构建核心集",
                "实验效果：在标准与对抗训练下，对抗准确率比现有方法提升高达7%和5%"
            ],
            "tags_zh": [
                "核心集选择",
                "对抗鲁棒性",
                "样本硬度",
                "数据为中心方法",
                "平均输入梯度范数"
            ],
            "_index": 112
        },
        {
            "title": "High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation",
            "authors": [
                "Runyang Feng",
                "Hyung Jin Chang",
                "Tze Ho Elden Tse",
                "Boeun Kim",
                "Yi Chang",
                "Yixing Gao"
            ],
            "arxiv_id": "2510.11017v1",
            "summary": "Modeling high-resolution spatiotemporal representations, including both\nglobal dynamic contexts (e.g., holistic human motion tendencies) and local\nmotion details (e.g., high-frequency changes of keypoints), is essential for\nvideo-based human pose estimation (VHPE). Current state-of-the-art methods\ntypically unify spatiotemporal learning within a single type of modeling\nstructure (convolution or attention-based blocks), which inherently have\ndifficulties in balancing global and local dynamic modeling and may bias the\nnetwork to one of them, leading to suboptimal performance. Moreover, existing\nVHPE models suffer from quadratic complexity when capturing global\ndependencies, limiting their applicability especially for high-resolution\nsequences. Recently, the state space models (known as Mamba) have demonstrated\nsignificant potential in modeling long-range contexts with linear complexity;\nhowever, they are restricted to 1D sequential data. In this paper, we present a\nnovel framework that extends Mamba from two aspects to separately learn global\nand local high-resolution spatiotemporal representations for VHPE.\nSpecifically, we first propose a Global Spatiotemporal Mamba, which performs 6D\nselective space-time scan and spatial- and temporal-modulated scan merging to\nefficiently extract global representations from high-resolution sequences. We\nfurther introduce a windowed space-time scan-based Local Refinement Mamba to\nenhance the high-frequency details of localized keypoint motions. Extensive\nexperiments on four benchmark datasets demonstrate that the proposed model\noutperforms state-of-the-art VHPE approaches while achieving better\ncomputational trade-offs.",
            "headline_zh": "提出全局-局部状态空间模型以解决视频人体姿态估计中的高分辨率时空建模问题",
            "intro_zh": [
                "现有方法难以平衡全局与局部动态建模，且全局依赖捕获复杂度高",
                "扩展Mamba模型，分别设计全局和局部模块以高效提取时空表示",
                "在多个基准数据集上优于现有方法，并实现更好的计算权衡"
            ],
            "tags_zh": [
                "视频人体姿态估计",
                "状态空间模型",
                "高分辨率时空建模",
                "全局-局部动态建模",
                "线性复杂度",
                "Mamba扩展"
            ],
            "_index": 113
        },
        {
            "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces",
            "authors": [
                "Subhransu S. Bhattacharjee",
                "Hao Lu",
                "Dylan Campbell",
                "Rahul Shome"
            ],
            "arxiv_id": "2510.11014v1",
            "summary": "Priors are vital for planning under partial observability, yet difficult to\nobtain in practice. We present a sampling-based pipeline that leverages\nlarge-scale pretrained generative models to produce probabilistic priors\ncapturing environmental uncertainty and spatio-semantic relationships in a\nzero-shot manner. Conditioned on partial observations, the pipeline recovers\ncomplete RGB-D point cloud samples with occupancy and target semantics,\nformulated to be directly useful in configuration-space planning. We establish\na Matterport3D benchmark of rooms partially visible through doorways, where a\nrobot must navigate to an unobserved target object. Effective priors for this\nsetting must represent both occupancy and target-location uncertainty in\nunobserved regions. Experiments show that our approach recovers commonsense\nspatial semantics consistent with ground truth, yielding diverse, clean 3D\npoint clouds usable in motion planning, highlight the promise of generative\nmodels as a rich source of priors for robotic planning.",
            "headline_zh": "提出基于生成模型的采样管道，为零样本环境不确定性提供先验，用于配置空间规划。",
            "intro_zh": [
                "核心问题：部分可观测下规划的先验难以获取，需处理环境不确定性和空间语义关系。",
                "方法要点：利用预训练生成模型，从部分观测恢复完整RGB-D点云，包含占用和目标语义。",
                "实验效果：在Matterport3D基准上，生成的点云具有常识空间语义，可用于运动规划。"
            ],
            "tags_zh": [
                "生成模型",
                "配置空间规划",
                "环境不确定性",
                "RGB-D点云",
                "零样本学习"
            ],
            "_index": 114
        },
        {
            "title": "COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models",
            "authors": [
                "Sanchit Sinha",
                "Guangzhi Xiong",
                "Aidong Zhang"
            ],
            "arxiv_id": "2510.11012v1",
            "summary": "Compositional reasoning remains a persistent weakness of modern vision\nlanguage models (VLMs): they often falter when a task hinges on understanding\nhow multiple objects, attributes, and relations interact within an image.\nMultiple research works have attempted to improve compositionality performance\nby creative tricks such as improving prompt structure, chain of thought\nreasoning, etc. A more recent line of work attempts to impart additional\nreasoning in VLMs using well-trained Large Language Models (LLMs), which are\nfar superior in linguistic understanding than VLMs to compensate for the\nlimited linguistic prowess of VLMs. However, these approaches are either\nresource-intensive or do not provide an interpretable reasoning process. In\nthis paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs\nwith carefully designed neurosymbolic concept trees learned from LLMs to\nimprove VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning\nprocess boosts compositionality performance and provides a rationale behind VLM\npredictions. Empirical results on four compositionality benchmarks, Winoground,\nEqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with\nvarying sizes, demonstrate that COCO-Tree significantly improves compositional\ngeneralization by 5-10% over baselines.",
            "headline_zh": "提出COCO-Tree概念树方法以增强视觉语言模型的组合推理能力",
            "intro_zh": [
                "核心问题：视觉语言模型在组合推理中表现不佳，难以理解多对象、属性和关系的交互。",
                "方法要点：使用神经符号概念树从大型语言模型学习，提升推理过程并提供解释性。",
                "实验或效果：在多个基准测试中，组合泛化性能提升5-10%，优于基线方法。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "组合推理",
                "概念树",
                "神经符号方法",
                "基准测试",
                "解释性推理"
            ],
            "_index": 115
        },
        {
            "title": "Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation",
            "authors": [
                "Kai Han",
                "Siqi Ma",
                "Chengxuan Qian",
                "Jun Chen",
                "Chongwen Lyu",
                "Yuqing Song",
                "Zhe Liu"
            ],
            "arxiv_id": "2510.11005v1",
            "summary": "Accurate segmentation of tumors and adjacent normal tissues in medical images\nis essential for surgical planning and tumor staging. Although foundation\nmodels generally perform well in segmentation tasks, they often struggle to\nfocus on foreground areas in complex, low-contrast backgrounds, where some\nmalignant tumors closely resemble normal organs, complicating contextual\ndifferentiation. To address these challenges, we propose the Foreground-Aware\nSpectrum Segmentation (FASS) framework. First, we introduce a foreground-aware\nmodule to amplify the distinction between background and the entire volume\nspace, allowing the model to concentrate more effectively on target areas.\nNext, a feature-level frequency enhancement module, based on wavelet transform,\nextracts discriminative high-frequency features to enhance boundary recognition\nand detail perception. Eventually, we introduce an edge constraint module to\npreserve geometric continuity in segmentation boundaries. Extensive experiments\non multiple medical datasets demonstrate superior performance across all\nmetrics, validating the effectiveness of our framework, particularly in\nrobustness under complex conditions and fine structure recognition. Our\nframework significantly enhances segmentation of low-contrast images, paving\nthe way for applications in more diverse and complex medical imaging scenarios.",
            "headline_zh": "提出FASS框架以解决腹部医学图像中低对比度肿瘤分割难题",
            "intro_zh": [
                "核心问题：基础模型在复杂低对比度背景下难以区分肿瘤与正常组织",
                "方法要点：结合前景感知、小波变换频率增强和边缘约束模块",
                "实验或效果：多数据集验证，在复杂条件下鲁棒性和细节识别表现优异"
            ],
            "tags_zh": [
                "医学图像分割",
                "频率域分析",
                "小波变换",
                "前景感知",
                "边界约束"
            ],
            "_index": 116
        },
        {
            "title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation",
            "authors": [
                "Ruihang Xu",
                "Dewei Zhou",
                "Fan Ma",
                "Yi Yang"
            ],
            "arxiv_id": "2510.11000v1",
            "summary": "Multi-instance image generation (MIG) remains a significant challenge for\nmodern diffusion models due to key limitations in achieving precise control\nover object layout and preserving the identity of multiple distinct subjects.\nTo address these limitations, we introduce ContextGen, a novel Diffusion\nTransformer framework for multi-instance generation that is guided by both\nlayout and reference images. Our approach integrates two key technical\ncontributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates\nthe composite layout image into the generation context to robustly anchor the\nobjects in their desired positions, and Identity Consistency Attention (ICA),\nan innovative attention mechanism that leverages contextual reference images to\nensure the identity consistency of multiple instances. Recognizing the lack of\nlarge-scale, hierarchically-structured datasets for this task, we introduce\nIMIG-100K, the first dataset with detailed layout and identity annotations.\nExtensive experiments demonstrate that ContextGen sets a new state-of-the-art,\noutperforming existing methods in control precision, identity fidelity, and\noverall visual quality.",
            "headline_zh": "提出ContextGen框架以解决多实例图像生成中的布局控制和身份一致性问题",
            "intro_zh": [
                "核心问题：扩散模型在多实例生成中难以精确控制对象布局和保持多个不同主体的身份一致性",
                "方法要点：引入Contextual Layout Anchoring机制和Identity Consistency Attention机制，结合布局和参考图像指导生成",
                "实验或效果：在控制精度、身份保真度和视觉质量上优于现有方法，并构建了IMIG-100K数据集"
            ],
            "tags_zh": [
                "多实例图像生成",
                "扩散变换器",
                "布局控制",
                "身份一致性",
                "注意力机制",
                "数据集构建"
            ],
            "_index": 117
        },
        {
            "title": "Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency",
            "authors": [
                "Yuxin Cheng",
                "Binxiao Huang",
                "Taiqiang Wu",
                "Wenyong Zhou",
                "Chenchen Ding",
                "Zhengwu Liu",
                "Graziano Chesi",
                "Ngai Wong"
            ],
            "arxiv_id": "2510.10993v1",
            "summary": "3D Gaussian inpainting, a critical technique for numerous applications in\nvirtual reality and multimedia, has made significant progress with pretrained\ndiffusion models. However, ensuring multi-view consistency, an essential\nrequirement for high-quality inpainting, remains a key challenge. In this work,\nwe present PAInpainter, a novel approach designed to advance 3D Gaussian\ninpainting by leveraging perspective-aware content propagation and consistency\nverification across multi-view inpainted images. Our method iteratively refines\ninpainting and optimizes the 3D Gaussian representation with multiple views\nadaptively sampled from a perspective graph. By propagating inpainted images as\nprior information and verifying consistency across neighboring views,\nPAInpainter substantially enhances global consistency and texture fidelity in\nrestored 3D scenes. Extensive experiments demonstrate the superiority of\nPAInpainter over existing methods. Our approach achieves superior 3D inpainting\nquality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and\nNeRFiller datasets, respectively, highlighting its effectiveness and\ngeneralization capability.",
            "headline_zh": "提出PAInpainter方法以解决3D高斯修复中的多视角一致性问题",
            "intro_zh": [
                "核心问题：现有3D高斯修复方法难以确保多视角一致性，影响修复质量。",
                "方法要点：利用视角感知内容传播和一致性验证，迭代优化3D高斯表示。",
                "实验效果：在SPIn-NeRF和NeRFiller数据集上PSNR达26.03dB和29.51dB，优于现有方法。"
            ],
            "tags_zh": [
                "3D高斯修复",
                "多视角一致性",
                "视角感知传播",
                "扩散模型",
                "虚拟现实",
                "图像修复"
            ],
            "_index": 118
        },
        {
            "title": "A Survey on Agentic Multimodal Large Language Models",
            "authors": [
                "Huanjin Yao",
                "Ruifei Zhang",
                "Jiaxing Huang",
                "Jingyi Zhang",
                "Yibo Wang",
                "Bo Fang",
                "Ruolin Zhu",
                "Yongcheng Jing",
                "Shunyu Liu",
                "Guanbin Li",
                "Dacheng Tao"
            ],
            "arxiv_id": "2510.10991v1",
            "summary": "With the recent emergence of revolutionary autonomous agentic systems,\nresearch community is witnessing a significant shift from traditional static,\npassive, and domain-specific AI agents toward more dynamic, proactive, and\ngeneralizable agentic AI. Motivated by the growing interest in agentic AI and\nits potential trajectory toward AGI, we present a comprehensive survey on\nAgentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we\nexplore the emerging paradigm of agentic MLLMs, delineating their conceptual\nfoundations and distinguishing characteristics from conventional MLLM-based\nagents. We establish a conceptual framework that organizes agentic MLLMs along\nthree fundamental dimensions: (i) Agentic internal intelligence functions as\nthe system's commander, enabling accurate long-horizon planning through\nreasoning, reflection, and memory; (ii) Agentic external tool invocation,\nwhereby models proactively use various external tools to extend their\nproblem-solving capabilities beyond their intrinsic knowledge; and (iii)\nAgentic environment interaction further situates models within virtual or\nphysical environments, allowing them to take actions, adapt strategies, and\nsustain goal-directed behavior in dynamic real-world scenarios. To further\naccelerate research in this area for the community, we compile open-source\ntraining frameworks, training and evaluation datasets for developing agentic\nMLLMs. Finally, we review the downstream applications of agentic MLLMs and\noutline future research directions for this rapidly evolving field. To\ncontinuously track developments in this rapidly evolving field, we will also\nactively update a public repository at\nhttps://github.com/HJYao00/Awesome-Agentic-MLLMs.",
            "headline_zh": "提出代理多模态大语言模型框架，以增强AI在动态环境中的自主问题解决能力",
            "intro_zh": [
                "核心问题：传统AI代理静态被动，需转向动态主动的通用代理AI。",
                "方法要点：建立三维框架，涵盖内部智能、外部工具调用和环境交互。",
                "实验或效果：编译开源框架与数据集，推动代理MLLM研究与应用。"
            ],
            "tags_zh": [
                "代理多模态大语言模型",
                "自主智能系统",
                "工具调用",
                "环境交互",
                "开源框架",
                "AGI研究"
            ],
            "_index": 119
        },
        {
            "title": "Mixup Helps Understanding Multimodal Video Better",
            "authors": [
                "Xiaoyu Ma",
                "Ding Ding",
                "Hao Chen"
            ],
            "arxiv_id": "2510.10986v1",
            "summary": "Multimodal video understanding plays a crucial role in tasks such as action\nrecognition and emotion classification by combining information from different\nmodalities. However, multimodal models are prone to overfitting strong\nmodalities, which can dominate learning and suppress the contributions of\nweaker ones. To address this challenge, we first propose Multimodal Mixup (MM),\nwhich applies the Mixup strategy at the aggregated multimodal feature level to\nmitigate overfitting by generating virtual feature-label pairs. While MM\neffectively improves generalization, it treats all modalities uniformly and\ndoes not account for modality imbalance during training. Building on MM, we\nfurther introduce Balanced Multimodal Mixup (B-MM), which dynamically adjusts\nthe mixing ratios for each modality based on their relative contributions to\nthe learning objective. Extensive experiments on several datasets demonstrate\nthe effectiveness of our methods in improving generalization and multimodal\nrobustness.",
            "headline_zh": "提出平衡多模态混合方法以解决多模态视频理解中的模态不平衡问题",
            "intro_zh": [
                "核心问题：多模态模型易过拟合强模态，抑制弱模态贡献。",
                "方法要点：先引入多模态混合，再基于模态贡献动态调整混合比例。",
                "实验或效果：在多个数据集上验证方法提升泛化性和鲁棒性。"
            ],
            "tags_zh": [
                "多模态视频理解",
                "模态不平衡",
                "混合策略",
                "泛化性提升",
                "鲁棒性增强"
            ],
            "_index": 120
        },
        {
            "title": "On the Optimal Representation Efficiency of Barlow Twins: An Information-Geometric Interpretation",
            "authors": [
                "Di Zhang"
            ],
            "arxiv_id": "2510.10980v1",
            "summary": "Self-supervised learning (SSL) has achieved remarkable success by learning\nmeaningful representations without labeled data. However, a unified theoretical\nframework for understanding and comparing the efficiency of different SSL\nparadigms remains elusive. In this paper, we introduce a novel\ninformation-geometric framework to quantify representation efficiency. We\ndefine representation efficiency $\\eta$ as the ratio between the effective\nintrinsic dimension of the learned representation space and its ambient\ndimension, where the effective dimension is derived from the spectral\nproperties of the Fisher Information Matrix (FIM) on the statistical manifold\ninduced by the encoder. Within this framework, we present a theoretical\nanalysis of the Barlow Twins method. Under specific but natural assumptions, we\nprove that Barlow Twins achieves optimal representation efficiency ($\\eta = 1$)\nby driving the cross-correlation matrix of representations towards the identity\nmatrix, which in turn induces an isotropic FIM. This work provides a rigorous\ntheoretical foundation for understanding the effectiveness of Barlow Twins and\noffers a new geometric perspective for analyzing SSL algorithms.",
            "headline_zh": "提出信息几何框架分析Barlow Twins自监督学习，证明其表示效率最优",
            "intro_zh": [
                "核心问题：缺乏统一理论框架比较自监督学习方法的表示效率",
                "方法要点：定义表示效率比，基于Fisher信息矩阵谱性质推导有效维度",
                "实验或效果：证明Barlow Twins在特定假设下实现最优表示效率"
            ],
            "tags_zh": [
                "自监督学习",
                "表示效率",
                "信息几何",
                "Barlow Twins",
                "Fisher信息矩阵"
            ],
            "_index": 121
        },
        {
            "title": "AMO-HEAD: Adaptive MARG-Only Heading Estimation for UAVs under Magnetic Disturbances",
            "authors": [
                "Qizhi Guo",
                "Siyuan Yang",
                "Junning Lyu",
                "Jianjun Sun",
                "Defu Lin",
                "Shaoming He"
            ],
            "arxiv_id": "2510.10979v1",
            "summary": "Accurate and robust heading estimation is crucial for unmanned aerial\nvehicles (UAVs) when conducting indoor inspection tasks. However, the cluttered\nnature of indoor environments often introduces severe magnetic disturbances,\nwhich can significantly degrade heading accuracy. To address this challenge,\nthis paper presents an Adaptive MARG-Only Heading (AMO-HEAD) estimation\napproach for UAVs operating in magnetically disturbed environments. AMO-HEAD is\na lightweight and computationally efficient Extended Kalman Filter (EKF)\nframework that leverages inertial and magnetic sensors to achieve reliable\nheading estimation. In the proposed approach, gyroscope angular rate\nmeasurements are integrated to propagate the quaternion state, which is\nsubsequently corrected using accelerometer and magnetometer data. The corrected\nquaternion is then used to compute the UAV's heading. An adaptive process noise\ncovariance method is introduced to model and compensate for gyroscope\nmeasurement noise, bias drift, and discretization errors arising from the Euler\nmethod integration. To mitigate the effects of external magnetic disturbances,\na scaling factor is applied based on real-time magnetic deviation detection. A\ntheoretical observability analysis of the proposed AMO-HEAD is performed using\nthe Lie derivative. Extensive experiments were conducted in real world indoor\nenvironments with customized UAV platforms. The results demonstrate the\neffectiveness of the proposed algorithm in providing precise heading estimation\nunder magnetically disturbed conditions.",
            "headline_zh": "提出自适应MARG航向估计方法，解决无人机在磁干扰环境下的航向精度问题",
            "intro_zh": [
                "室内环境磁干扰严重，导致无人机航向估计精度下降",
                "基于EKF框架，集成陀螺仪、加速度计和磁力计数据，自适应补偿噪声和磁干扰",
                "真实环境实验验证，在磁干扰条件下提供精确航向估计"
            ],
            "tags_zh": [
                "航向估计",
                "扩展卡尔曼滤波",
                "磁干扰补偿",
                "无人机导航",
                "自适应算法"
            ],
            "_index": 122
        },
        {
            "title": "RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model",
            "authors": [
                "Mingtong Dai",
                "Lingbo Liu",
                "Yongjie Bai",
                "Yang Liu",
                "Zhouxia Wang",
                "Rui SU",
                "Chunjie Chen",
                "Liang Lin",
                "Xinyu Wu"
            ],
            "arxiv_id": "2510.10975v1",
            "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs.We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.",
            "headline_zh": "提出RoVer框架，通过测试时验证增强视觉-语言-动作模型性能",
            "intro_zh": [
                "核心问题：VLA模型性能提升依赖数据与模型规模扩展，成本高昂且受限。",
                "方法要点：使用机器人过程奖励模型验证候选动作，并预测动作空间方向进行扩展。",
                "实验或效果：通过共享感知特征缓存，在相同计算预算下评估更多候选动作。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "测试时缩放",
                "过程奖励模型",
                "动作空间方向",
                "共享感知缓存",
                "机器人决策"
            ],
            "_index": 123
        },
        {
            "title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning",
            "authors": [
                "Sanchit Sinha",
                "Oana Frunza",
                "Kashif Rasul",
                "Yuriy Nevmyvaka",
                "Aidong Zhang"
            ],
            "arxiv_id": "2510.10973v1",
            "summary": "The capabilities of Large Vision-Language Models (LVLMs) have reached\nstate-of-the-art on many visual reasoning tasks, including chart reasoning, yet\nthey still falter on out-of-distribution (OOD) data, and degrade further when\nasked to produce their chain-of-thought (CoT) rationales, limiting\nexplainability. We present Chart-RVR, a general framework that fine-tunes LVLMs\nto be more robust and explainable for chart reasoning by coupling Group\nRelative Policy Optimization (GRPO) with automatically verifiable rewards. Our\nframework comprises of three rewards that maximize: (i) correct chart-type\nclassification, (ii) faithful chart table reconstruction, and (iii) process\nconformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently\noutperforms standard supervised fine-tuning (SFT) on both in-distribution and\nout-of-distribution datasets, closing the OOD performance gap while improving\nrationale fidelity. The resulting models, the Chart-RVR-3B series, achieve\nstate-of-the-art results on six chart-reasoning benchmarks spanning in-domain\nand OOD settings, surpassing all existing models of comparable size. Beyond\naccuracy, Chart-RVR yields more interpretable CoT rationales, strengthening\ntrust and reliability - showcasing the power of verifiable rewards with GRPO\nfor training reliable, interpretable chart-reasoning models.",
            "headline_zh": "提出Chart-RVR框架，通过可验证奖励增强大视觉语言模型在图表推理中的鲁棒性和可解释性。",
            "intro_zh": [
                "核心问题：大视觉语言模型在分布外数据和生成可解释推理链时性能下降。",
                "方法要点：结合群组相对策略优化与自动可验证奖励，优化图表分类、表格重建和过程一致性。",
                "实验或效果：在多个基准测试中超越标准微调，缩小分布外性能差距并提高推理链忠实度。"
            ],
            "tags_zh": [
                "图表推理",
                "强化学习",
                "可验证奖励",
                "大视觉语言模型",
                "可解释性",
                "分布外鲁棒性"
            ],
            "_index": 124
        },
        {
            "title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation",
            "authors": [
                "Zeteng Lin",
                "Xingxing Li",
                "Wen You",
                "Xiaoyang Li",
                "Zehan Lu",
                "Yujun Cai",
                "Jing Tang"
            ],
            "arxiv_id": "2510.10969v1",
            "summary": "Existing vision language models (VLMs), including GPT-4 and DALL-E, often\nstruggle to preserve logic, object identity, and style in multimodal image-text\ngeneration. This limitation significantly hinders the generalization capability\nof VLMs in complex image-text input-output scenarios. To address this issue, we\npropose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which\nenhances existing interleaved VLMs through explicit structured reasoning,\nthereby mitigating context drift in logic, entity identity, and style. The\nproposed framework operates in two stages. (1) A dynamic IUT-Plug extraction\nmodule parses visual scenes into hierarchical symbolic structures. (2) A\ncoordinated narrative-flow and image synthesis mechanism ensures cross-modal\nconsistency. To evaluate our approach, we construct a novel benchmark based on\n3,000 real human-generated question-answer pairs over fine-tuned large models,\nintroducing a dynamic evaluation protocol for quantifying context drift in\ninterleaved VLMs. Experimental results demonstrate that IUT-Plug not only\nimproves accuracy on established benchmarks but also effectively alleviates the\nthree critical forms of context drift across diverse multimodal question\nanswering (QA) scenarios.",
            "headline_zh": "提出IUT-Plug模块以解决多模态图像-文本生成中的逻辑、实体和风格漂移问题",
            "intro_zh": [
                "现有视觉语言模型在复杂图像-文本场景中难以保持逻辑、对象身份和风格一致性",
                "基于图像理解树构建动态提取和协调机制，增强结构化推理和跨模态一致性",
                "实验显示在多样化多模态问答场景中有效缓解上下文漂移并提升准确性"
            ],
            "tags_zh": [
                "图像理解树",
                "多模态生成",
                "上下文漂移",
                "结构化推理",
                "跨模态一致性",
                "问答基准"
            ],
            "_index": 125
        },
        {
            "title": "Game-Theoretic Risk-Shaped Reinforcement Learning for Safe Autonomous Driving",
            "authors": [
                "Dong Hu",
                "Fenqing Hu",
                "Lidong Yang",
                "Chao Huang"
            ],
            "arxiv_id": "2510.10960v1",
            "summary": "Ensuring safety in autonomous driving (AD) remains a significant challenge,\nespecially in highly dynamic and complex traffic environments where diverse\nagents interact and unexpected hazards frequently emerge. Traditional\nreinforcement learning (RL) methods often struggle to balance safety,\nefficiency, and adaptability, as they primarily focus on reward maximization\nwithout explicitly modeling risk or safety constraints. To address these\nlimitations, this study proposes a novel game-theoretic risk-shaped RL (GTR2L)\nframework for safe AD. GTR2L incorporates a multi-level game-theoretic world\nmodel that jointly predicts the interactive behaviors of surrounding vehicles\nand their associated risks, along with an adaptive rollout horizon that adjusts\ndynamically based on predictive uncertainty. Furthermore, an uncertainty-aware\nbarrier mechanism enables flexible modulation of safety boundaries. A dedicated\nrisk modeling approach is also proposed, explicitly capturing both epistemic\nand aleatoric uncertainty to guide constrained policy optimization and enhance\ndecision-making in complex environments. Extensive evaluations across diverse\nand safety-critical traffic scenarios show that GTR2L significantly outperforms\nstate-of-the-art baselines, including human drivers, in terms of success rate,\ncollision and violation reduction, and driving efficiency. The code is\navailable at https://github.com/DanielHu197/GTR2L.",
            "headline_zh": "提出博弈论风险塑造强化学习框架以解决自动驾驶安全挑战",
            "intro_zh": [
                "传统强化学习在动态交通中难以平衡安全与效率，缺乏风险建模",
                "结合多级博弈论模型预测交互行为与风险，自适应调整决策视野",
                "实验显示在多种场景下显著提升成功率、减少碰撞和违规"
            ],
            "tags_zh": [
                "自动驾驶安全",
                "强化学习",
                "博弈论",
                "风险建模",
                "不确定性感知",
                "策略优化"
            ],
            "_index": 126
        },
        {
            "title": "Comparative Evaluation of Neural Network Architectures for Generalizable Human Spatial Preference Prediction in Unseen Built Environments",
            "authors": [
                "Maral Doctorarastoo",
                "Katherine A. Flanigan",
                "Mario Bergés",
                "Christopher McComb"
            ],
            "arxiv_id": "2510.10954v1",
            "summary": "The capacity to predict human spatial preferences within built environments\nis instrumental for developing Cyber-Physical-Social Infrastructure Systems\n(CPSIS). A significant challenge in this domain is the generalizability of\npreference models, particularly their efficacy in predicting preferences within\nenvironmental configurations not encountered during training. While deep\nlearning models have shown promise in learning complex spatial and contextual\ndependencies, it remains unclear which neural network architectures are most\neffective at generalizing to unseen layouts. To address this, we conduct a\ncomparative study of Graph Neural Networks, Convolutional Neural Networks, and\nstandard feedforward Neural Networks using synthetic data generated from a\nsimplified and synthetic pocket park environment. Beginning with this\nillustrative case study, allows for controlled analysis of each model's ability\nto transfer learned preference patterns to unseen spatial scenarios. The models\nare evaluated based on their capacity to predict preferences influenced by\nheterogeneous physical, environmental, and social features. Generalizability\nscore is calculated using the area under the precision-recall curve for the\nseen and unseen layouts. This generalizability score is appropriate for\nimbalanced data, providing insights into the suitability of each neural network\narchitecture for preference-aware human behavior modeling in unseen built\nenvironments.",
            "headline_zh": "比较图神经网络、卷积神经网络和前馈神经网络在未见建筑环境中预测人类空间偏好的泛化能力",
            "intro_zh": [
                "核心问题：预测人类空间偏好的模型在未见环境布局中泛化能力不足。",
                "方法要点：使用合成口袋公园数据比较不同神经网络架构的泛化性能。",
                "实验或效果：基于精确率-召回率曲线下面积评估模型在可见和未见布局中的泛化得分。"
            ],
            "tags_zh": [
                "空间偏好预测",
                "神经网络架构比较",
                "泛化能力评估",
                "建筑环境建模",
                "图神经网络",
                "卷积神经网络"
            ],
            "_index": 127
        },
        {
            "title": "Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors",
            "authors": [
                "Namhoon Kim",
                "Sara Fridovich-Keil"
            ],
            "arxiv_id": "2510.10947v1",
            "summary": "Generative models have shown strong potential as data-driven priors for\nsolving inverse problems such as reconstructing medical images from\nundersampled measurements. While these priors improve reconstruction quality\nwith fewer measurements, they risk hallucinating features when test images lie\noutside the training distribution. Existing uncertainty quantification methods\nin this setting (i) require an in-distribution calibration dataset, which may\nnot be available, (ii) provide heuristic rather than statistical estimates, or\n(iii) quantify uncertainty from model capacity or limited measurements rather\nthan distribution shift. We propose an instance-level, calibration-free\nuncertainty indicator that is sensitive to distribution shift, requires no\nknowledge of the training distribution, and incurs no retraining cost. Our key\nhypothesis is that reconstructions of in-distribution images remain stable\nunder random measurement variations, while reconstructions of\nout-of-distribution (OOD) images exhibit greater instability. We use this\nstability as a proxy for detecting distribution shift. Our proposed OOD\nindicator is efficiently computable for any computational imaging inverse\nproblem; we demonstrate it on tomographic reconstruction of MNIST digits, where\na learned proximal network trained only on digit \"0\" is evaluated on all ten\ndigits. Reconstructions of OOD digits show higher variability and\ncorrespondingly higher reconstruction error, validating this indicator. These\nresults suggest a deployment strategy that pairs generative priors with\nlightweight guardrails, enabling aggressive measurement reduction for\nin-distribution cases while automatically warning when priors are applied out\nof distribution.",
            "headline_zh": "提出基于重构稳定性的分布偏移不确定性指标，用于生成先验逆问题",
            "intro_zh": [
                "生成先逆问题中，测试图像分布偏移易致幻觉，现有方法需校准或非统计估计",
                "利用重构稳定性作为代理指标，无需训练分布知识或重训练，检测分布偏移",
                "在MNIST断层重建实验中，OOD数字重构变异性高，验证指标有效性"
            ],
            "tags_zh": [
                "逆问题",
                "生成先验",
                "分布偏移检测",
                "不确定性估计",
                "重构稳定性",
                "计算成像"
            ],
            "_index": 128
        },
        {
            "title": "DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose Estimation of Textureless Objects",
            "authors": [
                "Jiahong Chen",
                "Jinghao Wang",
                "Zi Wang",
                "Ziwen Wang",
                "Banglei Guan",
                "Qifeng Yu"
            ],
            "arxiv_id": "2510.10933v1",
            "summary": "6D pose estimation of textureless objects is valuable for industrial robotic\napplications, yet remains challenging due to the frequent loss of depth\ninformation. Current multi-view methods either rely on depth data or\ninsufficiently exploit multi-view geometric cues, limiting their performance.\nIn this paper, we propose DKPMV, a pipeline that achieves dense keypoint-level\nfusion using only multi-view RGB images as input. We design a three-stage\nprogressive pose optimization strategy that leverages dense multi-view keypoint\ngeometry information. To enable effective dense keypoint fusion, we enhance the\nkeypoint network with attentional aggregation and symmetry-aware training,\nimproving prediction accuracy and resolving ambiguities on symmetric objects.\nExtensive experiments on the ROBI dataset demonstrate that DKPMV outperforms\nstate-of-the-art multi-view RGB approaches and even surpasses the RGB-D methods\nin the majority of cases. The code will be available soon.",
            "headline_zh": "提出DKPMV以解决无纹理物体6D姿态估计中多视角RGB图像融合不足的问题",
            "intro_zh": [
                "核心问题：无纹理物体6D姿态估计常因深度信息缺失而困难，现有多视角方法依赖深度数据或几何线索利用不足。",
                "方法要点：设计三阶段渐进姿态优化策略，通过注意力聚合和对称感知训练增强关键点网络，实现密集关键点融合。",
                "实验或效果：在ROBI数据集上优于多视角RGB方法，多数情况下超越RGB-D方法，代码即将发布。"
            ],
            "tags_zh": [
                "6D姿态估计",
                "多视角融合",
                "密集关键点",
                "无纹理物体",
                "注意力机制",
                "对称感知训练"
            ],
            "_index": 129
        },
        {
            "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models",
            "authors": [
                "Zonghuan Xu",
                "Xiang Zheng",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "arxiv_id": "2510.10932v1",
            "summary": "With the growing deployment of Vision-Language-Action (VLA) models in\nreal-world embodied AI systems, their increasing vulnerability to backdoor\nattacks poses a serious safety threat. A backdoored VLA agent can be covertly\ntriggered by a pre-injected backdoor to execute adversarial actions,\npotentially causing system failures or even physical harm. Although backdoor\nattacks on VLA models have been explored, prior work has focused only on\nuntargeted attacks, leaving the more practically threatening scenario of\ntargeted manipulation unexamined. In this paper, we study targeted backdoor\nattacks on VLA models and introduce TabVLA, a novel framework that enables such\nattacks via black-box fine-tuning. TabVLA explores two deployment-relevant\ninference-time threat models: input-stream editing and in-scene triggering. It\nformulates poisoned data generation as an optimization problem to improve\nattack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal\nthat the vision channel is the principal attack surface: targeted backdoors\nsucceed with minimal poisoning, remain robust across variations in trigger\ndesign, and are degraded only by positional mismatches between fine-tuning and\ninference triggers. We also investigate a potential detection-based defense\nagainst TabVLA, which reconstructs latent visual triggers from the input stream\nto flag activation-conditioned backdoor samples. Our work highlights the\nvulnerability of VLA models to targeted backdoor manipulation and underscores\nthe need for more advanced defenses.",
            "headline_zh": "提出TabVLA框架，针对视觉-语言-动作模型进行目标后门攻击",
            "intro_zh": [
                "核心问题：VLA模型易受目标后门攻击，可能导致系统故障或物理伤害",
                "方法要点：通过黑盒微调优化中毒数据生成，支持输入流编辑和场景内触发",
                "实验或效果：在OpenVLA-7B上验证，视觉通道为主要攻击面，攻击成功率高且鲁棒"
            ],
            "tags_zh": [
                "后门攻击",
                "视觉-语言-动作模型",
                "黑盒微调",
                "中毒数据生成",
                "目标攻击",
                "安全威胁"
            ],
            "_index": 130
        },
        {
            "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
            "authors": [
                "Chunyu Xie",
                "Bin Wang",
                "Fanjing Kong",
                "Jincheng Li",
                "Dawei Liang",
                "Ji Ao",
                "Dawei Leng",
                "Yuhui Yin"
            ],
            "arxiv_id": "2510.10921v1",
            "summary": "Fine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models like CLIP\nperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduce FG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrich fine-grained supervision, including region-text matching and long-caption\nmodeling, alongside multiple discriminative objectives. We further introduce\nthe Textual Intra-modal Contrastive (TIC) loss to better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuring long-caption retrieval and bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthat FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.",
            "headline_zh": "提出FG-CLIP 2模型以解决双语细粒度视觉-语言对齐问题",
            "intro_zh": [
                "核心问题：现有模型在细粒度视觉-语言对齐上表现不足，尤其在非英语场景。",
                "方法要点：采用区域-文本匹配、长字幕建模及TIC损失，提升双语细粒度对齐。",
                "实验或效果：在29个数据集上超越现有方法，实现双语最先进性能。"
            ],
            "tags_zh": [
                "细粒度视觉-语言对齐",
                "双语模型",
                "区域-文本匹配",
                "长字幕建模",
                "TIC损失",
                "多模态基准"
            ],
            "_index": 131
        },
        {
            "title": "DreamMakeup: Face Makeup Customization using Latent Diffusion Models",
            "authors": [
                "Geon Yeong Park",
                "Inhwa Han",
                "Serin Yang",
                "Yeobin Hong",
                "Seongmin Jeong",
                "Heechan Jeon",
                "Myeongjin Goh",
                "Sung Won Yi",
                "Jin Nam",
                "Jong Chul Ye"
            ],
            "arxiv_id": "2510.10918v1",
            "summary": "The exponential growth of the global makeup market has paralleled\nadvancements in virtual makeup simulation technology. Despite the progress led\nby GANs, their application still encounters significant challenges, including\ntraining instability and limited customization capabilities. Addressing these\nchallenges, we introduce DreamMakup - a novel training-free Diffusion model\nbased Makeup Customization method, leveraging the inherent advantages of\ndiffusion models for superior controllability and precise real-image editing.\nDreamMakeup employs early-stopped DDIM inversion to preserve the facial\nstructure and identity while enabling extensive customization through various\nconditioning inputs such as reference images, specific RGB colors, and textual\ndescriptions. Our model demonstrates notable improvements over existing\nGAN-based and recent diffusion-based frameworks - improved customization,\ncolor-matching capabilities, identity preservation and compatibility with\ntextual descriptions or LLMs with affordable computational costs.",
            "headline_zh": "提出DreamMakeup基于潜在扩散模型实现免训练面部化妆定制，提升可控性和编辑精度。",
            "intro_zh": [
                "GAN在虚拟化妆模拟中存在训练不稳定和定制能力有限的问题。",
                "采用早期停止DDIM反演保护面部结构和身份，支持多条件输入如参考图、颜色和文本。",
                "实验显示在定制、颜色匹配、身份保持和文本兼容性方面优于现有方法，计算成本低。"
            ],
            "tags_zh": [
                "面部化妆定制",
                "潜在扩散模型",
                "DDIM反演",
                "多条件输入",
                "身份保持",
                "虚拟化妆模拟"
            ],
            "_index": 132
        },
        {
            "title": "More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks",
            "authors": [
                "Xinyu Shao",
                "Yanzhe Tang",
                "Pengwei Xie",
                "Kaiwen Zhou",
                "Yuzheng Zhuang",
                "Xingyue Quan",
                "Jianye Hao",
                "Long Zeng",
                "Xiu Li"
            ],
            "arxiv_id": "2510.10912v1",
            "summary": "Many language-guided robotic systems rely on collapsing spatial reasoning\ninto discrete points, making them brittle to perceptual noise and semantic\nambiguity. To address this challenge, we propose RoboMAP, a framework that\nrepresents spatial targets as continuous, adaptive affordance heatmaps. This\ndense representation captures the uncertainty in spatial grounding and provides\nricher information for downstream policies, thereby significantly enhancing\ntask success and interpretability. RoboMAP surpasses the previous\nstate-of-the-art on a majority of grounding benchmarks with up to a 50x speed\nimprovement, and achieves an 82\\% success rate in real-world manipulation.\nAcross extensive simulated and physical experiments, it demonstrates robust\nperformance and shows strong zero-shot generalization to navigation. More\ndetails and videos can be found at https://robo-map.github.io.",
            "headline_zh": "提出RoboMAP框架，使用自适应功能热图解决机器人空间接地中的不确定性问题",
            "intro_zh": [
                "核心问题：语言引导机器人系统依赖离散点进行空间推理，易受感知噪声和语义模糊影响",
                "方法要点：采用连续自适应功能热图表示空间目标，捕捉不确定性并提供丰富信息",
                "实验或效果：在基准测试中超越SOTA，速度提升50倍，真实世界操作成功率82%"
            ],
            "tags_zh": [
                "空间接地",
                "功能热图",
                "机器人任务",
                "不确定性建模",
                "零样本泛化"
            ],
            "_index": 133
        },
        {
            "title": "SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model",
            "authors": [
                "Honghui Yuan",
                "Keiji Yanai"
            ],
            "arxiv_id": "2510.10910v1",
            "summary": "With the rapid development of diffusion models, style transfer has made\nremarkable progress. However, flexible and localized style editing for scene\ntext remains an unsolved challenge. Although existing scene text editing\nmethods have achieved text region editing, they are typically limited to\ncontent replacement and simple styles, which lack the ability of free-style\ntransfer. In this paper, we introduce SceneTextStylizer, a novel training-free\ndiffusion-based framework for flexible and high-fidelity style transfer of text\nin scene images. Unlike prior approaches that either perform global style\ntransfer or focus solely on textual content modification, our method enables\nprompt-guided style transformation specifically for text regions, while\npreserving both text readability and stylistic consistency. To achieve this, we\ndesign a feature injection module that leverages diffusion model inversion and\nself-attention to transfer style features effectively. Additionally, a region\ncontrol mechanism is introduced by applying a distance-based changing mask at\neach denoising step, enabling precise spatial control. To further enhance\nvisual quality, we incorporate a style enhancement module based on the Fourier\ntransform to reinforce stylistic richness. Extensive experiments demonstrate\nthat our method achieves superior performance in scene text style\ntransformation, outperforming existing state-of-the-art methods in both visual\nfidelity and text preservation.",
            "headline_zh": "提出SceneTextStylizer框架，实现无训练场景文本风格迁移",
            "intro_zh": [
                "核心问题：现有方法难以对场景文本进行灵活、局部化风格编辑",
                "方法要点：结合扩散模型反演、自注意力和傅里叶变换实现风格特征注入与增强",
                "实验或效果：在视觉保真度和文本可读性上优于现有先进方法"
            ],
            "tags_zh": [
                "场景文本编辑",
                "扩散模型",
                "风格迁移",
                "无训练框架",
                "傅里叶变换"
            ],
            "_index": 134
        },
        {
            "title": "Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey",
            "authors": [
                "Shuanghao Bai",
                "Wenxuan Song",
                "Jiayi Chen",
                "Yuheng Ji",
                "Zhide Zhong",
                "Jin Yang",
                "Han Zhao",
                "Wanqi Zhou",
                "Wei Zhao",
                "Zhe Li",
                "Pengxiang Ding",
                "Cheng Chi",
                "Haoang Li",
                "Chang Xu",
                "Xiaolong Zheng",
                "Donglin Wang",
                "Shanghang Zhang",
                "Badong Chen"
            ],
            "arxiv_id": "2510.10903v1",
            "summary": "Embodied intelligence has witnessed remarkable progress in recent years,\ndriven by advances in computer vision, natural language processing, and the\nrise of large-scale multimodal models. Among its core challenges, robot\nmanipulation stands out as a fundamental yet intricate problem, requiring the\nseamless integration of perception, planning, and control to enable interaction\nwithin diverse and unstructured environments. This survey presents a\ncomprehensive overview of robotic manipulation, encompassing foundational\nbackground, task-organized benchmarks and datasets, and a unified taxonomy of\nexisting methods. We extend the classical division between high-level planning\nand low-level control by broadening high-level planning to include language,\ncode, motion, affordance, and 3D representations, while introducing a new\ntaxonomy of low-level learning-based control grounded in training paradigms\nsuch as input modeling, latent learning, and policy learning. Furthermore, we\nprovide the first dedicated taxonomy of key bottlenecks, focusing on data\ncollection, utilization, and generalization, and conclude with an extensive\nreview of real-world applications. Compared with prior surveys, our work offers\nboth a broader scope and deeper insight, serving as an accessible roadmap for\nnewcomers and a structured reference for experienced researchers. All related\nresources, including research papers, open-source datasets, and projects, are\ncurated for the community at\nhttps://github.com/BaiShuanghao/Awesome-Robotics-Manipulation.",
            "headline_zh": "提出机器人操作统一框架，涵盖规划与控制新分类，以应对非结构化环境挑战。",
            "intro_zh": [
                "核心问题：机器人操作需整合感知、规划与控制，适应多样非结构化环境。",
                "方法要点：扩展高层规划至语言、代码等，引入基于训练范式的低层控制分类。",
                "实验或效果：提供首个关键瓶颈分类，涵盖数据收集、利用与泛化。"
            ],
            "tags_zh": [
                "机器人操作",
                "高层规划",
                "低层控制",
                "数据瓶颈",
                "统一分类",
                "真实世界应用"
            ],
            "_index": 135
        },
        {
            "title": "An Adaptive Transition Framework for Game-Theoretic Based Takeover",
            "authors": [
                "Dikshant Shehmar",
                "Matthew E. Taylor",
                "Ehsan Hashemi"
            ],
            "arxiv_id": "2510.10893v1",
            "summary": "The transition of control from autonomous systems to human drivers is\ncritical in automated driving systems, particularly due to the out-of-the-loop\n(OOTL) circumstances that reduce driver readiness and increase reaction times.\nExisting takeover strategies are based on fixed time-based transitions, which\nfail to account for real-time driver performance variations. This paper\nproposes an adaptive transition strategy that dynamically adjusts the control\nauthority based on both the time and tracking ability of the driver trajectory.\nShared control is modeled as a cooperative differential game, where control\nauthority is modulated through time-varying objective functions instead of\nblending control torques directly. To ensure a more natural takeover, a\ndriver-specific state-tracking matrix is introduced, allowing the transition to\nalign with individual control preferences. Multiple transition strategies are\nevaluated using a cumulative trajectory error metric. Human-in-the-loop control\nscenarios of the standardized ISO lane change maneuvers demonstrate that\nadaptive transitions reduce trajectory deviations and driver control effort\ncompared to conventional strategies. Experiments also confirm that continuously\nadjusting control authority based on real-time deviations enhances vehicle\nstability while reducing driver effort during takeover.",
            "headline_zh": "提出自适应控制权转移策略以解决自动驾驶接管中驾驶员准备不足问题",
            "intro_zh": [
                "核心问题：固定时间转移策略未考虑驾驶员实时性能变化，导致接管反应慢。",
                "方法要点：基于合作微分游戏建模共享控制，动态调整控制权以匹配驾驶员轨迹跟踪。",
                "实验效果：自适应策略减少轨迹偏差和驾驶员控制努力，提升车辆稳定性。"
            ],
            "tags_zh": [
                "自动驾驶接管",
                "共享控制",
                "微分游戏",
                "自适应策略",
                "驾驶员状态跟踪",
                "车辆稳定性"
            ],
            "_index": 136
        },
        {
            "title": "Topological Alignment of Shared Vision-Language Embedding Space",
            "authors": [
                "Junwon You",
                "Dasol Kang",
                "Jae-Hun Jung"
            ],
            "arxiv_id": "2510.10889v1",
            "summary": "Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shot\ncapabilities. However, their cross-modal alignment remains biased toward\nEnglish due to limited multilingual multimodal data. Recent multilingual\nextensions have alleviated this gap but enforce instance-level alignment while\nneglecting the global geometry of the shared embedding space. We address this\nproblem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), a\ntopology-aware framework aligning embedding spaces with topology-preserving\nconstraints. The proposed method applies persistent homology to define a\ntopological alignment loss and approximates persistence diagram with\ntheoretical error bounds using graph sparsification strategy. This work\nvalidates the proposed approach, showing enhanced structural coherence of\nmultilingual representations, higher zero-shot accuracy on the CIFAR-100, and\nstronger multilingual retrieval performance on the xFlickr&CO. Beyond VLMs, the\nproposed approach provides a general method for incorporating topological\nalignment into representation learning.",
            "headline_zh": "提出ToMCLIP框架以解决多语言视觉-语言模型嵌入空间全局几何对齐问题",
            "intro_zh": [
                "核心问题：多语言视觉-语言模型嵌入空间存在英语偏见，且现有方法忽视全局几何结构。",
                "方法要点：引入拓扑对齐损失，使用持久同调和图稀疏化策略保持嵌入空间拓扑。",
                "实验或效果：在CIFAR-100和xFlickr&CO上提升零样本准确率和多语言检索性能。"
            ],
            "tags_zh": [
                "多语言视觉-语言模型",
                "拓扑对齐",
                "持久同调",
                "图稀疏化",
                "零样本学习",
                "跨模态检索"
            ],
            "_index": 137
        },
        {
            "title": "QuayPoints: A Reasoning Framework to Bridge the Information Gap Between Global and Local Planning in Autonomous Racing",
            "authors": [
                "Yashom Dighe",
                "Youngjin Kim",
                "Karthik Dantu"
            ],
            "arxiv_id": "2510.10886v1",
            "summary": "Autonomous racing requires tight integration between perception, planning and\ncontrol to minimize latency as well as timely decision making. A standard\nautonomy pipeline comprising a global planner, local planner, and controller\nloses information as the higher-level racing context is sequentially propagated\ndownstream into specific task-oriented context. In particular, the global\nplanner's understanding of optimality is typically reduced to a sparse set of\nwaypoints, leaving the local planner to make reactive decisions with limited\ncontext. This paper investigates whether additional global insights,\nspecifically time-optimality information, can be meaningfully passed to the\nlocal planner to improve downstream decisions. We introduce a framework that\npreserves essential global knowledge and conveys it to the local planner\nthrough QuayPoints regions where deviations from the optimal raceline result in\nsignificant compromises to optimality. QuayPoints enable local planners to make\nmore informed global decisions when deviating from the raceline, such as during\nstrategic overtaking. To demonstrate this, we integrate QuayPoints into an\nexisting planner and show that it consistently overtakes opponents traveling at\nup to 75% of the ego vehicle's speed across four distinct race tracks.",
            "headline_zh": "提出QuayPoints框架以在自动驾驶赛车中传递全局最优性信息到局部规划器",
            "intro_zh": [
                "核心问题：标准自动驾驶管道中全局规划器的最优性信息在传递到局部规划器时丢失，导致决策受限",
                "方法要点：引入QuayPoints区域，将时间最优性信息从全局规划器传递到局部规划器，辅助偏离最优路径时的决策",
                "实验或效果：集成QuayPoints后，在四个赛道中能稳定超越速度达自车75%的对手"
            ],
            "tags_zh": [
                "自动驾驶赛车",
                "全局规划",
                "局部规划",
                "最优性传递",
                "决策框架",
                "路径规划"
            ],
            "_index": 138
        },
        {
            "title": "Where on Earth? A Vision-Language Benchmark for Probing Model Geolocation Skills Across Scales",
            "authors": [
                "Zhaofang Qian",
                "Hardy Chen",
                "Zeyu Wang",
                "Li Zhang",
                "Zijun Wang",
                "Xiaoke Huang",
                "Hui Liu",
                "Xianfeng Tang",
                "Zeyu Zheng",
                "Haoqin Tu",
                "Cihang Xie",
                "Yuyin Zhou"
            ],
            "arxiv_id": "2510.10880v1",
            "summary": "Vision-language models (VLMs) have advanced rapidly, yet their capacity for\nimage-grounded geolocation in open-world conditions, a task that is challenging\nand of demand in real life, has not been comprehensively evaluated. We present\nEarthWhere, a comprehensive benchmark for VLM image geolocation that evaluates\nvisual recognition, step-by-step reasoning, and evidence use. EarthWhere\ncomprises 810 globally distributed images across two complementary geolocation\nscales: WhereCountry (i.e., 500 multiple-choice question-answering, with\ncountry-level answer and panoramas) and WhereStreet (i.e., 310 fine-grained\nstreet-level identification tasks requiring multi-step reasoning with optional\nweb search). For evaluation, we adopt the final-prediction metrics: location\naccuracies within k km (Acc@k) for coordinates and hierarchical path scores for\ntextual localization. Beyond this, we propose to explicitly score intermediate\nreasoning chains using human-verified key visual clues and a Shapley-reweighted\nthinking score that attributes credit to each clue's marginal contribution. We\nbenchmark 13 state-of-the-art VLMs with web searching tools on our EarthWhere\nand report different types of final answer accuracies as well as the calibrated\nmodel thinking scores. Overall, Gemini-2.5-Pro achieves the best average\naccuracy at 56.32%, while the strongest open-weight model, GLM-4.5V, reaches\n34.71%. We reveal that web search and reasoning do not guarantee improved\nperformance when visual clues are limited, and models exhibit regional biases,\nachieving up to 42.7% higher scores in certain areas than others. These\nfindings highlight not only the promise but also the persistent challenges of\nmodels to mitigate bias and achieve robust, fine-grained localization. We\nopen-source our benchmark at https://github.com/UCSC-VLAA/EarthWhere.",
            "headline_zh": "提出EarthWhere基准以评估视觉语言模型在开放世界图像地理定位能力",
            "intro_zh": [
                "核心问题：视觉语言模型在开放世界图像地理定位能力未全面评估，任务具挑战性和现实需求。",
                "方法要点：构建EarthWhere基准，包含多尺度地理定位任务，并引入推理链评分和Shapley重加权思考分数。",
                "实验或效果：评估13个先进模型，Gemini-2.5-Pro平均准确率最高，模型存在区域偏见和推理限制。"
            ],
            "tags_zh": [
                "图像地理定位",
                "视觉语言模型",
                "基准评估",
                "多尺度推理",
                "区域偏见",
                "开放世界任务"
            ],
            "_index": 139
        },
        {
            "title": "rareboost3d: a synthetic lidar dataset with enhanced rare classes",
            "authors": [
                "Shutong Lin",
                "Zhengkang Xiang",
                "Jianzhong Qi",
                "Kourosh Khoshelham"
            ],
            "arxiv_id": "2510.10876v1",
            "summary": "Real-world point cloud datasets have made significant contributions to the\ndevelopment of LiDAR-based perception technologies, such as object segmentation\nfor autonomous driving. However, due to the limited number of instances in some\nrare classes, the long-tail problem remains a major challenge in existing\ndatasets. To address this issue, we introduce a novel, synthetic point cloud\ndataset named RareBoost3D, which complements existing real-world datasets by\nproviding significantly more instances for object classes that are rare in\nreal-world datasets. To effectively leverage both synthetic and real-world\ndata, we further propose a cross-domain semantic alignment method named CSC\nloss that aligns feature representations of the same class across different\ndomains. Experimental results demonstrate that this alignment significantly\nenhances the performance of LiDAR point cloud segmentation models over\nreal-world data.",
            "headline_zh": "提出RareBoost3D合成数据集和CSC损失方法以解决LiDAR点云分割中的长尾问题",
            "intro_zh": [
                "核心问题：真实LiDAR数据集存在长尾问题，稀有类别实例不足，影响分割性能。",
                "方法要点：引入合成数据集RareBoost3D，并设计跨域语义对齐CSC损失以对齐特征。",
                "实验或效果：实验表明该方法显著提升LiDAR点云分割模型在真实数据上的性能。"
            ],
            "tags_zh": [
                "LiDAR点云分割",
                "长尾问题",
                "合成数据集",
                "跨域对齐",
                "语义分割"
            ],
            "_index": 140
        },
        {
            "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding",
            "authors": [
                "Soroush Mehraban",
                "Andrea Iaboni",
                "Babak Taati"
            ],
            "arxiv_id": "2510.10868v1",
            "summary": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have\nachieved strong performance but often suffer from high computational cost and\ncomplexity due to deep transformer architectures and redundant tokens. In this\npaper, we introduce two HMR-specific merging strategies: Error-Constrained\nLayer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM\nselectively merges transformer layers that have minimal impact on the Mean Per\nJoint Position Error (MPJPE), while Mask-ToMe focuses on merging background\ntokens that contribute little to the final prediction. To further address the\npotential performance drop caused by merging, we propose a diffusion-based\ndecoder that incorporates temporal context and leverages pose priors learned\nfrom large-scale motion capture datasets. Experiments across multiple\nbenchmarks demonstrate that our method achieves up to 2.3x speed-up while\nslightly improving performance over the baseline.",
            "headline_zh": "提出FastHMR方法，通过层与令牌合并及扩散解码加速3D人体网格恢复",
            "intro_zh": [
                "核心问题：基于Transformer的3D人体网格恢复模型计算成本高、复杂度大，存在冗余令牌和层",
                "方法要点：引入误差约束层合并和掩码引导令牌合并策略，结合扩散解码器利用时间上下文和姿态先验",
                "实验或效果：在多个基准测试中实现最高2.3倍加速，同时性能略有提升"
            ],
            "tags_zh": [
                "3D人体网格恢复",
                "Transformer加速",
                "层合并",
                "令牌合并",
                "扩散解码",
                "计算效率"
            ],
            "_index": 141
        },
        {
            "title": "GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments",
            "authors": [
                "Ahmed Alanazi",
                "Duy Ho",
                "Yugyung Lee"
            ],
            "arxiv_id": "2510.10865v1",
            "summary": "Robots navigating dynamic, cluttered, and semantically complex environments\nmust integrate perception, symbolic reasoning, and spatial planning to\ngeneralize across diverse layouts and object categories. Existing methods often\nrely on static priors or limited memory, constraining adaptability under\npartial observability and semantic ambiguity. We present GRIP, Grid-based Relay\nwith Intermediate Planning, a unified, modular framework with three scalable\nvariants: GRIP-L (Lightweight), optimized for symbolic navigation via semantic\noccupancy grids; GRIP-F (Full), supporting multi-hop anchor chaining and\nLLM-based introspection; and GRIP-R (Real-World), enabling physical robot\ndeployment under perceptual uncertainty. GRIP integrates dynamic 2D grid\nconstruction, open-vocabulary object grounding, co-occurrence-aware symbolic\nplanning, and hybrid policy execution using behavioral cloning, D* search, and\ngrid-conditioned control. Empirical results on AI2-THOR and RoboTHOR benchmarks\nshow that GRIP achieves up to 9.6% higher success rates and over $2\\times$\nimprovement in path efficiency (SPL and SAE) on long-horizon tasks. Qualitative\nanalyses reveal interpretable symbolic plans in ambiguous scenes. Real-world\ndeployment on a Jetbot further validates GRIP's generalization under sensor\nnoise and environmental variation. These results position GRIP as a robust,\nscalable, and explainable framework bridging simulation and real-world\nnavigation.",
            "headline_zh": "提出GRIP框架以解决动态环境中机器人导航的适应性问题",
            "intro_zh": [
                "核心问题：动态、杂乱环境中机器人导航受限于静态先验和部分可观测性",
                "方法要点：集成动态网格构建、共现感知符号规划和混合策略执行",
                "实验或效果：在AI2-THOR和RoboTHOR上提升成功率9.6%和路径效率2倍以上"
            ],
            "tags_zh": [
                "机器人导航",
                "动态环境规划",
                "语义网格",
                "符号推理",
                "混合策略执行",
                "现实世界部署"
            ],
            "_index": 142
        }
    ]
}