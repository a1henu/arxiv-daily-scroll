{
    "papers": [
        {
            "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
            "authors": [
                "Ruyi Xu",
                "Guangxuan Xiao",
                "Yukang Chen",
                "Liuning He",
                "Kelly Peng",
                "Yao Lu",
                "Song Han"
            ],
            "arxiv_id": "2510.09608v1",
            "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
            "headline_zh": "提出StreamingVLM以实时理解无限视频流，解决延迟与内存问题。",
            "intro_zh": [
                "核心问题：传统VLM处理长视频时计算成本高、性能差，滑动窗口方法破坏连贯性或延迟高。",
                "方法要点：采用统一框架，通过SFT训练模拟流式推理，维护紧凑KV缓存以重用注意力状态。",
                "实验效果：在Inf-Streams-Eval基准上胜率66.18%，实时性能达8 FPS，并提升通用VQA能力。"
            ],
            "tags_zh": [
                "流式视觉语言模型",
                "长视频理解",
                "实时推理",
                "KV缓存优化",
                "监督微调",
                "基准评估"
            ],
            "_index": 0
        },
        {
            "title": "VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation",
            "authors": [
                "Shaoqi Dong",
                "Chaoyou Fu",
                "Haihan Gao",
                "Yi-Fan Zhang",
                "Chi Yan",
                "Chu Wu",
                "Xiaoyu Liu",
                "Yunhang Shen",
                "Jing Huo",
                "Deqiang Jiang",
                "Haoyu Cao",
                "Yang Gao",
                "Xing Sun",
                "Ran He",
                "Caifeng Shan"
            ],
            "arxiv_id": "2510.09607v1",
            "summary": "Vision-Language Action (VLA) models significantly advance robotic\nmanipulation by leveraging the strong perception capabilities of pretrained\nvision-language models (VLMs). By integrating action modules into these\npretrained models, VLA methods exhibit improved generalization. However,\ntraining them from scratch is costly. In this work, we propose a simple yet\neffective distillation-based framework that equips VLMs with action-execution\ncapability by transferring knowledge from pretrained small action models. Our\narchitecture retains the original VLM structure, adding only an action token\nand a state encoder to incorporate physical inputs. To distill action\nknowledge, we adopt a two-stage training strategy. First, we perform\nlightweight alignment by mapping VLM hidden states into the action space of the\nsmall action model, enabling effective reuse of its pretrained action decoder\nand avoiding expensive pretraining. Second, we selectively fine-tune the\nlanguage model, state encoder, and action modules, enabling the system to\nintegrate multimodal inputs with precise action generation. Specifically, the\naction token provides the VLM with a direct handle for predicting future\nactions, while the state encoder allows the model to incorporate robot dynamics\nnot captured by vision alone. This design yields substantial efficiency gains\nover training large VLA models from scratch. Compared with previous\nstate-of-the-art methods, our method achieves 97.3% average success rate on\nLIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In\nreal-world experiments across five manipulation tasks, our method consistently\noutperforms the teacher model, achieving 82.0% success rate (17% improvement),\nwhich demonstrate that action distillation effectively enables VLMs to generate\nprecise actions while substantially reducing training costs.",
            "headline_zh": "提出基于动作专家蒸馏的框架，高效赋予视觉语言模型动作执行能力",
            "intro_zh": [
                "核心问题：训练大型视觉语言动作模型成本高昂，限制其泛化能力。",
                "方法要点：通过两阶段蒸馏，重用预训练小动作模型知识，避免从头训练。",
                "实验效果：在LIBERO和真实任务中显著提升成功率，降低训练成本。"
            ],
            "tags_zh": [
                "视觉语言动作模型",
                "知识蒸馏",
                "机器人操作",
                "高效训练",
                "动作预测"
            ],
            "_index": 1
        },
        {
            "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
            "authors": [
                "Peiwen Sun",
                "Shiqiang Lang",
                "Dongming Wu",
                "Yi Ding",
                "Kaituo Feng",
                "Huadai Liu",
                "Zhen Ye",
                "Rui Liu",
                "Yun-Hui Liu",
                "Jianan Wang",
                "Xiangyu Yue"
            ],
            "arxiv_id": "2510.09606v1",
            "summary": "With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates a structured spatial reasoning knowledge system, scale-aware\nmodeling, and a progressive training paradigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising\napproximately 1M spatial QA pairs spanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining with SpaceVista-1M often yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a\nspatial reasoning model that accepts dense inputs beyond semantics and uses\nscale as an anchor for scale-aware experts and progressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including our SpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km .",
            "headline_zh": "提出SpaceVista以解决全尺度视觉空间推理问题，涵盖毫米到千米场景。",
            "intro_zh": [
                "核心问题：依赖室内3D扫描和手动标注，缺乏有效全尺度场景建模。",
                "方法要点：集成结构化知识系统、尺度感知建模和渐进训练范式。",
                "实验或效果：在5个基准测试中表现竞争性，展示全尺度和场景泛化能力。"
            ],
            "tags_zh": [
                "全尺度空间推理",
                "尺度感知建模",
                "渐进训练",
                "空间问答数据集",
                "多模态大语言模型",
                "视觉基准测试"
            ],
            "_index": 2
        },
        {
            "title": "STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging",
            "authors": [
                "Disharee Bhowmick",
                "Ranjith Ramanathan",
                "Sathyanarayanan N. Aakur"
            ],
            "arxiv_id": "2510.09593v1",
            "summary": "Time series data often contain latent temporal structure, transitions between\nlocally stationary regimes, repeated motifs, and bursts of variability, that\nare rarely leveraged in standard representation learning pipelines. Existing\nmodels typically operate on raw or fixed-window sequences, treating all time\nsteps as equally informative, which leads to inefficiencies, poor robustness,\nand limited scalability in long or noisy sequences. We propose STaTS, a\nlightweight, unsupervised framework for Structure-Aware Temporal Summarization\nthat adaptively compresses both univariate and multivariate time series into\ncompact, information-preserving token sequences. STaTS detects change points\nacross multiple temporal resolutions using a BIC-based statistical divergence\ncriterion, then summarizes each segment using simple functions like the mean or\ngenerative models such as GMMs. This process achieves up to 30x sequence\ncompression while retaining core temporal dynamics. STaTS operates as a\nmodel-agnostic preprocessor and can be integrated with existing unsupervised\ntime series encoders without retraining. Extensive experiments on 150+\ndatasets, including classification tasks on the UCR-85, UCR-128, and UEA-30\narchives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,\ndemonstrate that STaTS enables 85-90\\% of the full-model performance while\noffering dramatic reductions in computational cost. Moreover, STaTS improves\nrobustness under noise and preserves discriminative structure, outperforming\nuniform and clustering-based compression baselines. These results position\nSTaTS as a principled, general-purpose solution for efficient, structure-aware\ntime series modeling.",
            "headline_zh": "提出STaTS框架以解决时间序列结构感知压缩问题",
            "intro_zh": [
                "时间序列数据常含潜在结构，但现有方法忽略结构导致效率低和鲁棒性差",
                "STaTS使用统计差异准则检测变点，并自适应压缩为信息保留的令牌序列",
                "实验显示STaTS在150+数据集上实现高压缩比，保持85-90%性能并提升鲁棒性"
            ],
            "tags_zh": [
                "时间序列压缩",
                "结构感知建模",
                "无监督学习",
                "变点检测",
                "计算效率优化"
            ],
            "_index": 3
        },
        {
            "title": "Vision Language Models: A Survey of 26K Papers",
            "authors": [
                "Fengming Lin"
            ],
            "arxiv_id": "2510.09586v1",
            "summary": "We present a transparent, reproducible measurement of research trends across\n26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles\nand abstracts are normalized, phrase-protected, and matched against a\nhand-crafted lexicon to assign up to 35 topical labels and mine fine-grained\ncues about tasks, architectures, training regimes, objectives, datasets, and\nco-mentioned modalities. The analysis quantifies three macro shifts: (1) a\nsharp rise of multimodal vision-language-LLM work, which increasingly reframes\nclassic perception as instruction following and multi-step reasoning; (2)\nsteady expansion of generative methods, with diffusion research consolidating\naround controllability, distillation, and speed; and (3) resilient 3D and video\nactivity, with composition moving from NeRFs to Gaussian splatting and a\ngrowing emphasis on human- and agent-centric understanding. Within VLMs,\nparameter-efficient adaptation like prompting/adapters/LoRA and lightweight\nvision-language bridges dominate; training practice shifts from building\nencoders from scratch to instruction tuning and finetuning strong backbones;\ncontrastive objectives recede relative to cross-entropy/ranking and\ndistillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and\nICLR the highest VLM share, while reliability themes such as efficiency or\nrobustness diffuse across areas. We release the lexicon and methodology to\nenable auditing and extension. Limitations include lexicon recall and\nabstract-only scope, but the longitudinal signals are consistent across venues\nand years.",
            "headline_zh": "分析26K论文以量化计算机视觉与语言模型研究趋势",
            "intro_zh": [
                "核心问题：量化CVPR、ICLR和NeurIPS会议中视觉语言模型等主题的研究演变。",
                "方法要点：使用手工词典对标题和摘要进行标注，提取任务、架构等细粒度信息。",
                "实验或效果：识别多模态、生成方法和3D视频活动的宏观转变及跨会场比较。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "多模态研究",
                "生成方法",
                "3D视觉",
                "参数高效适应",
                "研究趋势分析"
            ],
            "_index": 4
        },
        {
            "title": "FSP-DETR: Few-Shot Prototypical Parasitic Ova Detection",
            "authors": [
                "Shubham Trehan",
                "Udhav Ramachandran",
                "Akash Rao",
                "Ruth Scimeca",
                "Sathyanarayanan N. Aakur"
            ],
            "arxiv_id": "2510.09583v1",
            "summary": "Object detection in biomedical settings is fundamentally constrained by the\nscarcity of labeled data and the frequent emergence of novel or rare\ncategories. We present FSP-DETR, a unified detection framework that enables\nrobust few-shot detection, open-set recognition, and generalization to unseen\nbiomedical tasks within a single model. Built upon a class-agnostic DETR\nbackbone, our approach constructs class prototypes from original support images\nand learns an embedding space using augmented views and a lightweight\ntransformer decoder. Training jointly optimizes a prototype matching loss, an\nalignment-based separation loss, and a KL divergence regularization to improve\ndiscriminative feature learning and calibration under scarce supervision.\nUnlike prior work that tackles these tasks in isolation, FSP-DETR enables\ninference-time flexibility to support unseen class recognition, background\nrejection, and cross-task adaptation without retraining. We also introduce a\nnew ova species detection benchmark with 20 parasite classes and establish\nstandardized evaluation protocols. Extensive experiments across ova, blood\ncell, and malaria detection tasks demonstrate that FSP-DETR significantly\noutperforms prior few-shot and prototype-based detectors, especially in\nlow-shot and open-set scenarios.",
            "headline_zh": "提出FSP-DETR框架以解决生物医学中少样本检测和开放集识别问题",
            "intro_zh": [
                "核心问题：生物医学目标检测中标注数据稀缺和新类别频繁出现",
                "方法要点：基于类无关DETR构建原型，使用增强视图和轻量解码器学习嵌入空间",
                "实验或效果：在寄生虫卵等任务中显著优于现有方法，尤其在少样本和开放集场景"
            ],
            "tags_zh": [
                "少样本目标检测",
                "原型学习",
                "开放集识别",
                "生物医学图像分析",
                "DETR框架",
                "跨任务适应"
            ],
            "_index": 5
        },
        {
            "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
            "authors": [
                "Xiao Yu",
                "Baolin Peng",
                "Michel Galley",
                "Hao Cheng",
                "Qianhui Wu",
                "Janardhan Kulkarni",
                "Suman Nath",
                "Zhou Yu",
                "Jianfeng Gao"
            ],
            "arxiv_id": "2510.09577v1",
            "summary": "Reasoning models have recently shown remarkable progress in domains such as\nmath and coding. However, their expert-level abilities in math and coding\ncontrast sharply with their performance in long-horizon, interactive tasks such\nas web navigation and computer/phone-use. Inspired by literature on human\ncognition, we argue that current AI agents need ''vicarious trial and error'' -\nthe capacity to mentally simulate alternative futures before acting - in order\nto enhance their understanding and performance in complex interactive\nenvironments. We introduce Dyna-Mind, a two-stage training framework that\nexplicitly teaches (V)LM agents to integrate such simulation into their\nreasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which\ntrains the agent to generate structured reasoning traces from expanded search\ntrees built from real experience gathered through environment interactions.\nReSim thus grounds the agent's reasoning in faithful world dynamics and equips\nit with the ability to anticipate future states in its reasoning. In stage 2,\nwe propose Dyna-GRPO, an online reinforcement learning method to further\nstrengthen the agent's simulation and decision-making ability by using both\noutcome rewards and intermediate states as feedback from real rollouts.\nExperiments on two synthetic benchmarks (Sokoban and ALFWorld) and one\nrealistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively\ninfuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome\nand interaction-level signals to learn better policies for long-horizon,\nplanning-intensive tasks. Together, these results highlight the central role of\nsimulation in enabling AI agents to reason, plan, and act more effectively in\nthe ever more challenging environments.",
            "headline_zh": "提出Dyna-Mind框架以提升AI代理在复杂交互任务中的推理与决策能力",
            "intro_zh": [
                "核心问题：AI代理在数学和编码领域表现优异，但在长视野交互任务中性能不足。",
                "方法要点：采用两阶段训练，先通过ReSim学习模拟推理，再以Dyna-GRPO强化决策。",
                "实验或效果：在合成和真实基准测试中验证了模拟能力提升和策略优化效果。"
            ],
            "tags_zh": [
                "AI代理",
                "模拟推理",
                "强化学习",
                "长视野任务",
                "交互环境",
                "决策优化"
            ],
            "_index": 6
        },
        {
            "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control",
            "authors": [
                "Minkyoung Cho",
                "Ruben Ohana",
                "Christian Jacobsen",
                "Adityan Jothi",
                "Min-Hung Chen",
                "Z. Morley Mao",
                "Ethem Can"
            ],
            "arxiv_id": "2510.09561v1",
            "summary": "Current controllable diffusion models typically rely on fixed architectures\nthat modify intermediate activations to inject guidance conditioned on a new\nmodality. This approach uses a static conditioning strategy for a dynamic,\nmulti-stage denoising process, limiting the model's ability to adapt its\nresponse as the generation evolves from coarse structure to fine detail. We\nintroduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that\nenables dynamic, context-aware control by conditioning the model's weights\ndirectly. Our framework uses a hypernetwork to generate LoRA adapters\non-the-fly, tailoring weight modifications for the frozen backbone at each\ndiffusion step based on time and the user's condition. This mechanism enables\nthe model to learn and execute an explicit, adaptive strategy for applying\nconditional guidance throughout the entire generation process. Through\nexperiments on various data domains, we demonstrate that this dynamic,\nparametric control significantly enhances generative fidelity and adherence to\nspatial conditions compared to static, activation-based methods. TC-LoRA\nestablishes an alternative approach in which the model's conditioning strategy\nis modified through a deeper functional adaptation of its weights, allowing\ncontrol to align with the dynamic demands of the task and generative stage.",
            "headline_zh": "提出TC-LoRA以动态适应扩散模型的控制需求",
            "intro_zh": [
                "静态条件策略限制扩散模型在生成过程中动态调整能力",
                "使用超网络实时生成LoRA适配器，基于时间和条件动态调整权重",
                "实验显示动态控制提升生成保真度和空间条件遵循度"
            ],
            "tags_zh": [
                "可控扩散模型",
                "动态权重调整",
                "LoRA适配器",
                "超网络",
                "生成保真度"
            ],
            "_index": 7
        },
        {
            "title": "FLOWING: Implicit Neural Flows for Structure-Preserving Morphing",
            "authors": [
                "Arthur Bizzi",
                "Matias Grynberg",
                "Vitor Matias",
                "Daniel Perazzo",
                "João Paulo Lima",
                "Luiz Velho",
                "Nuno Gonçalves",
                "João Pereira",
                "Guilherme Schardong",
                "Tiago Novello"
            ],
            "arxiv_id": "2510.09537v1",
            "summary": "Morphing is a long-standing problem in vision and computer graphics,\nrequiring a time-dependent warping for feature alignment and a blending for\nsmooth interpolation. Recently, multilayer perceptrons (MLPs) have been\nexplored as implicit neural representations (INRs) for modeling such\ndeformations, due to their meshlessness and differentiability; however,\nextracting coherent and accurate morphings from standard MLPs typically relies\non costly regularizations, which often lead to unstable training and prevent\neffective feature alignment. To overcome these limitations, we propose FLOWING\n(FLOW morphING), a framework that recasts warping as the construction of a\ndifferential vector flow, naturally ensuring continuity, invertibility, and\ntemporal coherence by encoding structural flow properties directly into the\nnetwork architectures. This flow-centric approach yields principled and stable\ntransformations, enabling accurate and structure-preserving morphing of both 2D\nimages and 3D shapes. Extensive experiments across a range of applications -\nincluding face and image morphing, as well as Gaussian Splatting morphing -\nshow that FLOWING achieves state-of-the-art morphing quality with faster\nconvergence. Code and pretrained models are available at\nhttp://schardong.github.io/flowing.",
            "headline_zh": "提出FLOWING框架，通过微分向量流实现结构保持的2D图像和3D形状变形",
            "intro_zh": [
                "核心问题：传统多层感知机变形方法依赖昂贵正则化，导致训练不稳定和特征对齐困难",
                "方法要点：将变形建模为微分向量流，确保连续性、可逆性和时间一致性",
                "实验或效果：在面部和图像变形等应用中，实现最先进质量并加速收敛"
            ],
            "tags_zh": [
                "隐式神经表示",
                "图像变形",
                "3D形状变形",
                "微分向量流",
                "结构保持变形"
            ],
            "_index": 8
        },
        {
            "title": "PRNet: Original Information Is All You Have",
            "authors": [
                "PeiHuang Zheng",
                "Yunlong Zhao",
                "Zheng Cui",
                "Yang Li"
            ],
            "arxiv_id": "2510.09531v1",
            "summary": "Small object detection in aerial images suffers from severe information\ndegradation during feature extraction due to limited pixel representations,\nwhere shallow spatial details fail to align effectively with semantic\ninformation, leading to frequent misses and false positives. Existing FPN-based\nmethods attempt to mitigate these losses through post-processing enhancements,\nbut the reconstructed details often deviate from the original image\ninformation, impeding their fusion with semantic content. To address this\nlimitation, we propose PRNet, a real-time detection framework that prioritizes\nthe preservation and efficient utilization of primitive shallow spatial\nfeatures to enhance small object representations. PRNet achieves this via two\nmodules:the Progressive Refinement Neck (PRN) for spatial-semantic alignment\nthrough backbone reuse and iterative refinement, and the Enhanced SliceSamp\n(ESSamp) for preserving shallow information during downsampling via optimized\nrearrangement and convolution. Extensive experiments on the VisDrone, AI-TOD,\nand UAVDT datasets demonstrate that PRNet outperforms state-of-the-art methods\nunder comparable computational constraints, achieving superior\naccuracy-efficiency trade-offs.",
            "headline_zh": "提出PRNet以解决航拍图像小目标检测中的信息退化问题",
            "intro_zh": [
                "核心问题：航拍图像小目标检测中，浅层空间细节与语义信息对齐困难，导致漏检和误检",
                "方法要点：通过渐进细化颈部和增强切片采样模块，保留并高效利用原始浅层特征",
                "实验或效果：在VisDrone等数据集上优于现有方法，实现高精度与效率的平衡"
            ],
            "tags_zh": [
                "小目标检测",
                "航拍图像",
                "特征保留",
                "实时检测",
                "渐进细化",
                "空间语义对齐"
            ],
            "_index": 9
        },
        {
            "title": "Diagonal Artifacts in Samsung Images: PRNU Challenges and Solutions",
            "authors": [
                "David Vázquez-Padín",
                "Fernando Pérez-González",
                "Alejandro Martín-Del-Río"
            ],
            "arxiv_id": "2510.09509v1",
            "summary": "We investigate diagonal artifacts present in images captured by several\nSamsung smartphones and their impact on PRNU-based camera source verification.\nWe first show that certain Galaxy S series models share a common pattern\ncausing fingerprint collisions, with a similar issue also found in some Galaxy\nA models. Next, we demonstrate that reliable PRNU verification remains feasible\nfor devices supporting PRO mode with raw capture, since raw images bypass the\nprocessing pipeline that introduces artifacts. This option, however, is not\navailable for the mid-range A series models or in forensic cases without access\nto raw images. Finally, we outline potential forensic applications of the\ndiagonal artifacts, such as reducing misdetections in HDR images and localizing\nregions affected by synthetic bokeh in portrait-mode images.",
            "headline_zh": "分析三星图像对角线伪影对PRNU验证的影响并提出解决方案",
            "intro_zh": [
                "三星Galaxy S和A系列图像存在对角线伪影，导致PRNU指纹碰撞问题",
                "使用PRO模式原始图像可避免伪影，但中端A系列和法医场景无法获取",
                "伪影可用于减少HDR误检和定位人像模式合成虚化区域"
            ],
            "tags_zh": [
                "PRNU验证",
                "图像伪影",
                "相机源识别",
                "原始图像",
                "法医图像分析",
                "智能手机图像处理"
            ],
            "_index": 10
        },
        {
            "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
            "authors": [
                "Zixin Zhang",
                "Kanghao Chen",
                "Xingwang Lin",
                "Lutao Jiang",
                "Xu Zheng",
                "Yuanhuiyi Lyu",
                "Litao Guo",
                "Yinchuan Li",
                "Ying-Cong Chen"
            ],
            "arxiv_id": "2510.09507v1",
            "summary": "The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available.",
            "headline_zh": "提出PhysToolBench基准以评估多模态大语言模型对物理工具的理解能力",
            "intro_zh": [
                "核心问题：多模态大语言模型对物理工具的理解能力尚未量化，影响其在具身AI中的应用。",
                "方法要点：构建包含1000+图像-文本对的视觉问答数据集，评估工具识别、理解和创造三个难度级别。",
                "实验或效果：评估32个模型发现工具理解存在显著缺陷，并提供初步解决方案。"
            ],
            "tags_zh": [
                "物理工具理解",
                "多模态大语言模型",
                "视觉问答基准",
                "具身AI",
                "工具创造"
            ],
            "_index": 11
        },
        {
            "title": "A methodology for clinically driven interactive segmentation evaluation",
            "authors": [
                "Parhom Esmaeili",
                "Virginia Fernandez",
                "Pedro Borges",
                "Eli Gibson",
                "Sebastien Ourselin",
                "M. Jorge Cardoso"
            ],
            "arxiv_id": "2510.09499v1",
            "summary": "Interactive segmentation is a promising strategy for building robust,\ngeneralisable algorithms for volumetric medical image segmentation. However,\ninconsistent and clinically unrealistic evaluation hinders fair comparison and\nmisrepresents real-world performance. We propose a clinically grounded\nmethodology for defining evaluation tasks and metrics, and built a software\nframework for constructing standardised evaluation pipelines. We evaluate\nstate-of-the-art algorithms across heterogeneous and complex tasks and observe\nthat (i) minimising information loss when processing user interactions is\ncritical for model robustness, (ii) adaptive-zooming mechanisms boost\nrobustness and speed convergence, (iii) performance drops if validation\nprompting behaviour/budgets differ from training, (iv) 2D methods perform well\nwith slab-like images and coarse targets, but 3D context helps with large or\nirregularly shaped targets, (v) performance of non-medical-domain models (e.g.\nSAM2) degrades with poor contrast and complex shapes.",
            "headline_zh": "提出临床驱动的交互式分割评估方法以解决医学图像分割评估不一致问题",
            "intro_zh": [
                "核心问题：交互式分割评估不一致且临床不现实，阻碍公平比较和真实性能评估",
                "方法要点：定义临床基础评估任务与指标，构建标准化评估软件框架",
                "实验或效果：评估先进算法，发现交互处理信息损失最小化、自适应缩放等关键因素"
            ],
            "tags_zh": [
                "交互式分割",
                "医学图像评估",
                "临床驱动方法",
                "标准化框架",
                "算法鲁棒性"
            ],
            "_index": 12
        },
        {
            "title": "Few-shot multi-token DreamBooth with LoRa for style-consistent character generation",
            "authors": [
                "Ruben Pascual",
                "Mikel Sesma-Sara",
                "Aranzazu Jurio",
                "Daniel Paternain",
                "Mikel Galar"
            ],
            "arxiv_id": "2510.09475v1",
            "summary": "The audiovisual industry is undergoing a profound transformation as it is\nintegrating AI developments not only to automate routine tasks but also to\ninspire new forms of art. This paper addresses the problem of producing a\nvirtually unlimited number of novel characters that preserve the artistic style\nand shared visual traits of a small set of human-designed reference characters,\nthus broadening creative possibilities in animation, gaming, and related\ndomains. Our solution builds upon DreamBooth, a well-established fine-tuning\ntechnique for text-to-image diffusion models, and adapts it to tackle two core\nchallenges: capturing intricate character details beyond textual prompts and\nthe few-shot nature of the training data. To achieve this, we propose a\nmulti-token strategy, using clustering to assign separate tokens to individual\ncharacters and their collective style, combined with LoRA-based\nparameter-efficient fine-tuning. By removing the class-specific regularization\nset and introducing random tokens and embeddings during generation, our\napproach allows for unlimited character creation while preserving the learned\nstyle. We evaluate our method on five small specialized datasets, comparing it\nto relevant baselines using both quantitative metrics and a human evaluation\nstudy. Our results demonstrate that our approach produces high-quality, diverse\ncharacters while preserving the distinctive aesthetic features of the reference\ncharacters, with human evaluation further reinforcing its effectiveness and\nhighlighting the potential of our method.",
            "headline_zh": "提出多令牌DreamBooth与LoRA方法，解决少样本风格一致角色生成问题",
            "intro_zh": [
                "核心问题：从少量参考角色生成无限新角色，需保持艺术风格和共享视觉特征",
                "方法要点：结合多令牌策略和LoRA微调，移除类特定正则化，引入随机令牌",
                "实验或效果：在五个数据集上评估，定量和人类评估显示高质多样角色生成"
            ],
            "tags_zh": [
                "少样本学习",
                "角色生成",
                "风格一致性",
                "DreamBooth",
                "LoRA微调",
                "多令牌策略"
            ],
            "_index": 13
        },
        {
            "title": "D-TPT: Dimensional Entropy Maximization for Calibrating Test-Time Prompt Tuning in Vision-Language Models",
            "authors": [
                "Jisu Han",
                "Wonjun Hwang"
            ],
            "arxiv_id": "2510.09473v1",
            "summary": "Test-time adaptation paradigm provides flexibility towards domain shifts by\nperforming immediate adaptation on unlabeled target data from the source model.\nVision-Language Models (VLMs) leverage their generalization capabilities for\ndiverse downstream tasks, and test-time prompt tuning has emerged as a\nprominent solution for adapting VLMs. In this work, we explore contrastive VLMs\nand identify the modality gap caused by a single dominant feature dimension\nacross modalities. We observe that the dominant dimensions in both text and\nimage modalities exhibit high predictive sensitivity, and that constraining\ntheir influence can improve calibration error. Building on this insight, we\npropose dimensional entropy maximization that regularizes the distribution of\ntextual features toward uniformity to mitigate the dependency of dominant\ndimensions. Our method alleviates the degradation of calibration performance in\ntest-time prompt tuning, offering a simple yet effective solution to enhance\nthe reliability of VLMs in real-world deployment scenarios.",
            "headline_zh": "提出维度熵最大化方法以解决测试时提示调优中的校准性能下降问题",
            "intro_zh": [
                "核心问题：模态间隙由单一主导特征维度引起，导致预测敏感性和校准误差增加",
                "方法要点：通过正则化文本特征分布向均匀性，减少主导维度依赖，提升校准",
                "实验或效果：缓解校准性能退化，增强视觉语言模型在真实部署中的可靠性"
            ],
            "tags_zh": [
                "测试时适应",
                "视觉语言模型",
                "提示调优",
                "校准误差",
                "维度熵最大化"
            ],
            "_index": 14
        },
        {
            "title": "SilvaScenes: Tree Segmentation and Species Classification from Under-Canopy Images in Natural Forests",
            "authors": [
                "David-Alexandre Duclos",
                "William Guimont-Martin",
                "Gabriel Jeanson",
                "Arthur Larochelle-Tremblay",
                "Théo Defosse",
                "Frédéric Moore",
                "Philippe Nolet",
                "François Pomerleau",
                "Philippe Giguère"
            ],
            "arxiv_id": "2510.09458v1",
            "summary": "Interest in robotics for forest management is growing, but perception in\ncomplex, natural environments remains a significant hurdle. Conditions such as\nheavy occlusion, variable lighting, and dense vegetation pose challenges to\nautomated systems, which are essential for precision forestry, biodiversity\nmonitoring, and the automation of forestry equipment. These tasks rely on\nadvanced perceptual capabilities, such as detection and fine-grained species\nclassification of individual trees. Yet, existing datasets are inadequate to\ndevelop such perception systems, as they often focus on urban settings or a\nlimited number of species. To address this, we present SilvaScenes, a new\ndataset for instance segmentation of tree species from under-canopy images.\nCollected across five bioclimatic domains in Quebec, Canada, SilvaScenes\nfeatures 1476 trees from 24 species with annotations from forestry experts. We\ndemonstrate the relevance and challenging nature of our dataset by benchmarking\nmodern deep learning approaches for instance segmentation. Our results show\nthat, while tree segmentation is easy, with a top mean average precision (mAP)\nof 67.65%, species classification remains a significant challenge with an mAP\nof only 35.69%. Our dataset and source code will be available at\nhttps://github.com/norlab-ulaval/SilvaScenes.",
            "headline_zh": "提出SilvaScenes数据集以解决自然林冠下树木分割与物种分类的感知挑战",
            "intro_zh": [
                "核心问题：自然森林中遮挡、光照变化和植被密集导致机器人感知困难，现有数据集不足",
                "方法要点：构建包含24种1476棵树木的实例分割数据集，基于专家标注和深度学习基准测试",
                "实验或效果：树木分割mAP达67.65%，物种分类mAP仅35.69%，显示分类任务更具挑战性"
            ],
            "tags_zh": [
                "实例分割",
                "物种分类",
                "森林机器人感知",
                "深度学习基准",
                "自然环境数据集"
            ],
            "_index": 15
        },
        {
            "title": "Dynamic Weight-based Temporal Aggregation for Low-light Video Enhancement",
            "authors": [
                "Ruirui Lin",
                "Guoxi Huang",
                "Nantheera Anantrasirichai"
            ],
            "arxiv_id": "2510.09450v1",
            "summary": "Low-light video enhancement (LLVE) is challenging due to noise, low contrast,\nand color degradations. Learning-based approaches offer fast inference but\nstill struggle with heavy noise in real low-light scenes, primarily due to\nlimitations in effectively leveraging temporal information. In this paper, we\naddress this issue with DWTA-Net, a novel two-stage framework that jointly\nexploits short- and long-term temporal cues. Stage I employs Visual State-Space\nblocks for multi-frame alignment, recovering brightness, color, and structure\nwith local consistency. Stage II introduces a recurrent refinement module with\ndynamic weight-based temporal aggregation guided by optical flow, adaptively\nbalancing static and dynamic regions. A texture-adaptive loss further preserves\nfine details while promoting smoothness in flat areas. Experiments on\nreal-world low-light videos show that DWTA-Net effectively suppresses noise and\nartifacts, delivering superior visual quality compared with state-of-the-art\nmethods.",
            "headline_zh": "提出DWTA-Net框架以解决低光视频增强中的噪声和伪影问题",
            "intro_zh": [
                "核心问题：低光视频存在噪声、低对比度和颜色退化，现有方法难以有效利用时序信息。",
                "方法要点：采用两阶段框架，结合短长期时序线索，通过动态权重聚合和光流引导优化。",
                "实验或效果：在真实低光视频上验证，有效抑制噪声和伪影，视觉质量优于现有方法。"
            ],
            "tags_zh": [
                "低光视频增强",
                "时序聚合",
                "动态权重",
                "光流引导",
                "噪声抑制",
                "视觉质量提升"
            ],
            "_index": 16
        },
        {
            "title": "Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians",
            "authors": [
                "Jin-Chuan Shi",
                "Chengye Su",
                "Jiajun Wang",
                "Ariel Shamir",
                "Miao Wang"
            ],
            "arxiv_id": "2510.09438v1",
            "summary": "Editing 4D scenes reconstructed from monocular videos based on text prompts\nis a valuable yet challenging task with broad applications in content creation\nand virtual environments. The key difficulty lies in achieving semantically\nprecise edits in localized regions of complex, dynamic scenes, while preserving\nthe integrity of unedited content. To address this, we introduce Mono4DEditor,\na novel framework for flexible and accurate text-driven 4D scene editing. Our\nmethod augments 3D Gaussians with quantized CLIP features to form a\nlanguage-embedded dynamic representation, enabling efficient semantic querying\nof arbitrary spatial regions. We further propose a two-stage point-level\nlocalization strategy that first selects candidate Gaussians via CLIP\nsimilarity and then refines their spatial extent to improve accuracy. Finally,\ntargeted edits are performed on localized regions using a diffusion-based video\nediting model, with flow and scribble guidance ensuring spatial fidelity and\ntemporal coherence. Extensive experiments demonstrate that Mono4DEditor enables\nhigh-quality, text-driven edits across diverse scenes and object types, while\npreserving the appearance and geometry of unedited areas and surpassing prior\napproaches in both flexibility and visual fidelity.",
            "headline_zh": "提出Mono4DEditor框架，通过语言嵌入高斯实现单目视频4D场景的文本驱动编辑",
            "intro_zh": [
                "核心问题：单目视频4D场景编辑中，实现语义精确的局部编辑并保持未编辑内容完整性。",
                "方法要点：使用量化CLIP特征增强3D高斯，结合两阶段点级定位和扩散模型进行编辑。",
                "实验或效果：在多样场景中实现高质量编辑，超越现有方法，保持时空一致性。"
            ],
            "tags_zh": [
                "4D场景编辑",
                "文本驱动编辑",
                "3D高斯表示",
                "CLIP特征嵌入",
                "扩散模型编辑",
                "单目视频重建"
            ],
            "_index": 17
        },
        {
            "title": "Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation",
            "authors": [
                "Mert İnan",
                "Anthony Sicilia",
                "Alex Xie",
                "Saujas Vaduguru",
                "Daniel Fried",
                "Malihe Alikhani"
            ],
            "arxiv_id": "2510.09390v1",
            "summary": "Establishing shared goals is a fundamental step in human-AI communication.\nHowever, ambiguities can lead to outputs that seem correct but fail to reflect\nthe speaker's intent. In this paper, we explore this issue with a focus on the\ndata visualization domain, where ambiguities in natural language impact the\ngeneration of code that visualizes data. The availability of multiple views on\nthe contextual (e.g., the intended plot and the code rendering the plot) allows\nfor a unique and comprehensive analysis of diverse ambiguity types. We develop\na taxonomy of types of ambiguity that arise in this task and propose metrics to\nquantify them. Using Matplotlib problems from the DS-1000 dataset, we\ndemonstrate that our ambiguity metrics better correlate with human annotations\nthan uncertainty baselines. Our work also explores how multi-turn dialogue can\nreduce ambiguity, therefore, improve code accuracy by better matching user\ngoals. We evaluate three pragmatic models to inform our dialogue strategies:\nGricean Cooperativity, Discourse Representation Theory, and Questions under\nDiscussion. A simulated user study reveals how pragmatic dialogues reduce\nambiguity and enhance code accuracy, highlighting the value of multi-turn\nexchanges in code generation.",
            "headline_zh": "提出歧义分类与多轮对话方法，以提升数据可视化代码生成的准确性。",
            "intro_zh": [
                "核心问题：自然语言歧义导致数据可视化代码生成不准确，影响用户意图匹配。",
                "方法要点：开发歧义分类法，并基于语用模型设计多轮对话策略以澄清目标。",
                "实验或效果：在DS-1000数据集上验证歧义指标优于基线，模拟用户研究显示多轮对话提高代码准确率。"
            ],
            "tags_zh": [
                "数据可视化",
                "代码生成",
                "自然语言歧义",
                "多轮对话",
                "语用模型",
                "歧义分类"
            ],
            "_index": 18
        },
        {
            "title": "Utilizing dynamic sparsity on pretrained DETR",
            "authors": [
                "Reza Sedghi",
                "Anand Subramoney",
                "David Kappel"
            ],
            "arxiv_id": "2510.09380v1",
            "summary": "Efficient inference with transformer-based models remains a challenge,\nespecially in vision tasks like object detection. We analyze the inherent\nsparsity in the MLP layers of DETR and introduce two methods to exploit it\nwithout retraining. First, we propose Static Indicator-Based Sparsification\n(SIBS), a heuristic method that predicts neuron inactivity based on fixed\nactivation patterns. While simple, SIBS offers limited gains due to the\ninput-dependent nature of sparsity. To address this, we introduce Micro-Gated\nSparsification (MGS), a lightweight gating mechanism trained on top of a\npretrained DETR. MGS predicts dynamic sparsity using a small linear layer and\nachieves up to 85 to 95% activation sparsity. Experiments on the COCO dataset\nshow that MGS maintains or even improves performance while significantly\nreducing computation. Our method offers a practical, input-adaptive approach to\nsparsification, enabling efficient deployment of pretrained vision transformers\nwithout full model retraining.",
            "headline_zh": "提出动态稀疏化方法以提升预训练DETR在目标检测中的推理效率",
            "intro_zh": [
                "核心问题：Transformer模型在视觉任务中推理效率低，尤其DETR的MLP层存在固有稀疏性。",
                "方法要点：引入SIBS和MGS，MGS使用轻量门控机制预测动态稀疏，无需完整重训练。",
                "实验效果：在COCO数据集上，MGS实现85-95%激活稀疏，保持或提升性能并减少计算。"
            ],
            "tags_zh": [
                "动态稀疏化",
                "DETR模型",
                "目标检测",
                "推理效率",
                "门控机制",
                "COCO数据集"
            ],
            "_index": 19
        },
        {
            "title": "Minkowski-MambaNet: A Point Cloud Framework with Selective State Space Models for Forest Biomass Quantification",
            "authors": [
                "Jinxiang Tu",
                "Dayong Ren",
                "Fei Shi",
                "Zhenhong Jia",
                "Yahong Ren",
                "Jiwei Qin",
                "Fang He"
            ],
            "arxiv_id": "2510.09367v1",
            "summary": "Accurate forest biomass quantification is vital for carbon cycle monitoring.\nWhile airborne LiDAR excels at capturing 3D forest structure, directly\nestimating woody volume and Aboveground Biomass (AGB) from point clouds is\nchallenging due to difficulties in modeling long-range dependencies needed to\ndistinguish trees.We propose Minkowski-MambaNet, a novel deep learning\nframework that directly estimates volume and AGB from raw LiDAR. Its key\ninnovation is integrating the Mamba model's Selective State Space Model (SSM)\ninto a Minkowski network, enabling effective encoding of global context and\nlong-range dependencies for improved tree differentiation. Skip connections are\nincorporated to enhance features and accelerate convergence.Evaluated on Danish\nNational Forest Inventory LiDAR data, Minkowski-MambaNet significantly\noutperforms state-of-the-art methods, providing more accurate and robust\nestimates. Crucially, it requires no Digital Terrain Model (DTM) and is robust\nto boundary artifacts. This work offers a powerful tool for large-scale forest\nbiomass analysis, advancing LiDAR-based forest inventories.",
            "headline_zh": "提出Minkowski-MambaNet框架，结合选择性状态空间模型，从LiDAR点云直接估计森林生物量",
            "intro_zh": [
                "核心问题：点云中长程依赖建模困难，影响树木区分和生物量估计准确性",
                "方法要点：集成Mamba的选择性状态空间模型到Minkowski网络，增强全局上下文编码",
                "实验或效果：在丹麦国家森林清单数据上，显著优于现有方法，无需DTM且鲁棒"
            ],
            "tags_zh": [
                "点云处理",
                "选择性状态空间模型",
                "森林生物量估计",
                "LiDAR数据分析",
                "深度学习框架"
            ],
            "_index": 20
        },
        {
            "title": "A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI Synthesis",
            "authors": [
                "Valentin Biller",
                "Lucas Zimmer",
                "Can Erdur",
                "Sandeep Nagar",
                "Daniel Rückert",
                "Niklas Bubeck",
                "Jonas Weidner"
            ],
            "arxiv_id": "2510.09365v1",
            "summary": "Magnetic resonance imaging (MRI) inpainting supports numerous clinical and\nresearch applications. We introduce the first generative model that conditions\non voxel-level, continuous tumor concentrations to synthesize high-fidelity\nbrain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this\narchitecture to the complementary task of healthy tissue restoration by setting\nthe tumor concentrations to zero. Our latent diffusion model conditioned on\nboth tissue segmentations and the tumor concentrations generates 3D spatially\ncoherent and anatomically consistent images for both tumor synthesis and\nhealthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5,\nand for tumor inpainting, we achieve 17.4. Our code is available at:\nhttps://github.com/valentin-biller/ldm.git",
            "headline_zh": "提出基于生物物理条件的生成模型以合成3D脑肿瘤MRI和修复健康组织",
            "intro_zh": [
                "核心问题：MRI修复在临床和研究中应用广泛，但缺乏基于肿瘤浓度的生成模型。",
                "方法要点：使用潜在扩散模型，条件输入组织分割和肿瘤浓度，生成空间一致图像。",
                "实验或效果：健康组织修复PSNR为18.5，肿瘤修复为17.4，代码已开源。"
            ],
            "tags_zh": [
                "脑肿瘤MRI合成",
                "潜在扩散模型",
                "生物物理条件",
                "图像修复",
                "3D医学影像"
            ],
            "_index": 21
        },
        {
            "title": "Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes",
            "authors": [
                "Yikang Zhang",
                "Rui Fan"
            ],
            "arxiv_id": "2510.09364v1",
            "summary": "3D Gaussian splatting (3DGS) has demonstrated impressive performance in\nsynthesizing high-fidelity novel views. Nonetheless, its effectiveness\ncritically depends on the quality of the initialized point cloud. Specifically,\nachieving uniform and complete point coverage over the underlying scene\nstructure requires overlapping observation frustums, an assumption that is\noften violated in unbounded, dynamic urban environments. Training Gaussian\nmodels with partially initialized point clouds often leads to distortions and\nartifacts, as camera rays may fail to intersect valid surfaces, resulting in\nincorrect gradient propagation to Gaussian primitives associated with occluded\nor invisible geometry. Additionally, existing densification strategies simply\nclone and split Gaussian primitives from existing ones, incapable of\nreconstructing missing structures. To address these limitations, we propose\nVAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban\nscenes. Our method identifies unreliable geometry structures via voxel-based\nvisibility reasoning, selects informative supporting views through\ndiversity-aware view selection, and recovers missing structures via patch\nmatching-based multi-view stereo reconstruction. This design enables the\ngeneration of new Gaussian primitives guided by reliable geometric priors, even\nin regions lacking initial points. Extensive experiments on the Waymo and\nnuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS\napproaches and significantly improves the quality of reconstructed geometry for\nboth static and dynamic objects. Source code will be released upon publication.",
            "headline_zh": "提出VAD-GS框架以解决动态城市场景中3D高斯溅射的几何恢复问题",
            "intro_zh": [
                "核心问题：动态无界城市场景中，点云初始化不完整导致3D高斯溅射产生失真和伪影",
                "方法要点：通过体素可见性推理、多样性视图选择和补丁匹配多视图立体重建恢复缺失结构",
                "实验或效果：在Waymo和nuScenes数据集上优于现有方法，提升静态和动态对象重建质量"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "几何恢复",
                "动态城市场景",
                "多视图立体",
                "可见性推理",
                "点云初始化"
            ],
            "_index": 22
        },
        {
            "title": "BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception",
            "authors": [
                "Junyan Ye",
                "Dongzhi Jiang",
                "Jun He",
                "Baichuan Zhou",
                "Zilong Huang",
                "Zhiyuan Yan",
                "Hongsheng Li",
                "Conghui He",
                "Weijia Li"
            ],
            "arxiv_id": "2510.09361v1",
            "summary": "Recently, Multimodal Large Language Models (MLLMs) have made rapid progress,\nparticularly in enhancing their reasoning capabilities. However, existing\nreasoning benchmarks still primarily assess language-based reasoning, often\ntreating visual input as replaceable context. To address this gap, we introduce\nBLINK-Twice, a vision-centric reasoning benchmark grounded in challenging\nperceptual tasks. Instead of relying on external knowledge, our tasks require\nmodels to reason from visual content alone, shifting the focus from\nlanguage-based to image-grounded reasoning. Compared to prior perception\nbenchmarks, it moves beyond shallow perception (\"see\") and requires\nfine-grained observation and analytical reasoning (\"observe\"). BLINK-Twice\nintegrates three core components: seven types of visual challenges for testing\nvisual reasoning, natural adversarial image pairs that enforce reliance on\nvisual content, and annotated reasoning chains for fine-grained evaluation of\nthe reasoning process rather than final answers alone. We evaluate 20 leading\nMLLMs, including 12 foundation models and 8 reasoning-enhanced models.\nBLINK-Twice poses a significant challenge to current models. While existing\nreasoning strategies in the language space-such as chain-of-thought or\nself-criticism can improve performance, they often result in unstable and\nredundant reasoning. We observe that repeated image observation improves\nperformance across models, and active visual interaction, as demonstrated by\nmodels like o3, highlights the need for a new paradigm for vision reasoning.\nThe dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice",
            "headline_zh": "提出BLINK-Twice基准以评估多模态大语言模型的视觉感知推理能力",
            "intro_zh": [
                "现有基准多关注语言推理，视觉输入常被忽略，导致视觉推理评估不足",
                "基准基于视觉内容设计七类挑战，使用自然对抗图像对和标注推理链进行细粒度评估",
                "评估20个领先模型，发现现有推理策略不稳定，重复观察和主动视觉交互可提升性能"
            ],
            "tags_zh": [
                "视觉推理基准",
                "多模态大语言模型",
                "感知任务",
                "对抗图像对",
                "推理链评估",
                "视觉交互"
            ],
            "_index": 23
        },
        {
            "title": "Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models",
            "authors": [
                "Qihang Ma",
                "Shengyu Li",
                "Jie Tang",
                "Dingkang Yang",
                "Shaodong Chen",
                "Yingyi Zhang",
                "Chao Feng",
                "Jiao Ran"
            ],
            "arxiv_id": "2510.09358v1",
            "summary": "Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only\nmethods by incorporating multiple modalities of input information to produce a\nset of conclusive phrases. Traditional multi-modal approaches have been proven\nto have significant limitations in handling the challenging absence and unseen\nscenarios. Additionally, we identify shortcomings in existing benchmarks that\noverestimate model capability due to significant overlap in training tests. In\nthis work, we propose leveraging vision-language models (VLMs) for the MMKP\ntask. Firstly, we use two widely-used strategies, e.g., zero-shot and\nsupervised fine-tuning (SFT) to assess the lower bound performance of VLMs.\nNext, to improve the complex reasoning capabilities of VLMs, we adopt\nFine-tune-CoT, which leverages high-quality CoT reasoning data generated by a\nteacher model to finetune smaller models. Finally, to address the\n\"overthinking\" phenomenon, we propose a dynamic CoT strategy which adaptively\ninjects CoT data during training, allowing the model to flexibly leverage its\nreasoning capabilities during the inference stage. We evaluate the proposed\nstrategies on various datasets and the experimental results demonstrate the\neffectiveness of the proposed approaches. The code is available at\nhttps://github.com/bytedance/DynamicCoT.",
            "headline_zh": "提出动态思维链策略以提升多模态关键词预测在视觉语言模型中的性能",
            "intro_zh": [
                "多模态关键词预测面临模态缺失和未见场景的挑战，现有基准高估模型能力",
                "采用零样本和微调评估基线，引入Fine-tune-CoT和动态CoT增强推理能力",
                "实验验证动态CoT策略有效，代码已开源"
            ],
            "tags_zh": [
                "多模态关键词预测",
                "视觉语言模型",
                "思维链",
                "动态推理",
                "微调策略",
                "基准评估"
            ],
            "_index": 24
        },
        {
            "title": "Enhancing Infrared Vision: Progressive Prompt Fusion Network and Benchmark",
            "authors": [
                "Jinyuan Liu",
                "Zihang Chen",
                "Zhu Liu",
                "Zhiying Jiang",
                "Long Ma",
                "Xin Fan",
                "Risheng Liu"
            ],
            "arxiv_id": "2510.09343v1",
            "summary": "We engage in the relatively underexplored task named thermal infrared image\nenhancement. Existing infrared image enhancement methods primarily focus on\ntackling individual degradations, such as noise, contrast, and blurring, making\nit difficult to handle coupled degradations. Meanwhile, all-in-one enhancement\nmethods, commonly applied to RGB sensors, often demonstrate limited\neffectiveness due to the significant differences in imaging models. In sight of\nthis, we first revisit the imaging mechanism and introduce a Progressive Prompt\nFusion Network (PPFN). Specifically, the PPFN initially establishes prompt\npairs based on the thermal imaging process. For each type of degradation, we\nfuse the corresponding prompt pairs to modulate the model's features, providing\nadaptive guidance that enables the model to better address specific\ndegradations under single or multiple conditions. In addition, a Selective\nProgressive Training (SPT) mechanism is introduced to gradually refine the\nmodel's handling of composite cases to align the enhancement process, which not\nonly allows the model to remove camera noise and retain key structural details,\nbut also enhancing the overall contrast of the thermal image. Furthermore, we\nintroduce the most high-quality, multi-scenarios infrared benchmark covering a\nwide range of scenarios. Extensive experiments substantiate that our approach\nnot only delivers promising visual results under specific degradation but also\nsignificantly improves performance on complex degradation scenes, achieving a\nnotable 8.76\\% improvement. Code is available at\nhttps://github.com/Zihang-Chen/HM-TIR.",
            "headline_zh": "提出渐进提示融合网络以增强热红外图像，处理耦合退化问题。",
            "intro_zh": [
                "核心问题：现有方法难以处理热红外图像中的耦合退化，如噪声、对比度和模糊。",
                "方法要点：基于热成像过程建立提示对，融合以调制特征，自适应处理单或多退化。",
                "实验或效果：在复杂退化场景中性能提升8.76%，并引入高质量多场景基准。"
            ],
            "tags_zh": [
                "热红外图像增强",
                "渐进提示融合网络",
                "耦合退化处理",
                "选择性渐进训练",
                "红外基准数据集"
            ],
            "_index": 25
        },
        {
            "title": "Efficient Bayesian Inference from Noisy Pairwise Comparisons",
            "authors": [
                "Till Aczel",
                "Lucas Theis",
                "Wattenhofer Roger"
            ],
            "arxiv_id": "2510.09333v1",
            "summary": "Evaluating generative models is challenging because standard metrics often\nfail to reflect human preferences. Human evaluations are more reliable but\ncostly and noisy, as participants vary in expertise, attention, and diligence.\nPairwise comparisons improve consistency, yet aggregating them into overall\nquality scores requires careful modeling. Bradley-Terry-based methods update\nitem scores from comparisons, but existing approaches either ignore rater\nvariability or lack convergence guarantees, limiting robustness and\ninterpretability. We introduce BBQ, a Bayesian Bradley-Terry variant that\nexplicitly models rater quality, downweighting or removing unreliable\nparticipants, and provides guaranteed monotonic likelihood convergence through\nan Expectation-Maximization algorithm. Empirical results show that BBQ achieves\nfaster convergence, well-calibrated uncertainty estimates, and more robust,\ninterpretable rankings compared to baseline Bradley-Terry models, even with\nnoisy or crowdsourced raters. This framework enables more reliable and\ncost-effective human evaluation of generative models.",
            "headline_zh": "提出BBQ贝叶斯Bradley-Terry模型以解决生成模型评估中噪声成对比较的聚合问题",
            "intro_zh": [
                "核心问题：生成模型评估中人类评价成本高、噪声大，标准指标难以反映偏好",
                "方法要点：BBQ模型显式建模评分者质量，使用EM算法保证单调似然收敛",
                "实验或效果：BBQ收敛更快，提供校准不确定性估计和鲁棒排名，优于基线模型"
            ],
            "tags_zh": [
                "贝叶斯推断",
                "成对比较",
                "Bradley-Terry模型",
                "生成模型评估",
                "EM算法",
                "评分者质量建模"
            ],
            "_index": 26
        },
        {
            "title": "Instance-Aware Robust Consistency Regularization for Semi-Supervised Nuclei Instance Segmentation",
            "authors": [
                "Zenan Lin",
                "Wei Li",
                "Jintao Chen",
                "Zihao Wu",
                "Wenxiong Kang",
                "Changxin Gao",
                "Liansheng Wang",
                "Jin-Gang Yu"
            ],
            "arxiv_id": "2510.09329v1",
            "summary": "Nuclei instance segmentation in pathological images is crucial for downstream\ntasks such as tumor microenvironment analysis. However, the high cost and\nscarcity of annotated data limit the applicability of fully supervised methods,\nwhile existing semi-supervised methods fail to adequately regularize\nconsistency at the instance level, lack leverage of the inherent prior\nknowledge of pathological structures, and are prone to introducing noisy\npseudo-labels during training. In this paper, we propose an Instance-Aware\nRobust Consistency Regularization Network (IRCR-Net) for accurate\ninstance-level nuclei segmentation. Specifically, we introduce the\nMatching-Driven Instance-Aware Consistency (MIAC) and Prior-Driven\nInstance-Aware Consistency (PIAC) mechanisms to refine the nuclei instance\nsegmentation result of the teacher and student subnetwork, particularly for\ndensely distributed and overlapping nuclei. We incorporate morphological prior\nknowledge of nuclei in pathological images and utilize these priors to assess\nthe quality of pseudo-labels generated from unlabeled data. Low-quality\npseudo-labels are discarded, while high-quality predictions are enhanced to\nreduce pseudo-label noise and benefit the network's robust training.\nExperimental results demonstrate that the proposed method significantly\nenhances semi-supervised nuclei instance segmentation performance across\nmultiple public datasets compared to existing approaches, even surpassing fully\nsupervised methods in some scenarios.",
            "headline_zh": "提出IRCR-Net以解决半监督细胞核实例分割中的实例级一致性问题",
            "intro_zh": [
                "核心问题：半监督方法在细胞核实例分割中缺乏实例级一致性正则化，易产生噪声伪标签",
                "方法要点：引入MIAC和PIAC机制，利用形态先验知识优化伪标签质量，减少噪声",
                "实验或效果：在多个公开数据集上性能显著提升，部分场景超越全监督方法"
            ],
            "tags_zh": [
                "半监督学习",
                "细胞核实例分割",
                "一致性正则化",
                "伪标签优化",
                "病理图像分析"
            ],
            "_index": 27
        },
        {
            "title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation",
            "authors": [
                "Wenyao Zhang",
                "Hongsi Liu",
                "Bohan Li",
                "Jiawei He",
                "Zekun Qi",
                "Yunnan Wang",
                "Shengyang Zhao",
                "Xinqiang Yu",
                "Wenjun Zeng",
                "Xin Jin"
            ],
            "arxiv_id": "2510.09320v1",
            "summary": "Current self-supervised monocular depth estimation (MDE) approaches encounter\nperformance limitations due to insufficient semantic-spatial knowledge\nextraction. To address this challenge, we propose Hybrid-depth, a novel\nframework that systematically integrates foundation models (e.g., CLIP and\nDINO) to extract visual priors and acquire sufficient contextual information\nfor MDE. Our approach introduces a coarse-to-fine progressive learning\nframework: 1) Firstly, we aggregate multi-grained features from CLIP (global\nsemantics) and DINO (local spatial details) under contrastive language\nguidance. A proxy task comparing close-distant image patches is designed to\nenforce depth-aware feature alignment using text prompts; 2) Next, building on\nthe coarse features, we integrate camera pose information and pixel-wise\nlanguage alignment to refine depth predictions. This module seamlessly\nintegrates with existing self-supervised MDE pipelines (e.g., Monodepth2,\nManyDepth) as a plug-and-play depth encoder, enhancing continuous depth\nestimation. By aggregating CLIP's semantic context and DINO's spatial details\nthrough language guidance, our method effectively addresses feature granularity\nmismatches. Extensive experiments on the KITTI benchmark demonstrate that our\nmethod significantly outperforms SOTA methods across all metrics, which also\nindeed benefits downstream tasks like BEV perception. Code is available at\nhttps://github.com/Zhangwenyao1/Hybrid-depth.",
            "headline_zh": "提出混合粒度特征聚合框架，通过语言引导解决自监督单目深度估计中语义-空间知识不足问题",
            "intro_zh": [
                "核心问题：自监督单目深度估计中语义-空间知识提取不足，导致性能受限",
                "方法要点：整合CLIP和DINO模型，通过粗到细语言引导聚合多粒度特征",
                "实验或效果：在KITTI基准上显著超越SOTA方法，并提升下游任务如BEV感知"
            ],
            "tags_zh": [
                "自监督单目深度估计",
                "混合粒度特征聚合",
                "语言引导学习",
                "CLIP模型",
                "DINO模型",
                "KITTI基准"
            ],
            "_index": 28
        },
        {
            "title": "RadioFlow: Efficient Radio Map Construction Framework with Flow Matching",
            "authors": [
                "Haozhe Jia",
                "Wenshuo Chen",
                "Xiucheng Wang",
                "Nan Cheng",
                "Hongbo Zhang",
                "Kuimou Yu",
                "Songning Lai",
                "Nanjian Jia",
                "Bowen Tian",
                "Hongru Xiao",
                "Yutao Yue"
            ],
            "arxiv_id": "2510.09314v1",
            "summary": "Accurate and real-time radio map (RM) generation is crucial for\nnext-generation wireless systems, yet diffusion-based approaches often suffer\nfrom large model sizes, slow iterative denoising, and high inference latency,\nwhich hinder practical deployment. To overcome these limitations, we propose\n\\textbf{RadioFlow}, a novel flow-matching-based generative framework that\nachieves high-fidelity RM generation through single-step efficient sampling.\nUnlike conventional diffusion models, RadioFlow learns continuous transport\ntrajectories between noise and data, enabling both training and inference to be\nsignificantly accelerated while preserving reconstruction accuracy.\nComprehensive experiments demonstrate that RadioFlow achieves state-of-the-art\nperformance with \\textbf{up to 8$\\times$ fewer parameters} and \\textbf{over\n4$\\times$ faster inference} compared to the leading diffusion-based baseline\n(RadioDiff). This advancement provides a promising pathway toward scalable,\nenergy-efficient, and real-time electromagnetic digital twins for future 6G\nnetworks. We release the code at\n\\href{https://github.com/Hxxxz0/RadioFlow}{GitHub}.",
            "headline_zh": "提出RadioFlow框架，通过流匹配实现高效无线地图构建，解决扩散模型参数大、推理慢问题。",
            "intro_zh": [
                "扩散模型在无线地图生成中参数大、迭代去噪慢，推理延迟高，阻碍实际部署。",
                "采用流匹配方法学习噪声到数据的连续传输轨迹，实现单步高效采样。",
                "实验显示参数减少8倍、推理加速4倍，保持高精度，适用于6G网络数字孪生。"
            ],
            "tags_zh": [
                "无线地图构建",
                "流匹配生成",
                "高效采样",
                "6G网络",
                "数字孪生",
                "推理加速"
            ],
            "_index": 29
        },
        {
            "title": "Rewiring Development in Brain Segmentation: Leveraging Adult Brain Priors for Enhancing Infant MRI Segmentation",
            "authors": [
                "Alemu Sisay Nigru",
                "Michele Svanera",
                "Austin Dibble",
                "Connor Dalby",
                "Mattia Savardi",
                "Sergio Benini"
            ],
            "arxiv_id": "2510.09306v1",
            "summary": "Accurate segmentation of infant brain MRI is critical for studying early\nneurodevelopment and diagnosing neurological disorders. Yet, it remains a\nfundamental challenge due to continuously evolving anatomy of the subjects,\nmotion artifacts, and the scarcity of high-quality labeled data. In this work,\nwe present LODi, a novel framework that utilizes prior knowledge from an adult\nbrain MRI segmentation model to enhance the segmentation performance of infant\nscans. Given the abundance of publicly available adult brain MRI data, we\npre-train a segmentation model on a large adult dataset as a starting point.\nThrough transfer learning and domain adaptation strategies, we progressively\nadapt the model to the 0-2 year-old population, enabling it to account for the\nanatomical and imaging variability typical of infant scans. The adaptation of\nthe adult model is carried out using weakly supervised learning on infant brain\nscans, leveraging silver-standard ground truth labels obtained with FreeSurfer.\nBy introducing a novel training strategy that integrates hierarchical feature\nrefinement and multi-level consistency constraints, our method enables fast,\naccurate, age-adaptive segmentation, while mitigating scanner and site-specific\nbiases. Extensive experiments on both internal and external datasets\ndemonstrate the superiority of our approach over traditional supervised\nlearning and domain-specific models. Our findings highlight the advantage of\nleveraging adult brain priors as a foundation for age-flexible neuroimaging\nanalysis, paving the way for more reliable and generalizable brain MRI\nsegmentation across the lifespan.",
            "headline_zh": "提出LODi框架，利用成人脑MRI先验增强婴儿脑MRI分割性能",
            "intro_zh": [
                "婴儿脑MRI分割面临解剖变化、运动伪影和标注数据稀缺等挑战",
                "通过成人脑分割模型预训练，结合迁移学习和弱监督适应婴儿数据",
                "实验显示优于传统监督学习和领域特定模型，提升分割准确性和泛化性"
            ],
            "tags_zh": [
                "脑MRI分割",
                "迁移学习",
                "弱监督学习",
                "婴儿神经影像",
                "领域适应",
                "特征精炼"
            ],
            "_index": 30
        },
        {
            "title": "CapGeo: A Caption-Assisted Approach to Geometric Reasoning",
            "authors": [
                "Yuying Li",
                "Siyi Qian",
                "Hao Liang",
                "Leqi Zheng",
                "Ruichuan An",
                "Yongzhen Guo",
                "Wentao Zhang"
            ],
            "arxiv_id": "2510.09302v1",
            "summary": "Geometric reasoning remains a core challenge for Multimodal Large Language\nModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3\nand Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite\nexhibiting strong textual reasoning abilities on tasks like the International\nMathematical Olympiad (IMO). This gap suggests that the bottleneck lies in\nunderstanding geometric diagrams rather than reasoning itself. Since geometric\nfigures can often be faithfully described in concise textual form, converting\nvisual content into captions offers a promising direction. Motivated by this\ninsight, we introduce CapGeo, a caption-assisted reasoning framework that\nbridges visual and textual modalities. Experiments show substantial\nimprovements when models are equipped with captions: Qwen2.5-VL-72B improves\nfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to\n73.0%. To systematically evaluate and identify high-quality geometric\ncaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated\nfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based\nevaluation metric that correlates strongly with downstream CapGeo performance,\nenabling reliable assessment of geometric captioning ability. Together, our\nframework and benchmark highlight a new pathway toward advancing geometric\nreasoning in MLLMs.",
            "headline_zh": "提出CapGeo框架，通过几何图描述辅助多模态大模型解决几何推理问题",
            "intro_zh": [
                "核心问题：多模态大模型在几何推理中难以可靠理解几何图，瓶颈在于视觉内容理解",
                "方法要点：引入CapGeo框架，将几何图转换为文本描述，桥接视觉与文本模态",
                "实验或效果：在基准测试中，模型性能显著提升，如Qwen2.5-VL-72B从8.6%升至59.0%"
            ],
            "tags_zh": [
                "几何推理",
                "多模态大模型",
                "图描述生成",
                "基准数据集",
                "性能评估"
            ],
            "_index": 31
        },
        {
            "title": "Foraging with the Eyes: Dynamics in Human Visual Gaze and Deep Predictive Modeling",
            "authors": [
                "Tejaswi V. Panchagnula"
            ],
            "arxiv_id": "2510.09299v1",
            "summary": "Animals often forage via Levy walks stochastic trajectories with heavy tailed\nstep lengths optimized for sparse resource environments. We show that human\nvisual gaze follows similar dynamics when scanning images. While traditional\nmodels emphasize image based saliency, the underlying spatiotemporal statistics\nof eye movements remain underexplored. Understanding these dynamics has broad\napplications in attention modeling and vision-based interfaces. In this study,\nwe conducted a large scale human subject experiment involving 40 participants\nviewing 50 diverse images under unconstrained conditions, recording over 4\nmillion gaze points using a high speed eye tracker. Analysis of these data\nshows that the gaze trajectory of the human eye also follows a Levy walk akin\nto animal foraging. This suggests that the human eye forages for visual\ninformation in an optimally efficient manner. Further, we trained a\nconvolutional neural network (CNN) to predict fixation heatmaps from image\ninput alone. The model accurately reproduced salient fixation regions across\nnovel images, demonstrating that key components of gaze behavior are learnable\nfrom visual structure alone. Our findings present new evidence that human\nvisual exploration obeys statistical laws analogous to natural foraging and\nopen avenues for modeling gaze through generative and predictive frameworks.",
            "headline_zh": "揭示人类视觉注视遵循莱维行走动态，并训练CNN预测注视热图。",
            "intro_zh": [
                "核心问题：人类视觉注视的时空统计动态是否类似动物觅食的莱维行走。",
                "方法要点：分析大规模眼动数据，训练卷积神经网络预测注视热图。",
                "实验或效果：40名参与者观看50张图像，模型能准确预测新图像的注视区域。"
            ],
            "tags_zh": [
                "视觉注视动态",
                "莱维行走",
                "眼动追踪",
                "卷积神经网络",
                "注视预测",
                "视觉探索"
            ],
            "_index": 32
        },
        {
            "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
            "authors": [
                "Siyuan Huang",
                "Xiaoye Qu",
                "Yafu Li",
                "Yun Luo",
                "Zefeng He",
                "Daizong Liu",
                "Yu Cheng"
            ],
            "arxiv_id": "2510.09285v1",
            "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs.",
            "headline_zh": "提出视觉感知策略优化以增强多模态强化学习中的视觉推理能力",
            "intro_zh": [
                "核心问题：多模态强化学习中视觉感知在优化过程中被忽视，影响推理准确性。",
                "方法要点：引入令牌感知度量视觉依赖，通过重加权优势和聚焦关键令牌优化策略。",
                "实验或效果：在八个基准测试中显著优于现有模型，适用于7B和32B规模。"
            ],
            "tags_zh": [
                "多模态强化学习",
                "令牌感知",
                "视觉依赖",
                "策略优化",
                "推理基准"
            ],
            "_index": 33
        },
        {
            "title": "MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel Understanding",
            "authors": [
                "Ming Dai",
                "Sen Yang",
                "Boqiang Duan",
                "Wankou Yang",
                "Jingdong Wang"
            ],
            "arxiv_id": "2510.09274v1",
            "summary": "Referring Video Object Segmentation (RefVOS) seeks to segment target objects\nin videos guided by natural language descriptions, demanding both temporal\nreasoning and fine-grained visual comprehension. Existing sampling strategies\nfor LLM-based approaches typically rely on either handcrafted heuristics or\nexternal keyframe models. The former often overlooks essential temporal cues,\nwhile the latter increases system complexity. To address this, we propose a\nunified framework that jointly optimizes Temporal Sentence Grounding (TSG) and\nRefVOS, naturally incorporating key moment grounding capability. During\ntraining, we introduce a novel TSG paradigm that employs a dedicated\n\\texttt{[FIND]} token for key moment identification through temporal token\nsimilarity matching, thereby avoiding the need for external timestamp\nencodings. For inference, we design a Moment-Centric Sampling (MCS) strategy\nthat densely samples informative moments while sparsely sampling non-essential\nframes, preserving both motion details and global context. To further enhance\ntracking stability, we develop Bidirectional Anchor-updated Propagation (BAP),\nwhich leverages the most relevant moment as start point for high-quality mask\ninitialization and dynamically updates at sampled points to mitigate\naccumulated errors. Code and model will be available at:\nhttps://github.com/Dmmm1997/MomentSeg",
            "headline_zh": "提出MomentSeg框架，通过关键时刻采样优化视频像素理解，解决RefVOS中的时序推理问题。",
            "intro_zh": [
                "核心问题：现有RefVOS采样策略忽略时序线索或增加系统复杂性。",
                "方法要点：联合优化TSG和RefVOS，使用FIND令牌识别关键时刻。",
                "实验或效果：未知，但代码和模型将开源。"
            ],
            "tags_zh": [
                "视频对象分割",
                "时序句子定位",
                "关键时刻采样",
                "双向锚点传播",
                "语言引导分割"
            ],
            "_index": 34
        },
        {
            "title": "Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects",
            "authors": [
                "Zirun Zhou",
                "Zhengyang Xiao",
                "Haochuan Xu",
                "Jing Sun",
                "Di Wang",
                "Jingfeng Zhang"
            ],
            "arxiv_id": "2510.09269v1",
            "summary": "Recent advances in vision-language-action (VLA) models have greatly improved\nembodied AI, enabling robots to follow natural language instructions and\nperform diverse tasks. However, their reliance on uncurated training datasets\nraises serious security concerns. Existing backdoor attacks on VLAs mostly\nassume white-box access and result in task failures instead of enforcing\nspecific actions. In this work, we reveal a more practical threat: attackers\ncan manipulate VLAs by simply injecting physical objects as triggers into the\ntraining dataset. We propose goal-oriented backdoor attacks (GoBA), where the\nVLA behaves normally in the absence of physical triggers but executes\npredefined and goal-oriented actions in the presence of physical triggers.\nSpecifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO\nthat incorporates diverse physical triggers and goal-oriented backdoor actions.\nIn addition, we propose a three-level evaluation that categorizes the victim\nVLA's actions under GoBA into three states: nothing to do, try to do, and\nsuccess to do. Experiments show that GoBA enables the victim VLA to\nsuccessfully achieve the backdoor goal in 97 percentage of inputs when the\nphysical trigger is present, while causing zero performance degradation on\nclean inputs. Finally, by investigating factors related to GoBA, we find that\nthe action trajectory and trigger color significantly influence attack\nperformance, while trigger size has surprisingly little effect. The code and\nBadLIBERO dataset are accessible via the project page at\nhttps://goba-attack.github.io/.",
            "headline_zh": "提出目标导向后门攻击，通过物理对象操纵视觉-语言-动作模型",
            "intro_zh": [
                "视觉-语言-动作模型依赖未筛选数据集，存在安全风险，现有攻击多为白盒且导致任务失败",
                "引入物理对象作为触发器，在触发时执行预设目标动作，不影响正常输入性能",
                "实验显示攻击成功率97%，动作轨迹和触发器颜色显著影响性能，代码和数据集已公开"
            ],
            "tags_zh": [
                "后门攻击",
                "视觉-语言-动作模型",
                "物理触发器",
                "目标导向动作",
                "安全评估",
                "数据集污染"
            ],
            "_index": 35
        },
        {
            "title": "Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy",
            "authors": [
                "Patrick Wienholt",
                "Sophie Caselitz",
                "Robert Siepmann",
                "Philipp Bruners",
                "Keno Bressem",
                "Christiane Kuhl",
                "Jakob Nikolas Kather",
                "Sven Nebelung",
                "Daniel Truhn"
            ],
            "arxiv_id": "2510.09256v1",
            "summary": "To determine whether using discrete semantic entropy (DSE) to reject\nquestions likely to generate hallucinations can improve the accuracy of\nblack-box vision-language models (VLMs) in radiologic image based visual\nquestion answering (VQA). This retrospective study evaluated DSE using two\npublicly available, de-identified datasets: (i) the VQA-Med 2019 benchmark (500\nimages with clinical questions and short-text answers) and (ii) a diagnostic\nradiology dataset (206 cases: 60 computed tomography scans, 60 magnetic\nresonance images, 60 radiographs, 26 angiograms) with corresponding\nground-truth diagnoses. GPT-4o and GPT-4.1 answered each question 15 times\nusing a temperature of 1.0. Baseline accuracy was determined using\nlow-temperature answers (temperature 0.1). Meaning-equivalent responses were\ngrouped using bidirectional entailment checks, and DSE was computed from the\nrelative frequencies of the resulting semantic clusters. Accuracy was\nrecalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and\n95% confidence intervals were obtained using bootstrap resampling and a\nBonferroni-corrected threshold of p < .004 for statistical significance. Across\n706 image-question pairs, baseline accuracy was 51.7% for GPT-4o and 54.8% for\nGPT-4.1. After filtering out high-entropy questions (DSE > 0.3), accuracy on\nthe remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and\n63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains\nwere observed across both datasets and largely remained statistically\nsignificant after Bonferroni correction. DSE enables reliable hallucination\ndetection in black-box VLMs by quantifying semantic inconsistency. This method\nsignificantly improves diagnostic answer accuracy and offers a filtering\nstrategy for clinical VLM applications.",
            "headline_zh": "提出离散语义熵以过滤放射学视觉语言模型中的幻觉，提升诊断准确性",
            "intro_zh": [
                "核心问题：黑盒视觉语言模型在放射学视觉问答中易产生幻觉，影响诊断准确性。",
                "方法要点：使用离散语义熵量化语义不一致性，过滤高熵问题以减少幻觉。",
                "实验效果：在706个图像-问题对中，过滤后GPT-4o准确率从51.7%提升至76.3%。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "放射学视觉问答",
                "离散语义熵",
                "幻觉过滤",
                "诊断准确性"
            ],
            "_index": 36
        },
        {
            "title": "Zero-shot image privacy classification with Vision-Language Models",
            "authors": [
                "Alina Elena Baia",
                "Alessio Xompero",
                "Andrea Cavallaro"
            ],
            "arxiv_id": "2510.09253v1",
            "summary": "While specialized learning-based models have historically dominated image\nprivacy prediction, the current literature increasingly favours adopting large\nVision-Language Models (VLMs) designed for generic tasks. This trend risks\noverlooking the performance ceiling set by purpose-built models due to a lack\nof systematic evaluation. To address this problem, we establish a zero-shot\nbenchmark for image privacy classification, enabling a fair comparison. We\nevaluate the top-3 open-source VLMs, according to a privacy benchmark, using\ntask-aligned prompts and we contrast their performance, efficiency, and\nrobustness against established vision-only and multi-modal methods.\nCounter-intuitively, our results show that VLMs, despite their\nresource-intensive nature in terms of high parameter count and slower\ninference, currently lag behind specialized, smaller models in privacy\nprediction accuracy. We also find that VLMs exhibit higher robustness to image\nperturbations.",
            "headline_zh": "建立零样本基准以公平比较视觉语言模型在图像隐私分类中的性能。",
            "intro_zh": [
                "核心问题：缺乏系统评估，视觉语言模型在图像隐私预测中可能被高估。",
                "方法要点：使用任务对齐提示评估开源视觉语言模型，对比专业模型。",
                "实验或效果：视觉语言模型精度较低但鲁棒性更高，资源消耗大。"
            ],
            "tags_zh": [
                "图像隐私分类",
                "视觉语言模型",
                "零样本学习",
                "基准评估",
                "鲁棒性分析"
            ],
            "_index": 37
        },
        {
            "title": "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras",
            "authors": [
                "Jindong Hong",
                "Wencheng Zhang",
                "Shiqin Qiao",
                "Jianhai Chen",
                "Jianing Qiu",
                "Chuanyang Zheng",
                "Qian Xu",
                "Yun Ji",
                "Qianyue Wen",
                "Weiwei Sun",
                "Hao Li",
                "Huizhen Li",
                "Huichao Wang",
                "Kai Wu",
                "Meng Li",
                "Yijun He",
                "Lingjie Luo",
                "Jiankai Sun"
            ],
            "arxiv_id": "2510.09230v1",
            "summary": "Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),\nare common conditions affecting the health of people worldwide, and have a high\nincidence rate among the elderly and workers engaged in repetitive shoulder\ntasks. In regions with scarce medical resources, achieving early and accurate\ndiagnosis poses significant challenges, and there is an urgent need for\nlow-cost and easily scalable auxiliary diagnostic solutions. This research\nintroduces videos captured by consumer-grade devices as the basis for\ndiagnosis, reducing the cost for users. We focus on the innovative application\nof Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of\nshoulder disorders and propose a Hybrid Motion Video Diagnosis framework\n(HMVDx). This framework divides the two tasks of action understanding and\ndisease diagnosis, which are respectively completed by two MLLMs. In addition\nto traditional evaluation indicators, this work proposes a novel metric called\nUsability Index by the logical process of medical decision-making (action\nrecognition, movement diagnosis, and final diagnosis). This index evaluates the\neffectiveness of MLLMs in the medical field from the perspective of the entire\nmedical diagnostic pathway, revealing the potential value of low-cost MLLMs in\nmedical applications for medical practitioners. In experimental comparisons,\nthe accuracy of HMVDx in diagnosing shoulder joint injuries has increased by\n79.6\\% compared with direct video diagnosis, a significant technical\ncontribution to future research on the application of MLLMs for video\nunderstanding in the medical field.",
            "headline_zh": "提出HMVDx框架，使用消费级摄像头和MLLMs辅助诊断肩部疾病。",
            "intro_zh": [
                "核心问题：医疗资源稀缺地区肩部疾病早期诊断成本高、难度大。",
                "方法要点：HMVDx框架分动作理解和疾病诊断两任务，由两个MLLMs完成。",
                "实验或效果：HMVDx诊断准确率比直接视频诊断提升79.6%。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "肩部疾病诊断",
                "消费级摄像头",
                "视频理解",
                "医疗辅助诊断"
            ],
            "_index": 38
        },
        {
            "title": "Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation",
            "authors": [
                "Vijay M. Galshetwar",
                "Praful Hambarde",
                "Prashant W. Patil",
                "Akshay Dudhane",
                "Sachin Chaudhary",
                "Santosh Kumar Vipparathi",
                "Subrahmanyam Murala"
            ],
            "arxiv_id": "2510.09228v1",
            "summary": "Adverse weather conditions such as haze, rain, and snow significantly degrade\nthe quality of images and videos, posing serious challenges to intelligent\ntransportation systems (ITS) that rely on visual input. These degradations\naffect critical applications including autonomous driving, traffic monitoring,\nand surveillance. This survey presents a comprehensive review of image and\nvideo restoration techniques developed to mitigate weather-induced visual\nimpairments. We categorize existing approaches into traditional prior-based\nmethods and modern data-driven models, including CNNs, transformers, diffusion\nmodels, and emerging vision-language models (VLMs). Restoration strategies are\nfurther classified based on their scope: single-task models,\nmulti-task/multi-weather systems, and all-in-one frameworks capable of handling\ndiverse degradations. In addition, we discuss day and night time restoration\nchallenges, benchmark datasets, and evaluation protocols. The survey concludes\nwith an in-depth discussion on limitations in current research and outlines\nfuture directions such as mixed/compound-degradation restoration, real-time\ndeployment, and agentic AI frameworks. This work aims to serve as a valuable\nreference for advancing weather-resilient vision systems in smart\ntransportation environments. Lastly, to stay current with rapid advancements in\nthis field, we will maintain regular updates of the latest relevant papers and\ntheir open-source implementations at\nhttps://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration",
            "headline_zh": "综述多天气图像视频恢复技术以提升智能交通系统视觉输入质量",
            "intro_zh": [
                "核心问题：雾霾、雨雪等恶劣天气导致图像视频质量下降，影响自动驾驶和交通监控。",
                "方法要点：分类传统先验方法和现代数据驱动模型，包括CNN、Transformer和扩散模型。",
                "实验或效果：讨论基准数据集、评估协议，并指出未来方向如混合降解恢复和实时部署。"
            ],
            "tags_zh": [
                "图像恢复",
                "视频恢复",
                "多天气处理",
                "智能交通系统",
                "数据驱动模型",
                "基准数据集"
            ],
            "_index": 39
        },
        {
            "title": "Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation",
            "authors": [
                "Wangyu Wu",
                "Xuhang Chen",
                "Zhenhong Chen",
                "Jing-En Jiang",
                "Kim-Fung Tsang",
                "Xiaowei Huang",
                "Fei Ma",
                "Jimin Xiao"
            ],
            "arxiv_id": "2510.09224v1",
            "summary": "Cross-Domain Sequential Recommendation (CDSR) plays a crucial role in modern\nconsumer electronics and e-commerce platforms, where users interact with\ndiverse services such as books, movies, and online retail products. These\nsystems must accurately capture both domain-specific and cross-domain\nbehavioral patterns to provide personalized and seamless consumer experiences.\nTo address this challenge, we propose \\textbf{TEMA-LLM} (\\textit{Tag-Enriched\nMulti-Attention with Large Language Models}), a practical and effective\nframework that integrates \\textit{Large Language Models (LLMs)} for semantic\ntag generation and enrichment. Specifically, TEMA-LLM employs LLMs to assign\ndomain-aware prompts and generate descriptive tags from item titles and\ndescriptions. The resulting tag embeddings are fused with item identifiers as\nwell as textual and visual features to construct enhanced item representations.\nA \\textit{Tag-Enriched Multi-Attention} mechanism is then introduced to jointly\nmodel user preferences within and across domains, enabling the system to\ncapture complex and evolving consumer interests. Extensive experiments on four\nlarge-scale e-commerce datasets demonstrate that TEMA-LLM consistently\noutperforms state-of-the-art baselines, underscoring the benefits of LLM-based\nsemantic tagging and multi-attention integration for consumer-facing\nrecommendation systems. The proposed approach highlights the potential of LLMs\nto advance intelligent, user-centric services in the field of consumer\nelectronics.",
            "headline_zh": "提出TEMA-LLM框架，利用大语言模型生成语义标签以提升跨域序列推荐性能",
            "intro_zh": [
                "跨域序列推荐需捕捉域内和跨域用户行为模式，以提供个性化体验",
                "集成大语言模型生成描述性标签，结合多特征构建增强物品表示",
                "在四个电商数据集上实验，TEMA-LLM优于现有基线方法"
            ],
            "tags_zh": [
                "跨域序列推荐",
                "大语言模型",
                "语义标签生成",
                "多注意力机制",
                "物品表示增强"
            ],
            "_index": 40
        },
        {
            "title": "Stable Video Infinity: Infinite-Length Video Generation with Error Recycling",
            "authors": [
                "Wuyang Li",
                "Wentao Pan",
                "Po-Chien Luan",
                "Yang Gao",
                "Alexandre Alahi"
            ],
            "arxiv_id": "2510.09212v1",
            "summary": "We propose Stable Video Infinity (SVI) that is able to generate\ninfinite-length videos with high temporal consistency, plausible scene\ntransitions, and controllable streaming storylines. While existing long-video\nmethods attempt to mitigate accumulated errors via handcrafted anti-drifting\n(e.g., modified noise scheduler, frame anchoring), they remain limited to\nsingle-prompt extrapolation, producing homogeneous scenes with repetitive\nmotions. We identify that the fundamental challenge extends beyond error\naccumulation to a critical discrepancy between the training assumption (seeing\nclean data) and the test-time autoregressive reality (conditioning on\nself-generated, error-prone outputs). To bridge this hypothesis gap, SVI\nincorporates Error-Recycling Fine-Tuning, a new type of efficient training that\nrecycles the Diffusion Transformer (DiT)'s self-generated errors into\nsupervisory prompts, thereby encouraging DiT to actively identify and correct\nits own errors. This is achieved by injecting, collecting, and banking errors\nthrough closed-loop recycling, autoregressively learning from error-injected\nfeedback. Specifically, we (i) inject historical errors made by DiT to\nintervene on clean inputs, simulating error-accumulated trajectories in flow\nmatching; (ii) efficiently approximate predictions with one-step bidirectional\nintegration and calculate errors with residuals; (iii) dynamically bank errors\ninto replay memory across discretized timesteps, which are resampled for new\ninput. SVI is able to scale videos from seconds to infinite durations with no\nadditional inference cost, while remaining compatible with diverse conditions\n(e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks,\nincluding consistent, creative, and conditional settings, thoroughly verifying\nits versatility and state-of-the-art role.",
            "headline_zh": "提出Stable Video Infinity，通过错误回收微调实现无限长度视频生成",
            "intro_zh": [
                "核心问题：训练假设与自回归生成间的误差累积和场景同质化问题",
                "方法要点：采用错误回收微调，将自生成错误作为监督提示进行训练",
                "实验或效果：在多个基准测试中验证其多功能性和先进性能"
            ],
            "tags_zh": [
                "无限视频生成",
                "错误回收微调",
                "扩散变换器",
                "自回归学习",
                "流匹配"
            ],
            "_index": 41
        },
        {
            "title": "3D Reconstruction from Transient Measurements with Time-Resolved Transformer",
            "authors": [
                "Yue Li",
                "Shida Sun",
                "Yu Hong",
                "Feihu Xu",
                "Zhiwei Xiong"
            ],
            "arxiv_id": "2510.09205v1",
            "summary": "Transient measurements, captured by the timeresolved systems, are widely\nemployed in photon-efficient reconstruction tasks, including line-of-sight\n(LOS) and non-line-of-sight (NLOS) imaging. However, challenges persist in\ntheir 3D reconstruction due to the low quantum efficiency of sensors and the\nhigh noise levels, particularly for long-range or complex scenes. To boost the\n3D reconstruction performance in photon-efficient imaging, we propose a generic\nTime-Resolved Transformer (TRT) architecture. Different from existing\ntransformers designed for high-dimensional data, TRT has two elaborate\nattention designs tailored for the spatio-temporal transient measurements.\nSpecifically, the spatio-temporal self-attention encoders explore both local\nand global correlations within transient data by splitting or downsampling\ninput features into different scales. Then, the spatio-temporal cross attention\ndecoders integrate the local and global features in the token space, resulting\nin deep features with high representation capabilities. Building on TRT, we\ndevelop two task-specific embodiments: TRT-LOS for LOS imaging and TRT-NLOS for\nNLOS imaging. Extensive experiments demonstrate that both embodiments\nsignificantly outperform existing methods on synthetic data and real-world data\ncaptured by different imaging systems. In addition, we contribute a\nlarge-scale, high-resolution synthetic LOS dataset with various noise levels\nand capture a set of real-world NLOS measurements using a custom-built imaging\nsystem, enhancing the data diversity in this field. Code and datasets are\navailable at https://github.com/Depth2World/TRT.",
            "headline_zh": "提出时间分辨Transformer以提升光子高效成像中的3D重建性能",
            "intro_zh": [
                "核心问题：瞬态测量在3D重建中面临传感器量子效率低和高噪声挑战",
                "方法要点：TRT架构通过时空自注意力和交叉注意力编码解码局部与全局特征",
                "实验或效果：在合成和真实数据上显著优于现有方法，并贡献新数据集"
            ],
            "tags_zh": [
                "3D重建",
                "瞬态测量",
                "时间分辨Transformer",
                "光子高效成像",
                "非视线成像"
            ],
            "_index": 42
        },
        {
            "title": "Cattle-CLIP: A Multimodal Framework for Cattle Behaviour Recognition",
            "authors": [
                "Huimin Liu",
                "Jing Gao",
                "Daria Baran",
                "AxelX Montout",
                "Neill W Campbell",
                "Andrew W Dowsey"
            ],
            "arxiv_id": "2510.09203v1",
            "summary": "Cattle behaviour is a crucial indicator of an individual animal health,\nproductivity and overall well-being. Video-based monitoring, combined with deep\nlearning techniques, has become a mainstream approach in animal biometrics, and\nit can offer high accuracy in some behaviour recognition tasks. We present\nCattle-CLIP, a multimodal deep learning framework for cattle behaviour\nrecognition, using semantic cues to improve the performance of video-based\nvisual feature recognition. It is adapted from the large-scale image-language\nmodel CLIP by adding a temporal integration module. To address the domain gap\nbetween web data used for the pre-trained model and real-world cattle\nsurveillance footage, we introduce tailored data augmentation strategies and\nspecialised text prompts. Cattle-CLIP is evaluated under both fully-supervised\nand few-shot learning scenarios, with a particular focus on data-scarce\nbehaviour recognition - an important yet under-explored goal in livestock\nmonitoring. To evaluate the proposed method, we release the CattleBehaviours6\ndataset, which comprises six types of indoor behaviours: feeding, drinking,\nstanding-self-grooming, standing-ruminating, lying-self-grooming and\nlying-ruminating. The dataset consists of 1905 clips collected from our John\nOldacre Centre dairy farm research platform housing 200 Holstein-Friesian cows.\nExperiments show that Cattle-CLIP achieves 96.1% overall accuracy across six\nbehaviours in a supervised setting, with nearly 100% recall for feeding,\ndrinking and standing-ruminating behaviours, and demonstrates robust\ngeneralisation with limited data in few-shot scenarios, highlighting the\npotential of multimodal learning in agricultural and animal behaviour analysis.",
            "headline_zh": "提出Cattle-CLIP多模态框架，以提升牛只行为识别的准确性和泛化能力。",
            "intro_zh": [
                "核心问题：牛只行为识别对健康和生产力监测至关重要，但数据稀缺和领域差异是挑战。",
                "方法要点：基于CLIP模型，添加时间集成模块，并采用数据增强和专用文本提示。",
                "实验或效果：在CattleBehaviours6数据集上，监督学习准确率达96.1%，少样本场景泛化强。"
            ],
            "tags_zh": [
                "多模态学习",
                "牛只行为识别",
                "CLIP模型",
                "时间集成",
                "少样本学习",
                "农业监控"
            ],
            "_index": 43
        },
        {
            "title": "Towards Safer and Understandable Driver Intention Prediction",
            "authors": [
                "Mukilan Karuppasamy",
                "Shankar Gangisetty",
                "Shyam Nandan Rai",
                "Carlo Masone",
                "C V Jawahar"
            ],
            "arxiv_id": "2510.09200v1",
            "summary": "Autonomous driving (AD) systems are becoming increasingly capable of handling\ncomplex tasks, mainly due to recent advances in deep learning and AI. As\ninteractions between autonomous systems and humans increase, the\ninterpretability of decision-making processes in driving systems becomes\nincreasingly crucial for ensuring safe driving operations. Successful\nhuman-machine interaction requires understanding the underlying representations\nof the environment and the driving task, which remains a significant challenge\nin deep learning-based systems. To address this, we introduce the task of\ninterpretability in maneuver prediction before they occur for driver safety,\ni.e., driver intent prediction (DIP), which plays a critical role in AD\nsystems. To foster research in interpretable DIP, we curate the eXplainable\nDriving Action Anticipation Dataset (DAAD-X), a new multimodal, ego-centric\nvideo dataset to provide hierarchical, high-level textual explanations as\ncausal reasoning for the driver's decisions. These explanations are derived\nfrom both the driver's eye-gaze and the ego-vehicle's perspective. Next, we\npropose Video Concept Bottleneck Model (VCBM), a framework that generates\nspatio-temporally coherent explanations inherently, without relying on post-hoc\ntechniques. Finally, through extensive evaluations of the proposed VCBM on the\nDAAD-X dataset, we demonstrate that transformer-based models exhibit greater\ninterpretability than conventional CNN-based models. Additionally, we introduce\na multilabel t-SNE visualization technique to illustrate the disentanglement\nand causal correlation among multiple explanations. Our data, code and models\nare available at: https://mukil07.github.io/VCBM.github.io/",
            "headline_zh": "提出视频概念瓶颈模型以提升自动驾驶中驾驶员意图预测的可解释性",
            "intro_zh": [
                "核心问题：自动驾驶系统决策过程缺乏可解释性，影响人机交互安全。",
                "方法要点：构建DAAD-X数据集，并开发VCBM框架生成时空一致的解释。",
                "实验或效果：评估显示基于Transformer的模型比CNN更具可解释性。"
            ],
            "tags_zh": [
                "驾驶员意图预测",
                "可解释人工智能",
                "多模态数据集",
                "视频理解",
                "Transformer模型"
            ],
            "_index": 44
        },
        {
            "title": "Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study",
            "authors": [
                "Sungwoo Kang"
            ],
            "arxiv_id": "2510.09187v1",
            "summary": "Cricket shot classification from video sequences remains a challenging\nproblem in sports video analysis, requiring effective modeling of both spatial\nand temporal features. This paper presents the first comprehensive baseline\nstudy comparing seven different deep learning approaches across four distinct\nresearch paradigms for cricket shot classification. We implement and\nsystematically evaluate traditional CNN-LSTM architectures, attention-based\nmodels, vision transformers, transfer learning approaches, and modern\nEfficientNet-GRU combinations on a unified benchmark. A critical finding of our\nstudy is the significant performance gap between claims in academic literature\nand practical implementation results. While previous papers reported accuracies\nof 96\\% (Balaji LRCN), 99.2\\% (IJERCSE), and 93\\% (Sensors), our standardized\nre-implementations achieve 46.0\\%, 55.6\\%, and 57.7\\% respectively. Our modern\nSOTA approach, combining EfficientNet-B0 with a GRU-based temporal model,\nachieves 92.25\\% accuracy, demonstrating that substantial improvements are\npossible with modern architectures and systematic optimization. All\nimplementations follow modern MLOps practices with PyTorch Lightning, providing\na reproducible research platform that exposes the critical importance of\nstandardized evaluation protocols in sports video analysis research.",
            "headline_zh": "提出现代深度学习基线研究，系统比较七种方法以解决板球击球分类问题。",
            "intro_zh": [
                "核心问题：板球击球视频分类需建模时空特征，现有文献与实际性能存在显著差距。",
                "方法要点：比较CNN-LSTM、注意力模型、视觉Transformer、迁移学习及EfficientNet-GRU组合。",
                "实验效果：现代SOTA方法达92.25%准确率，强调标准化评估协议的重要性。"
            ],
            "tags_zh": [
                "板球击球分类",
                "深度学习基线",
                "时空特征建模",
                "标准化评估",
                "EfficientNet-GRU",
                "视频分析"
            ],
            "_index": 45
        },
        {
            "title": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption",
            "authors": [
                "Johann-Friedrich Feiden",
                "Tim Küchler",
                "Denis Zavadski",
                "Bogdan Savchynskyy",
                "Carsten Rother"
            ],
            "arxiv_id": "2510.09182v1",
            "summary": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware.",
            "headline_zh": "提出在线视频深度预测方法oVDA，实现低内存消耗和时序一致性",
            "intro_zh": [
                "核心问题：现有视频深度预测方法依赖批处理，无法在线实时运行",
                "方法要点：借鉴大语言模型技术，缓存潜在特征并掩码训练帧",
                "实验效果：在精度和VRAM使用上优于其他在线方法，边缘设备可达20 FPS"
            ],
            "tags_zh": [
                "在线视频深度估计",
                "时序一致性",
                "低内存消耗",
                "边缘设备部署",
                "潜在特征缓存"
            ],
            "_index": 46
        },
        {
            "title": "TARO: Toward Semantically Rich Open-World Object Detection",
            "authors": [
                "Yuchen Zhang",
                "Yao Lu",
                "Johannes Betz"
            ],
            "arxiv_id": "2510.09173v1",
            "summary": "Modern object detectors are largely confined to a \"closed-world\" assumption,\nlimiting them to a predefined set of classes and posing risks when encountering\nnovel objects in real-world scenarios. While open-set detection methods aim to\naddress this by identifying such instances as 'Unknown', this is often\ninsufficient. Rather than treating all unknowns as a single class, assigning\nthem more descriptive subcategories can enhance decision-making in\nsafety-critical contexts. For example, identifying an object as an 'Unknown\nAnimal' (requiring an urgent stop) versus 'Unknown Debris' (requiring a safe\nlane change) is far more useful than just 'Unknown' in autonomous driving. To\nbridge this gap, we introduce TARO, a novel detection framework that not only\nidentifies unknown objects but also classifies them into coarse parent\ncategories within a semantic hierarchy. TARO employs a unique architecture with\na sparsemax-based head for modeling objectness, a hierarchy-guided relabeling\ncomponent that provides auxiliary supervision, and a classification module that\nlearns hierarchical relationships. Experiments show TARO can categorize up to\n29.9% of unknowns into meaningful coarse classes, significantly reduce\nconfusion between unknown and known classes, and achieve competitive\nperformance in both unknown recall and known mAP. Code will be made available.",
            "headline_zh": "提出TARO框架以解决开放世界物体检测中未知对象语义分类不足的问题",
            "intro_zh": [
                "核心问题：现有检测器局限于封闭世界，未知对象仅被识别为单一类别，缺乏语义丰富性。",
                "方法要点：采用稀疏最大头建模物体性，结合层次引导重标签和分类模块学习语义层次关系。",
                "实验或效果：TARO可将29.9%未知对象分类为粗粒度类别，减少未知与已知类混淆，性能竞争。"
            ],
            "tags_zh": [
                "开放世界物体检测",
                "语义层次分类",
                "稀疏最大头",
                "未知对象识别",
                "自动驾驶安全"
            ],
            "_index": 47
        },
        {
            "title": "Instance-Level Generation for Representation Learning",
            "authors": [
                "Yankun Wu",
                "Zakaria Laskar",
                "Giorgos Kordopatis-Zilos",
                "Noa Garcia",
                "Giorgos Tolias"
            ],
            "arxiv_id": "2510.09171v1",
            "summary": "Instance-level recognition (ILR) focuses on identifying individual objects\nrather than broad categories, offering the highest granularity in image\nclassification. However, this fine-grained nature makes creating large-scale\nannotated datasets challenging, limiting ILR's real-world applicability across\ndomains. To overcome this, we introduce a novel approach that synthetically\ngenerates diverse object instances from multiple domains under varied\nconditions and backgrounds, forming a large-scale training set. Unlike prior\nwork on automatic data synthesis, our method is the first to address\nILR-specific challenges without relying on any real images. Fine-tuning\nfoundation vision models on the generated data significantly improves retrieval\nperformance across seven ILR benchmarks spanning multiple domains. Our approach\noffers a new, efficient, and effective alternative to extensive data collection\nand curation, introducing a new ILR paradigm where the only input is the names\nof the target domains, unlocking a wide range of real-world applications.",
            "headline_zh": "提出实例级生成方法以解决细粒度识别中数据稀缺问题",
            "intro_zh": [
                "核心问题：实例级识别因细粒度特性导致大规模标注数据获取困难，限制跨领域应用。",
                "方法要点：无需真实图像，合成多领域、多条件下的对象实例，构建大规模训练集。",
                "实验或效果：在七个ILR基准上微调基础模型，显著提升检索性能。"
            ],
            "tags_zh": [
                "实例级识别",
                "数据合成",
                "表示学习",
                "跨领域检索",
                "基础模型微调"
            ],
            "_index": 48
        },
        {
            "title": "Online Topological Localization for Navigation Assistance in Bronchoscopy",
            "authors": [
                "Clara Tomasini",
                "Luis Riazuelo",
                "Ana C. Murillo"
            ],
            "arxiv_id": "2510.09144v1",
            "summary": "Video bronchoscopy is a fundamental procedure in respiratory medicine, where\nmedical experts navigate through the bronchial tree of a patient to diagnose or\noperate the patient. Surgeons need to determine the position of the scope as\nthey go through the airway until they reach the area of interest. This task is\nvery challenging for practitioners due to the complex bronchial tree structure\nand varying doctor experience and training. Navigation assistance to locate the\nbronchoscope during the procedure can improve its outcome. Currently used\ntechniques for navigational guidance commonly rely on previous CT scans of the\npatient to obtain a 3D model of the airway, followed by tracking of the scope\nwith additional sensors or image registration. These methods obtain accurate\nlocations but imply additional setup, scans and training. Accurate metric\nlocalization is not always required, and a topological localization with regard\nto a generic airway model can often suffice to assist the surgeon with\nnavigation. We present an image-based bronchoscopy topological localization\npipeline to provide navigation assistance during the procedure, with no need of\npatient CT scan. Our approach is trained only on phantom data, eliminating the\nhigh cost of real data labeling, and presents good generalization capabilities.\nThe results obtained surpass existing methods, particularly on real data test\nsequences.",
            "headline_zh": "提出基于图像的支气管镜拓扑定位方法，用于术中导航辅助，无需患者CT扫描。",
            "intro_zh": [
                "核心问题：支气管镜术中定位困难，现有方法依赖CT扫描和额外传感器，成本高。",
                "方法要点：仅使用幻影数据训练，实现拓扑定位，无需患者特定CT扫描。",
                "实验或效果：在真实数据测试序列上超越现有方法，展示良好泛化能力。"
            ],
            "tags_zh": [
                "支气管镜导航",
                "拓扑定位",
                "图像处理",
                "医学影像",
                "幻影数据训练"
            ],
            "_index": 49
        },
        {
            "title": "Training Feature Attribution for Vision Models",
            "authors": [
                "Aziz Bacha",
                "Thomas George"
            ],
            "arxiv_id": "2510.09135v1",
            "summary": "Deep neural networks are often considered opaque systems, prompting the need\nfor explainability methods to improve trust and accountability. Existing\napproaches typically attribute test-time predictions either to input features\n(e.g., pixels in an image) or to influential training examples. We argue that\nboth perspectives should be studied jointly. This work explores *training\nfeature attribution*, which links test predictions to specific regions of\nspecific training images and thereby provides new insights into the inner\nworkings of deep models. Our experiments on vision datasets show that training\nfeature attribution yields fine-grained, test-specific explanations: it\nidentifies harmful examples that drive misclassifications and reveals spurious\ncorrelations, such as patch-based shortcuts, that conventional attribution\nmethods fail to expose.",
            "headline_zh": "提出训练特征归因方法，联合分析测试预测与训练图像区域，提升视觉模型可解释性。",
            "intro_zh": [
                "核心问题：深度神经网络被视为黑盒，需可解释性方法增强信任与问责。",
                "方法要点：联合归因测试预测到特定训练图像区域，提供细粒度解释。",
                "实验或效果：在视觉数据集上识别有害样本和虚假相关性，优于传统方法。"
            ],
            "tags_zh": [
                "训练特征归因",
                "视觉模型可解释性",
                "深度神经网络",
                "测试预测归因",
                "虚假相关性检测"
            ],
            "_index": 50
        },
        {
            "title": "Polar Separable Transform for Efficient Orthogonal Rotation-Invariant Image Representation",
            "authors": [
                "Satya P. Singh",
                "Rashmi Chaudhry",
                "Anand Srivastava",
                "Jagath C. Rajapakse"
            ],
            "arxiv_id": "2510.09125v1",
            "summary": "Orthogonal moment-based image representations are fundamental in computer\nvision, but classical methods suffer from high computational complexity and\nnumerical instability at large orders. Zernike and pseudo-Zernike moments, for\ninstance, require coupled radial-angular processing that precludes efficient\nfactorization, resulting in $\\mathcal{O}(n^3N^2)$ to $\\mathcal{O}(n^6N^2)$\ncomplexity and $\\mathcal{O}(N^4)$ condition number scaling for the $n$th-order\nmoments on an $N\\times N$ image. We introduce \\textbf{PSepT} (Polar Separable\nTransform), a separable orthogonal transform that overcomes the\nnon-separability barrier in polar coordinates. PSepT achieves complete kernel\nfactorization via tensor-product construction of Discrete Cosine Transform\n(DCT) radial bases and Fourier harmonic angular bases, enabling independent\nradial and angular processing. This separable design reduces computational\ncomplexity to $\\mathcal{O}(N^2 \\log N)$, memory requirements to\n$\\mathcal{O}(N^2)$, and condition number scaling to $\\mathcal{O}(\\sqrt{N})$,\nrepresenting exponential improvements over polynomial approaches. PSepT\nexhibits orthogonality, completeness, energy conservation, and\nrotation-covariance properties. Experimental results demonstrate better\nnumerical stability, computational efficiency, and competitive classification\nperformance on structured datasets, while preserving exact reconstruction. The\nseparable framework enables high-order moment analysis previously infeasible\nwith classical methods, opening new possibilities for robust image analysis\napplications.",
            "headline_zh": "提出PSepT变换以高效实现正交旋转不变图像表示",
            "intro_zh": [
                "经典正交矩方法计算复杂且数值不稳定，限制高维分析",
                "PSepT通过DCT径向基与傅里叶角基张量积实现核分离",
                "实验显示复杂度降低、稳定性提升，分类性能具竞争力"
            ],
            "tags_zh": [
                "正交矩",
                "图像表示",
                "旋转不变性",
                "可分离变换",
                "计算效率",
                "数值稳定性"
            ],
            "_index": 51
        },
        {
            "title": "MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation",
            "authors": [
                "Dominik Winter",
                "Mai Bui",
                "Monica Azqueta Gavaldon",
                "Nicolas Triltsch",
                "Marco Rosati",
                "Nicolas Brieu"
            ],
            "arxiv_id": "2510.09121v1",
            "summary": "Scarcity of annotated data, particularly for rare or atypical morphologies,\npresent significant challenges for cell and nuclei segmentation in\ncomputational pathology. While manual annotation is labor-intensive and costly,\nsynthetic data offers a cost-effective alternative. We introduce a Multimodal\nSemantic Diffusion Model (MSDM) for generating realistic pixel-precise\nimage-mask pairs for cell and nuclei segmentation. By conditioning the\ngenerative process with cellular/nuclear morphologies (using horizontal and\nvertical maps), RGB color characteristics, and BERT-encoded assay/indication\nmetadata, MSDM generates datasests with desired morphological properties. These\nheterogeneous modalities are integrated via multi-head cross-attention,\nenabling fine-grained control over the generated images. Quantitative analysis\ndemonstrates that synthetic images closely match real data, with low\nWasserstein distances between embeddings of generated and real images under\nmatching biological conditions. The incorporation of these synthetic samples,\nexemplified by columnar cells, significantly improves segmentation model\naccuracy on columnar cells. This strategy systematically enriches data sets,\ndirectly targeting model deficiencies. We highlight the effectiveness of\nmultimodal diffusion-based augmentation for advancing the robustness and\ngeneralizability of cell and nuclei segmentation models. Thereby, we pave the\nway for broader application of generative models in computational pathology.",
            "headline_zh": "提出多模态语义扩散模型以生成病理图像-掩码对，提升细胞和核分割性能",
            "intro_zh": [
                "核心问题：标注数据稀缺，尤其对罕见或非典型形态细胞和核分割构成挑战",
                "方法要点：使用多模态条件扩散模型，整合形态、颜色和元数据生成图像-掩码对",
                "实验或效果：合成图像与真实数据相似，显著提高分割模型在特定细胞上的准确率"
            ],
            "tags_zh": [
                "计算病理学",
                "扩散模型",
                "细胞分割",
                "合成数据生成",
                "多模态条件生成"
            ],
            "_index": 52
        },
        {
            "title": "SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding",
            "authors": [
                "Weikai Huang",
                "Jieyu Zhang",
                "Taoyang Jia",
                "Chenhao Zheng",
                "Ziqi Gao",
                "Jae Sung Park",
                "Ranjay Krishna"
            ],
            "arxiv_id": "2510.09110v1",
            "summary": "Visual grouping -- operationalized via instance segmentation, visual\ngrounding, and object detection -- underpins applications from robotic\nperception to photo editing. Large annotated datasets are costly, biased in\ncoverage, and hard to scale. Synthetic data are promising but often lack\nflexibility, accuracy, and compositional diversity.\n  We present SOS, a simple and scalable data synthesis pipeline based on an\nobject-centric composition strategy. It pastes high-quality synthetic object\nsegments into new images using structured layout priors and generative\nrelighting, producing accurate and diverse masks, boxes, and referring\nexpressions. Models trained on 100000 synthetic images from SOS outperform\nthose trained on larger real-image datasets such as GRIT (20M) and V3Det (200K)\non detection and grounding tasks, achieving +10.9 AP on LVIS detection and +8.4\n$N_{\\text{Acc}}$ on gRefCOCO grounding. SOS enables controllable dataset\nconstruction and improves generalization in both low-data and closed-vocabulary\nsettings. Augmenting LVIS and COCO with synthetic object segments yields strong\nperformance across real-data scales and even larger gains under extremely\nlimited real data (for example, +3.83 $AP_{\\text{rare}}$ on LVIS instance\nsegmentation and +6.59 AP with a 1 percent COCO setup). This controllability\nalso supports targeted data generation for challenging intra-class referring in\nvisual grounding.",
            "headline_zh": "提出SOS合成数据管道以提升视觉检测、分割和接地任务性能",
            "intro_zh": [
                "视觉分组任务依赖大规模标注数据，但真实数据成本高、覆盖偏差且难以扩展",
                "SOS通过对象中心合成策略，结合布局先验和生成重光照，生成高质量合成对象片段",
                "在检测和接地任务中，SOS训练模型优于大型真实数据集，并增强低数据场景泛化能力"
            ],
            "tags_zh": [
                "合成数据生成",
                "对象检测",
                "实例分割",
                "视觉接地",
                "数据增强",
                "泛化性能"
            ],
            "_index": 53
        },
        {
            "title": "A Novel Multi-branch ConvNeXt Architecture for Identifying Subtle Pathological Features in CT Scans",
            "authors": [
                "Irash Perera",
                "Uthayasanker Thayasivam"
            ],
            "arxiv_id": "2510.09107v1",
            "summary": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis, especially for identifying subtle pathological features.\nThis paper introduces a novel multi-branch ConvNeXt architecture designed\nspecifically for the nuanced challenges of medical image analysis. While\napplied here to the specific problem of COVID-19 diagnosis, the methodology\noffers a generalizable framework for classifying a wide range of pathologies\nfrom CT scans. The proposed model incorporates a rigorous end-to-end pipeline,\nfrom meticulous data preprocessing and augmentation to a disciplined two-phase\ntraining strategy that leverages transfer learning effectively. The\narchitecture uniquely integrates features extracted from three parallel\nbranches: Global Average Pooling, Global Max Pooling, and a new\nAttention-weighted Pooling mechanism. The model was trained and validated on a\ncombined dataset of 2,609 CT slices derived from two distinct datasets.\nExperimental results demonstrate a superior performance on the validation set,\nachieving a final ROC-AUC of 0.9937, a validation accuracy of 0.9757, and an\nF1-score of 0.9825 for COVID-19 cases, outperforming all previously reported\nmodels on this dataset. These findings indicate that a modern, multi-branch\narchitecture, coupled with careful data handling, can achieve performance\ncomparable to or exceeding contemporary state-of-the-art models, thereby\nproving the efficacy of advanced deep learning techniques for robust medical\ndiagnostics.",
            "headline_zh": "提出多分支ConvNeXt架构以识别CT扫描中的细微病理特征",
            "intro_zh": [
                "核心问题：医学影像中细微病理特征的识别，应用于COVID-19诊断等场景",
                "方法要点：集成全局平均池化、全局最大池化和注意力加权池化的多分支架构",
                "实验或效果：在验证集上ROC-AUC达0.9937，准确率0.9757，优于现有模型"
            ],
            "tags_zh": [
                "多分支架构",
                "ConvNeXt",
                "CT扫描分析",
                "注意力机制",
                "医学影像诊断"
            ],
            "_index": 54
        },
        {
            "title": "Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation",
            "authors": [
                "Youwei Zheng",
                "Yuxi Ren",
                "Xin Xia",
                "Xuefeng Xiao",
                "Xiaohua Xie"
            ],
            "arxiv_id": "2510.09094v1",
            "summary": "Diffusion Transformer (DiT) has demonstrated remarkable performance in\ntext-to-image generation; however, its large parameter size results in\nsubstantial inference overhead. Existing parameter compression methods\nprimarily focus on pruning, but aggressive pruning often leads to severe\nperformance degradation due to reduced model capacity. To address this\nlimitation, we pioneer the transformation of a dense DiT into a Mixture of\nExperts (MoE) for structured sparsification, reducing the number of activated\nparameters while preserving model capacity. Specifically, we replace the\nFeed-Forward Networks (FFNs) in DiT Blocks with MoE layers, reducing the number\nof activated parameters in the FFNs by 62.5\\%. Furthermore, we propose the\nMixture of Blocks (MoB) to selectively activate DiT blocks, thereby further\nenhancing sparsity. To ensure an effective dense-to-MoE conversion, we design a\nmulti-step distillation pipeline, incorporating Taylor metric-based expert\ninitialization, knowledge distillation with load balancing, and group feature\nloss for MoB optimization. We transform large diffusion transformers (e.g.,\nFLUX.1 [dev]) into an MoE structure, reducing activated parameters by 60\\%\nwhile maintaining original performance and surpassing pruning-based approaches\nin extensive experiments. Overall, Dense2MoE establishes a new paradigm for\nefficient text-to-image generation.",
            "headline_zh": "提出Dense2MoE将扩散Transformer转换为MoE结构以高效文本到图像生成",
            "intro_zh": [
                "扩散Transformer参数大导致推理开销高，现有剪枝方法易造成性能下降",
                "将DiT块中前馈网络替换为MoE层，并引入混合块选择性激活以结构化稀疏化",
                "实验显示激活参数减少60%，性能保持原水平，优于剪枝方法"
            ],
            "tags_zh": [
                "文本到图像生成",
                "扩散Transformer",
                "专家混合",
                "结构化稀疏化",
                "知识蒸馏",
                "参数效率"
            ],
            "_index": 55
        },
        {
            "title": "GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration",
            "authors": [
                "Juanqin Liu",
                "Leonardo Plotegher",
                "Eloy Roura",
                "Shaoming He"
            ],
            "arxiv_id": "2510.09092v1",
            "summary": "The extensive application of unmanned aerial vehicles (UAVs) in military\nreconnaissance, environmental monitoring, and related domains has created an\nurgent need for accurate and efficient multi-object tracking (MOT)\ntechnologies, which are also essential for UAV situational awareness. However,\ncomplex backgrounds, small-scale targets, and frequent occlusions and\ninteractions continue to challenge existing methods in terms of detection\naccuracy and trajectory continuity. To address these issues, this paper\nproposes the Global-Local Detection and Tracking (GL-DT) framework. It employs\na Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and\nappearance features, combined with a global-local collaborative detection\nstrategy, effectively enhancing small-target detection. Building upon this, the\nJPTrack tracking algorithm is introduced to mitigate common issues such as ID\nswitches and trajectory fragmentation. Experimental results demonstrate that\nthe proposed approach significantly improves the continuity and stability of\nMOT while maintaining real-time performance, providing strong support for the\nadvancement of UAV detection and tracking technologies.",
            "headline_zh": "提出GL-DT框架以解决无人机多目标检测与跟踪中的小目标和轨迹中断问题",
            "intro_zh": [
                "核心问题：复杂背景、小目标和频繁遮挡导致检测精度低和轨迹不连续",
                "方法要点：采用时空特征融合和全局-局部协作检测，结合JPTrack算法减少ID切换",
                "实验或效果：显著提升跟踪连续性和稳定性，同时保持实时性能"
            ],
            "tags_zh": [
                "多目标跟踪",
                "无人机检测",
                "时空特征融合",
                "小目标检测",
                "轨迹连续性"
            ],
            "_index": 56
        },
        {
            "title": "MambaH-Fit: Rethinking Hyper-surface Fitting-based Point Cloud Normal Estimation via State Space Modelling",
            "authors": [
                "Weijia Wang",
                "Yuanzhi Su",
                "Pei-Gen Ye",
                "Yuan-Gen Wang",
                "Xuequan Lu"
            ],
            "arxiv_id": "2510.09088v1",
            "summary": "We present MambaH-Fit, a state space modelling framework tailored for\nhyper-surface fitting-based point cloud normal estimation. Existing normal\nestimation methods often fall short in modelling fine-grained geometric\nstructures, thereby limiting the accuracy of the predicted normals. Recently,\nstate space models (SSMs), particularly Mamba, have demonstrated strong\nmodelling capability by capturing long-range dependencies with linear\ncomplexity and inspired adaptations to point cloud processing. However,\nexisting Mamba-based approaches primarily focus on understanding global shape\nstructures, leaving the modelling of local, fine-grained geometric details\nlargely under-explored. To address the issues above, we first introduce an\nAttention-driven Hierarchical Feature Fusion (AHFF) scheme to adaptively fuse\nmulti-scale point cloud patch features, significantly enhancing geometric\ncontext learning in local point cloud neighbourhoods. Building upon this, we\nfurther propose Patch-wise State Space Model (PSSM) that models point cloud\npatches as implicit hyper-surfaces via state dynamics, enabling effective\nfine-grained geometric understanding for normal prediction. Extensive\nexperiments on benchmark datasets show that our method outperforms existing\nones in terms of accuracy, robustness, and flexibility. Ablation studies\nfurther validate the contribution of the proposed components.",
            "headline_zh": "提出MambaH-Fit框架，通过状态空间建模改进点云法向量估计的精细几何结构建模。",
            "intro_zh": [
                "现有方法在建模点云精细几何结构方面不足，影响法向量估计精度。",
                "引入注意力驱动层次特征融合和补丁状态空间模型，增强局部几何上下文学习。",
                "在基准数据集上实验显示，方法在准确性、鲁棒性和灵活性方面优于现有方法。"
            ],
            "tags_zh": [
                "点云法向量估计",
                "状态空间模型",
                "超曲面拟合",
                "注意力机制",
                "多尺度特征融合"
            ],
            "_index": 57
        },
        {
            "title": "Visual Anomaly Detection for Reliable Robotic Implantation of Flexible Microelectrode Array",
            "authors": [
                "Yitong Chen",
                "Xinyao Xu",
                "Ping Zhu",
                "Xinyong Han",
                "Fangbo Qin",
                "Shan Yu"
            ],
            "arxiv_id": "2510.09071v1",
            "summary": "Flexible microelectrode (FME) implantation into brain cortex is challenging\ndue to the deformable fiber-like structure of FME probe and the interaction\nwith critical bio-tissue. To ensure reliability and safety, the implantation\nprocess should be monitored carefully. This paper develops an image-based\nanomaly detection framework based on the microscopic cameras of the robotic FME\nimplantation system. The unified framework is utilized at four checkpoints to\ncheck the micro-needle, FME probe, hooking result, and implantation point,\nrespectively. Exploiting the existing object localization results, the aligned\nregions of interest (ROIs) are extracted from raw image and input to a\npretrained vision transformer (ViT). Considering the task specifications, we\npropose a progressive granularity patch feature sampling method to address the\nsensitivity-tolerance trade-off issue at different locations. Moreover, we\nselect a part of feature channels with higher signal-to-noise ratios from the\nraw general ViT features, to provide better descriptors for each specific\nscene. The effectiveness of the proposed methods is validated with the image\ndatasets collected from our implantation system.",
            "headline_zh": "提出基于视觉Transformer的异常检测框架，用于机器人柔性微电极植入过程监控",
            "intro_zh": [
                "核心问题：柔性微电极植入脑皮层时，因结构变形和生物组织交互，需可靠监控以确保安全",
                "方法要点：利用预训练ViT提取对齐ROI特征，采用渐进粒度采样和特征通道选择优化检测",
                "实验或效果：在植入系统收集的图像数据集上验证了方法的有效性，未知具体性能指标"
            ],
            "tags_zh": [
                "视觉异常检测",
                "机器人植入",
                "柔性微电极",
                "视觉Transformer",
                "特征采样",
                "脑皮层手术"
            ],
            "_index": 58
        },
        {
            "title": "MMAudioSep: Taming Video-to-Audio Generative Model Towards Video/Text-Queried Sound Separation",
            "authors": [
                "Akira Takahashi",
                "Shusuke Takahashi",
                "Yuki Mitsufuji"
            ],
            "arxiv_id": "2510.09065v1",
            "summary": "We introduce MMAudioSep, a generative model for video/text-queried sound\nseparation that is founded on a pretrained video-to-audio model. By leveraging\nknowledge about the relationship between video/text and audio learned through a\npretrained audio generative model, we can train the model more efficiently,\ni.e., the model does not need to be trained from scratch. We evaluate the\nperformance of MMAudioSep by comparing it to existing separation models,\nincluding models based on both deterministic and generative approaches, and\nfind it is superior to the baseline models. Furthermore, we demonstrate that\neven after acquiring functionality for sound separation via fine-tuning, the\nmodel retains the ability for original video-to-audio generation. This\nhighlights the potential of foundational sound generation models to be adopted\nfor sound-related downstream tasks. Our code is available at\nhttps://github.com/sony/mmaudiosep.",
            "headline_zh": "提出MMAudioSep，基于预训练视频到音频模型实现视频/文本查询的声音分离。",
            "intro_zh": [
                "核心问题：视频或文本查询下的声音分离任务，需高效利用视频/文本与音频关系。",
                "方法要点：利用预训练视频到音频生成模型知识，通过微调而非从头训练实现高效学习。",
                "实验或效果：在分离性能上优于基线模型，并保留原始视频到音频生成能力。"
            ],
            "tags_zh": [
                "声音分离",
                "视频到音频生成",
                "预训练模型",
                "微调",
                "多模态学习"
            ],
            "_index": 59
        },
        {
            "title": "OSCAR: Orthogonal Stochastic Control for Alignment-Respecting Diversity in Flow Matching",
            "authors": [
                "Jingxuan Wu",
                "Zhenglin Wan",
                "Xingrui Yu",
                "Yuzhe Yang",
                "Bo An",
                "Ivor Tsang"
            ],
            "arxiv_id": "2510.09060v1",
            "summary": "Flow-based text-to-image models follow deterministic trajectories, forcing\nusers to repeatedly sample to discover diverse modes, which is a costly and\ninefficient process. We present a training-free, inference-time control\nmechanism that makes the flow itself diversity-aware. Our method simultaneously\nencourages lateral spread among trajectories via a feature-space objective and\nreintroduces uncertainty through a time-scheduled stochastic perturbation.\nCrucially, this perturbation is projected to be orthogonal to the generation\nflow, a geometric constraint that allows it to boost variation without\ndegrading image details or prompt fidelity. Our procedure requires no\nretraining or modification to the base sampler and is compatible with common\nflow-matching solvers. Theoretically, our method is shown to monotonically\nincrease a volume surrogate while, due to its geometric constraints,\napproximately preserving the marginal distribution. This provides a principled\nexplanation for why generation quality is robustly maintained. Empirically,\nacross multiple text-to-image settings under fixed sampling budgets, our method\nconsistently improves diversity metrics such as the Vendi Score and Brisque\nover strong baselines, while upholding image quality and alignment.",
            "headline_zh": "提出正交随机控制方法以提升流匹配模型在文本到图像生成中的多样性",
            "intro_zh": [
                "流匹配模型轨迹确定性导致多样性不足，需重复采样",
                "通过特征空间目标和正交随机扰动增强轨迹多样性",
                "实验显示在固定采样预算下提升多样性指标并保持质量"
            ],
            "tags_zh": [
                "文本到图像生成",
                "流匹配模型",
                "多样性增强",
                "正交扰动",
                "训练免费控制"
            ],
            "_index": 60
        },
        {
            "title": "Lesion-Aware Post-Training of Latent Diffusion Models for Synthesizing Diffusion MRI from CT Perfusion",
            "authors": [
                "Junhyeok Lee",
                "Hyunwoong Kim",
                "Hyungjin Chung",
                "Heeseong Eom",
                "Joon Jang",
                "Chul-Ho Sohn",
                "Kyu Sung Choi"
            ],
            "arxiv_id": "2510.09056v1",
            "summary": "Image-to-Image translation models can help mitigate various challenges\ninherent to medical image acquisition. Latent diffusion models (LDMs) leverage\nefficient learning in compressed latent space and constitute the core of\nstate-of-the-art generative image models. However, this efficiency comes with a\ntrade-off, potentially compromising crucial pixel-level detail essential for\nhigh-fidelity medical images. This limitation becomes particularly critical\nwhen generating clinically significant structures, such as lesions, which often\noccupy only a small portion of the image. Failure to accurately reconstruct\nthese regions can severely impact diagnostic reliability and clinical\ndecision-making. To overcome this limitation, we propose a novel post-training\nframework for LDMs in medical image-to-image translation by incorporating\nlesion-aware medical pixel space objectives. This approach is essential, as it\nnot only enhances overall image quality but also improves the precision of\nlesion delineation. We evaluate our framework on brain CT-to-MRI translation in\nacute ischemic stroke patients, where early and accurate diagnosis is critical\nfor optimal treatment selection and improved patient outcomes. While diffusion\nMRI is the gold standard for stroke diagnosis, its clinical utility is often\nconstrained by high costs and low accessibility. Using a dataset of 817\npatients, we demonstrate that our framework improves overall image quality and\nenhances lesion delineation when synthesizing DWI and ADC images from CT\nperfusion scans, outperforming existing image-to-image translation models.\nFurthermore, our post-training strategy is easily adaptable to pre-trained LDMs\nand exhibits substantial potential for broader applications across diverse\nmedical image translation tasks.",
            "headline_zh": "提出病灶感知后训练框架，提升CT到MRI转换中病灶重建精度",
            "intro_zh": [
                "潜在扩散模型在医学图像转换中可能丢失关键像素细节，影响病灶重建",
                "引入病灶感知像素空间目标，增强整体图像质量和病灶描绘准确性",
                "在817名患者数据集上验证，合成DWI和ADC图像优于现有方法"
            ],
            "tags_zh": [
                "潜在扩散模型",
                "医学图像转换",
                "病灶重建",
                "后训练框架",
                "CT到MRI合成"
            ],
            "_index": 61
        },
        {
            "title": "Auto-scaling Continuous Memory for GUI Agent",
            "authors": [
                "Wenyi Wu",
                "Kun Zhou",
                "Ruoxin Yuan",
                "Vivian Yu",
                "Stephen Wang",
                "Zhiting Hu",
                "Biwei Huang"
            ],
            "arxiv_id": "2510.09038v1",
            "summary": "We study how to endow GUI agents with scalable memory that help generalize\nacross unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress\npast trajectories into text tokens, which balloons context length and misses\ndecisive visual cues (e.g., exact widget size and position). We propose a\ncontinuous memory that encodes each GUI trajectory into a fixed-length sequence\nof continuous embeddings using the VLM itself as an encoder; these embeddings\nare plugged directly into the backbone's input layer, sharply reducing context\ncost while preserving fine-grained visual information. As memory size and\nretrieval depth increase, performance improves monotonically, unlike text\nmemories that degrade with long prompts. To grow memory at low cost, we\nintroduce an auto-scaling data flywheel that (i) discovers new environments via\nsearch, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out\ntrajectories with the agent, and (iv) verifies success with the same VLM. Using\nthis pipeline, we collect 100k+ trajectories for about \\$4000 and fine-tune\nonly the memory encoder (LoRA on a Q-Former, 1.2\\% parameters) with 1,500\nsamples. On real-world GUI benchmarks, our memory-augmented agent consistently\nimproves success rates under long horizons and distribution shifts. Notably,\nQwen-2.5-VL-7B + continuous memory achieves performance comparable to\nstate-of-the-art closed-source models (e.g., GPT-4o, Claude-4).",
            "headline_zh": "提出连续记忆方法以增强GUI代理在陌生界面和长任务中的泛化能力",
            "intro_zh": [
                "核心问题：现有GUI代理压缩轨迹为文本，导致上下文过长且丢失关键视觉信息。",
                "方法要点：使用VLM编码轨迹为固定长度连续嵌入，直接输入骨干网络减少上下文成本。",
                "实验或效果：在真实GUI基准测试中，成功率和泛化能力显著提升，性能接近闭源模型。"
            ],
            "tags_zh": [
                "GUI代理",
                "连续记忆",
                "视觉语言模型",
                "自动扩展",
                "长任务泛化",
                "嵌入编码"
            ],
            "_index": 62
        },
        {
            "title": "Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels",
            "authors": [
                "Weitong Kong",
                "Zichao Zeng",
                "Di Wen",
                "Jiale Wei",
                "Kunyu Peng",
                "June Moh Goo",
                "Jan Boehm",
                "Rainer Stiefelhagen"
            ],
            "arxiv_id": "2510.09035v1",
            "summary": "Accurate perception is critical for vehicle safety, with LiDAR as a key\nenabler in autonomous driving. To ensure robust performance across\nenvironments, sensor types, and weather conditions without costly\nre-annotation, domain generalization in LiDAR-based 3D semantic segmentation is\nessential. However, LiDAR annotations are often noisy due to sensor\nimperfections, occlusions, and human errors. Such noise degrades segmentation\naccuracy and is further amplified under domain shifts, threatening system\nreliability. While noisy-label learning is well-studied in images, its\nextension to 3D LiDAR segmentation under domain generalization remains largely\nunexplored, as the sparse and irregular structure of point clouds limits direct\nuse of 2D methods. To address this gap, we introduce the novel task Domain\nGeneralization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL)\nand establish the first benchmark by adapting three representative noisy-label\nlearning strategies from image classification to 3D segmentation. However, we\nfind that existing noisy-label learning approaches adapt poorly to LiDAR data.\nWe therefore propose DuNe, a dual-view framework with strong and weak branches\nthat enforce feature-level consistency and apply cross-entropy loss based on\nconfidence-aware filtering of predictions. Our approach shows state-of-the-art\nperformance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and\n52.58% on SemanticPOSS under 10% symmetric label noise, with an overall\nArithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, thereby\ndemonstrating robust domain generalization in DGLSS-NL tasks. The code is\navailable on our project page.",
            "headline_zh": "提出DuNe框架以解决LiDAR语义分割在噪声标签下的单域泛化问题",
            "intro_zh": [
                "核心问题：LiDAR标注噪声在域泛化中加剧，影响3D语义分割的鲁棒性",
                "方法要点：采用双分支框架，通过特征一致性约束和置信度过滤优化损失函数",
                "实验或效果：在多个数据集上实现SOTA性能，mIoU达49.57%算术平均"
            ],
            "tags_zh": [
                "LiDAR语义分割",
                "域泛化",
                "噪声标签学习",
                "3D点云处理",
                "双分支框架"
            ],
            "_index": 63
        },
        {
            "title": "Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy",
            "authors": [
                "Xiaoxiao Ma",
                "Feng Zhao",
                "Pengyang Ling",
                "Haibo Qiu",
                "Zhixiang Wei",
                "Hu Yu",
                "Jie Huang",
                "Zhixiong Zeng",
                "Lin Ma"
            ],
            "arxiv_id": "2510.09012v1",
            "summary": "In this work, we first revisit the sampling issues in current autoregressive\n(AR) image generation models and identify that image tokens, unlike text\ntokens, exhibit lower information density and non-uniform spatial distribution.\nAccordingly, we present an entropy-informed decoding strategy that facilitates\nhigher autoregressive generation quality with faster synthesis speed.\nSpecifically, the proposed method introduces two main innovations: 1) dynamic\ntemperature control guided by spatial entropy of token distributions, enhancing\nthe balance between content diversity, alignment accuracy, and structural\ncoherence in both mask-based and scale-wise models, without extra computational\noverhead, and 2) entropy-aware acceptance rules in speculative decoding,\nachieving near-lossless generation at about 85\\% of the inference cost of\nconventional acceleration methods. Extensive experiments across multiple\nbenchmarks using diverse AR image generation models demonstrate the\neffectiveness and generalizability of our approach in enhancing both generation\nquality and sampling speed.",
            "headline_zh": "提出熵引导解码策略以提升自回归图像生成的质量与速度",
            "intro_zh": [
                "核心问题：图像令牌信息密度低且空间分布不均，影响自回归模型采样效率与质量。",
                "方法要点：动态温度控制与熵感知接受规则，平衡多样性、准确性和结构连贯性。",
                "实验效果：在多个基准测试中，实现高质量生成，推理成本降低约15%。"
            ],
            "tags_zh": [
                "自回归图像生成",
                "熵引导解码",
                "动态温度控制",
                "推测解码",
                "图像令牌分布",
                "生成加速"
            ],
            "_index": 64
        },
        {
            "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models",
            "authors": [
                "Hoigi Seo",
                "Dong Un Kang",
                "Hyunjin Cho",
                "Joohoon Lee",
                "Se Young Chun"
            ],
            "arxiv_id": "2510.09008v1",
            "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts.",
            "headline_zh": "提出视觉编码器修改策略以缓解大视觉语言模型中的物体幻觉问题",
            "intro_zh": [
                "核心问题：大视觉语言模型存在物体幻觉，即生成图像中不存在的物体描述。",
                "方法要点：通过对抗扰动识别不确定视觉令牌，并在自注意力过程中掩码它们。",
                "实验或效果：实验显示该方法显著减少幻觉，并能与其他方法协同工作。"
            ],
            "tags_zh": [
                "大视觉语言模型",
                "物体幻觉",
                "视觉编码器",
                "对抗扰动",
                "自注意力掩码",
                "不确定性估计"
            ],
            "_index": 65
        },
        {
            "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation",
            "authors": [
                "Yao Teng",
                "Fuyun Wang",
                "Xian Liu",
                "Zhekai Chen",
                "Han Shi",
                "Yu Wang",
                "Zhenguo Li",
                "Weiyang Liu",
                "Difan Zou",
                "Xihui Liu"
            ],
            "arxiv_id": "2510.08994v1",
            "summary": "As a new paradigm of visual content generation, autoregressive text-to-image\nmodels suffer from slow inference due to their sequential token-by-token\ndecoding process, often requiring thousands of model forward passes to generate\na single image. To address this inefficiency, we propose Speculative\nJacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising\nprocess into Jacobi iterations to enable parallel token generation in\nautoregressive models. Our method introduces a next-clean-token prediction\nparadigm that enables the pre-trained autoregressive models to accept\nnoise-perturbed token embeddings and predict the next clean tokens through\nlow-cost fine-tuning. This denoising paradigm guides the model towards more\nstable Jacobi trajectories. During inference, our method initializes token\nsequences with Gaussian noise and performs iterative\nnext-clean-token-prediction in the embedding space. We employ a probabilistic\ncriterion to verify and accept multiple tokens in parallel, and refine the\nunaccepted tokens for the next iteration with the denoising trajectory.\nExperiments show that our method can accelerate generation by reducing model\nforward passes while maintaining the visual quality of generated images.",
            "headline_zh": "提出推测性雅可比去噪解码以加速自回归文本到图像生成",
            "intro_zh": [
                "自回归文本到图像模型因逐令牌解码导致推理缓慢，需数千次前向传播",
                "引入去噪过程到雅可比迭代，通过预测下一干净令牌实现并行生成",
                "实验显示方法减少前向传播次数，同时保持生成图像视觉质量"
            ],
            "tags_zh": [
                "自回归模型",
                "文本到图像生成",
                "推理加速",
                "雅可比迭代",
                "去噪解码"
            ],
            "_index": 66
        },
        {
            "title": "Uncolorable Examples: Preventing Unauthorized AI Colorization via Perception-Aware Chroma-Restrictive Perturbation",
            "authors": [
                "Yuki Nii",
                "Futa Waseda",
                "Ching-Chun Chang",
                "Isao Echizen"
            ],
            "arxiv_id": "2510.08979v1",
            "summary": "AI-based colorization has shown remarkable capability in generating realistic\ncolor images from grayscale inputs. However, it poses risks of copyright\ninfringement -- for example, the unauthorized colorization and resale of\nmonochrome manga and films. Despite these concerns, no effective method\ncurrently exists to prevent such misuse. To address this, we introduce the\nfirst defensive paradigm, Uncolorable Examples, which embed imperceptible\nperturbations into grayscale images to invalidate unauthorized colorization. To\nensure real-world applicability, we establish four criteria: effectiveness,\nimperceptibility, transferability, and robustness. Our method, Perception-Aware\nChroma-Restrictive Perturbation (PAChroma), generates Uncolorable Examples that\nmeet these four criteria by optimizing imperceptible perturbations with a\nLaplacian filter to preserve perceptual quality, and applying diverse input\ntransformations during optimization to enhance transferability across models\nand robustness against common post-processing (e.g., compression). Experiments\non ImageNet and Danbooru datasets demonstrate that PAChroma effectively\ndegrades colorization quality while maintaining the visual appearance. This\nwork marks the first step toward protecting visual content from illegitimate AI\ncolorization, paving the way for copyright-aware defenses in generative media.",
            "headline_zh": "提出PAChroma方法以防止未经授权的AI着色，保护版权内容",
            "intro_zh": [
                "核心问题：AI着色技术可能导致版权侵权，如未经授权为黑白漫画和电影着色",
                "方法要点：通过优化不可感知扰动，结合拉普拉斯滤波保持感知质量，增强跨模型迁移性和鲁棒性",
                "实验或效果：在ImageNet和Danbooru数据集上验证，能有效降低着色质量，同时保持视觉外观"
            ],
            "tags_zh": [
                "AI着色防御",
                "不可感知扰动",
                "版权保护",
                "跨模型迁移性",
                "鲁棒性优化"
            ],
            "_index": 67
        },
        {
            "title": "HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images",
            "authors": [
                "Zichuan Wang",
                "Bo Peng",
                "Songlin Yang",
                "Zhenchen Tang",
                "Jing Dong"
            ],
            "arxiv_id": "2510.08978v1",
            "summary": "Although recent text-to-image (T2I) models have significantly improved the\noverall visual quality of generated images, they still struggle in the\ngeneration of accurate details in complex local regions, especially human\nhands. Generated hands often exhibit structural distortions and unrealistic\ntextures, which can be very noticeable even when the rest of the body is\nwell-generated. However, the quality assessment of hand regions remains largely\nneglected, limiting downstream task performance like human-centric generation\nquality optimization and AIGC detection. To address this, we propose the first\nquality assessment task targeting generated hand regions and showcase its\nabundant downstream applications. We first introduce the HandPair dataset for\ntraining hand quality assessment models. It consists of 48k images formed by\nhigh- and low-quality hand pairs, enabling low-cost, efficient supervision\nwithout manual annotation. Based on it, we develop HandEval, a carefully\ndesigned hand-specific quality assessment model. It leverages the powerful\nvisual understanding capability of Multimodal Large Language Model (MLLM) and\nincorporates prior knowledge of hand keypoints, gaining strong perception of\nhand quality. We further construct a human-annotated test set with hand images\nfrom various state-of-the-art (SOTA) T2I models to validate its quality\nevaluation capability. Results show that HandEval aligns better with human\njudgments than existing SOTA methods. Furthermore, we integrate HandEval into\nimage generation and AIGC detection pipelines, prominently enhancing generated\nhand realism and detection accuracy, respectively, confirming its universal\neffectiveness in downstream applications. Code and dataset will be available.",
            "headline_zh": "提出HandEval以评估生成图像中手部质量，提升下游任务性能",
            "intro_zh": [
                "文本到图像模型在手部细节生成中常出现结构扭曲和不真实纹理问题",
                "基于HandPair数据集和MLLM，开发HandEval模型，结合手部关键点先验知识",
                "实验显示HandEval优于现有方法，在图像生成和AIGC检测中显著提升效果"
            ],
            "tags_zh": [
                "手部质量评估",
                "文本到图像模型",
                "多模态大语言模型",
                "AIGC检测",
                "图像生成优化"
            ],
            "_index": 68
        },
        {
            "title": "Hierarchical Scheduling for Multi-Vector Image Retrieval",
            "authors": [
                "Maoliang Li",
                "Ke Li",
                "Yaoyang Liu",
                "Jiayu Chen",
                "Zihao Zheng",
                "Yinjun Wu",
                "Xiang Chen"
            ],
            "arxiv_id": "2510.08976v1",
            "summary": "To effectively leverage user-specific data, retrieval augmented generation\n(RAG) is employed in multimodal large language model (MLLM) applications.\nHowever, conventional retrieval approaches often suffer from limited retrieval\naccuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by\ndecomposing queries and matching against segmented images. They still suffer\nfrom sub-optimal accuracy and efficiency, overlooking alignment between the\nquery and varying image objects and redundant fine-grained image segments. In\nthis work, we present an efficient scheduling framework for image retrieval -\nHiMIR. First, we introduce a novel hierarchical paradigm, employing multiple\nintermediate granularities for varying image objects to enhance alignment.\nSecond, we minimize redundancy in retrieval by leveraging cross-hierarchy\nsimilarity consistency and hierarchy sparsity to minimize unnecessary matching\ncomputation. Furthermore, we configure parameters for each dataset\nautomatically for practicality across diverse scenarios. Our empirical study\nshows that, HiMIR not only achieves substantial accuracy improvements but also\nreduces computation by up to 3.5 times over the existing MVR system.",
            "headline_zh": "提出分层调度框架HiMIR以提升多向量图像检索的准确性和效率",
            "intro_zh": [
                "核心问题：传统多向量检索方法存在检索精度不足和效率低下问题，忽略查询与图像对象对齐及冗余分割",
                "方法要点：采用分层范式，利用多粒度图像对象增强对齐，并通过跨层级相似性一致性和层级稀疏性减少冗余匹配",
                "实验或效果：在实验中，HiMIR显著提升检索精度，并将计算量减少高达3.5倍"
            ],
            "tags_zh": [
                "多向量图像检索",
                "分层调度",
                "检索增强生成",
                "多模态大语言模型",
                "相似性一致性",
                "层级稀疏性"
            ],
            "_index": 69
        },
        {
            "title": "mmJoints: Expanding Joint Representations Beyond (x,y,z) in mmWave-Based 3D Pose Estimation",
            "authors": [
                "Zhenyu Wang",
                "Mahathir Monjur",
                "Shahriar Nirjon"
            ],
            "arxiv_id": "2510.08970v1",
            "summary": "In mmWave-based pose estimation, sparse signals and weak reflections often\ncause models to infer body joints from statistical priors rather than sensor\ndata. While prior knowledge helps in learning meaningful representations,\nover-reliance on it degrades performance in downstream tasks like gesture and\nactivity recognition. In this paper, we introduce mmJoints, a framework that\naugments a pre-trained, black-box mmWave-based 3D pose estimator's output with\nadditional joint descriptors. Rather than mitigating bias, mmJoints makes it\nexplicit by estimating the likelihood of a joint being sensed and the\nreliability of its predicted location. These descriptors enhance\ninterpretability and improve downstream task accuracy. Through extensive\nevaluations using over 115,000 signal frames across 13 pose estimation\nsettings, we show that mmJoints estimates descriptors with an error rate below\n4.2%. mmJoints also improves joint position accuracy by up to 12.5% and boosts\nactivity recognition by up to 16% over state-of-the-art methods.",
            "headline_zh": "提出mmJoints框架，通过增强关节描述符提升毫米波3D姿态估计的可靠性和下游任务性能",
            "intro_zh": [
                "毫米波姿态估计中，稀疏信号导致模型过度依赖先验知识，影响下游任务准确性",
                "mmJoints估计关节被感知的可能性和位置可靠性，增强输出可解释性",
                "实验显示，mmJoints降低描述符误差至4.2%以下，提升位置精度和活动识别性能"
            ],
            "tags_zh": [
                "毫米波3D姿态估计",
                "关节描述符",
                "可解释性增强",
                "下游任务优化",
                "可靠性估计"
            ],
            "_index": 70
        },
        {
            "title": "SAM2-3dMed: Empowering SAM2 for 3D Medical Image Segmentation",
            "authors": [
                "Yeqing Yang",
                "Le Xu",
                "Lixia Tian"
            ],
            "arxiv_id": "2510.08967v1",
            "summary": "Accurate segmentation of 3D medical images is critical for clinical\napplications like disease assessment and treatment planning. While the Segment\nAnything Model 2 (SAM2) has shown remarkable success in video object\nsegmentation by leveraging temporal cues, its direct application to 3D medical\nimages faces two fundamental domain gaps: 1) the bidirectional anatomical\ncontinuity between slices contrasts sharply with the unidirectional temporal\nflow in videos, and 2) precise boundary delineation, crucial for morphological\nanalysis, is often underexplored in video tasks. To bridge these gaps, we\npropose SAM2-3dMed, an adaptation of SAM2 for 3D medical imaging. Our framework\nintroduces two key innovations: 1) a Slice Relative Position Prediction (SRPP)\nmodule explicitly models bidirectional inter-slice dependencies by guiding SAM2\nto predict the relative positions of different slices in a self-supervised\nmanner; 2) a Boundary Detection (BD) module enhances segmentation accuracy\nalong critical organ and tissue boundaries. Extensive experiments on three\ndiverse medical datasets (the Lung, Spleen, and Pancreas in the Medical\nSegmentation Decathlon (MSD) dataset) demonstrate that SAM2-3dMed significantly\noutperforms state-of-the-art methods, achieving superior performance in\nsegmentation overlap and boundary precision. Our approach not only advances 3D\nmedical image segmentation performance but also offers a general paradigm for\nadapting video-centric foundation models to spatial volumetric data.",
            "headline_zh": "提出SAM2-3dMed以解决3D医学图像分割中的领域差距问题",
            "intro_zh": [
                "核心问题：SAM2直接应用于3D医学图像存在双向解剖连续性与视频单向时间流的差异，以及边界分割精度不足。",
                "方法要点：引入切片相对位置预测模块建模双向依赖，并添加边界检测模块提升分割精度。",
                "实验或效果：在多个医学数据集上显著优于现有方法，分割重叠和边界精度表现优越。"
            ],
            "tags_zh": [
                "3D医学图像分割",
                "切片相对位置预测",
                "边界检测",
                "自监督学习",
                "领域适应"
            ],
            "_index": 71
        },
        {
            "title": "Unleashing Perception-Time Scaling to Multimodal Reasoning Models",
            "authors": [
                "Yifan Li",
                "Zhenghao Chen",
                "Ziheng Wu",
                "Kun Zhou",
                "Ruipu Luo",
                "Can Zhang",
                "Zhentao He",
                "Yufei Zhan",
                "Wayne Xin Zhao",
                "Minghui Qiu"
            ],
            "arxiv_id": "2510.08964v1",
            "summary": "Recent advances in inference-time scaling, particularly those leveraging\nreinforcement learning with verifiable rewards, have substantially enhanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs). Inspired by\nthis success, similar strategies have been applied to multimodal reasoning, yet\ntheir impact on visual perception remains unclear. To investigate this gap, we\nintroduce DisTANCE, a perception-centric benchmark for visual estimation tasks.\nEvaluation results show that LVLMs exhibit limited estimation precision, and\ninference-time scaling offers only marginal gains. We attribute this to the\nfast perception paradigm of current LVLMs, where visual understanding is\ntreated as a one-shot output without modeling the underlying perceptual\nprocess. To address this, we propose Perception-Time Scaling (PTS), a novel\nparadigm that encourages token-rich perception and decomposes complex\nperception problems into intermediate tractable sub-problems, thereby enabling\nperception to align with and benefit from inference-time scaling. Combined with\nreinforcement learning techniques, PTS significantly improves perception\naccuracy, raising high-precision performance on DisTANCE from 8.0% to 64.7%,\nand generalizes well to out-of-domain tasks. Surprisingly, even though PTS data\nare purely synthetic, combining them with math reasoning data yields consistent\ngains in both reasoning and real-world perception benchmarks. Further analysis\nreveals that PTS introduces more perception-related tokens and increases the\nmodel's attention to image tokens. Our code and data will be publicly released.",
            "headline_zh": "提出感知时间缩放以提升多模态模型的视觉感知精度",
            "intro_zh": [
                "核心问题：当前大视觉语言模型在视觉感知任务中精度有限，推理时间缩放效果不佳。",
                "方法要点：引入感知时间缩放，分解复杂感知问题为可处理子问题，结合强化学习。",
                "实验或效果：在DisTANCE基准上高精度性能从8.0%提升至64.7%，泛化能力强。"
            ],
            "tags_zh": [
                "感知时间缩放",
                "多模态推理",
                "视觉估计",
                "强化学习",
                "基准评估",
                "合成数据"
            ],
            "_index": 72
        },
        {
            "title": "Denoised Diffusion for Object-Focused Image Augmentation",
            "authors": [
                "Nisha Pillai",
                "Aditi Virupakshaiah",
                "Harrison W. Smith",
                "Amanda J. Ashworth",
                "Prasanna Gowda",
                "Phillip R. Owens",
                "Adam R. Rivers",
                "Bindu Nanduri",
                "Mahalingam Ramkumar"
            ],
            "arxiv_id": "2510.08955v1",
            "summary": "Modern agricultural operations increasingly rely on integrated monitoring\nsystems that combine multiple data sources for farm optimization. Aerial\ndrone-based animal health monitoring serves as a key component but faces\nlimited data availability, compounded by scene-specific issues such as small,\noccluded, or partially visible animals. Transfer learning approaches often fail\nto address this limitation due to the unavailability of large datasets that\nreflect specific farm conditions, including variations in animal breeds,\nenvironments, and behaviors. Therefore, there is a need for developing a\nproblem-specific, animal-focused data augmentation strategy tailored to these\nunique challenges. To address this gap, we propose an object-focused data\naugmentation framework designed explicitly for animal health monitoring in\nconstrained data settings. Our approach segments animals from backgrounds and\naugments them through transformations and diffusion-based synthesis to create\nrealistic, diverse scenes that enhance animal detection and monitoring\nperformance. Our initial experiments demonstrate that our augmented dataset\nyields superior performance compared to our baseline models on the animal\ndetection task. By generating domain-specific data, our method empowers\nreal-time animal health monitoring solutions even in data-scarce scenarios,\nbridging the gap between limited data and practical applicability.",
            "headline_zh": "提出对象聚焦数据增强框架以解决农业动物健康监测中数据稀缺问题",
            "intro_zh": [
                "核心问题：农业无人机监测中动物数据稀缺，动物小、遮挡或部分可见，迁移学习受限。",
                "方法要点：分割动物与背景，通过变换和扩散合成生成真实多样场景，增强检测性能。",
                "实验或效果：初步实验显示增强数据集在动物检测任务上优于基线模型，提升实用性。"
            ],
            "tags_zh": [
                "数据增强",
                "动物检测",
                "扩散模型",
                "农业监测",
                "对象分割"
            ],
            "_index": 73
        },
        {
            "title": "FS-RWKV: Leveraging Frequency Spatial-Aware RWKV for 3T-to-7T MRI Translation",
            "authors": [
                "Yingtie Lei",
                "Zimeng Li",
                "Chi-Man Pun",
                "Yupeng Liu",
                "Xuhang Chen"
            ],
            "arxiv_id": "2510.08951v1",
            "summary": "Ultra-high-field 7T MRI offers enhanced spatial resolution and tissue\ncontrast that enables the detection of subtle pathological changes in\nneurological disorders. However, the limited availability of 7T scanners\nrestricts widespread clinical adoption due to substantial infrastructure costs\nand technical demands. Computational approaches for synthesizing 7T-quality\nimages from accessible 3T acquisitions present a viable solution to this\naccessibility challenge. Existing CNN approaches suffer from limited spatial\ncoverage, while Transformer models demand excessive computational overhead.\nRWKV architectures offer an efficient alternative for global feature modeling\nin medical image synthesis, combining linear computational complexity with\nstrong long-range dependency capture. Building on this foundation, we propose\nFrequency Spatial-RWKV (FS-RWKV), an RWKV-based framework for 3T-to-7T MRI\ntranslation. To better address the challenges of anatomical detail preservation\nand global tissue contrast recovery, FS-RWKV incorporates two key modules: (1)\nFrequency-Spatial Omnidirectional Shift (FSO-Shift), which performs discrete\nwavelet decomposition followed by omnidirectional spatial shifting on the\nlow-frequency branch to enhance global contextual representation while\npreserving high-frequency anatomical details; and (2) Structural Fidelity\nEnhancement Block (SFEB), a module that adaptively reinforces anatomical\nstructure through frequency-aware feature fusion. Comprehensive experiments on\nUNC and BNU datasets demonstrate that FS-RWKV consistently outperforms existing\nCNN-, Transformer-, GAN-, and RWKV-based baselines across both T1w and T2w\nmodalities, achieving superior anatomical fidelity and perceptual quality.",
            "headline_zh": "提出FS-RWKV框架，利用频率空间感知RWKV实现3T到7T MRI图像翻译。",
            "intro_zh": [
                "核心问题：7T MRI稀缺，需从3T图像合成高质量7T图像以提升临床可及性。",
                "方法要点：引入FSO-Shift和SFEB模块，增强全局上下文和解剖结构保真度。",
                "实验效果：在UNC和BNU数据集上优于CNN、Transformer等基线，提升解剖保真度。"
            ],
            "tags_zh": [
                "医学图像合成",
                "MRI翻译",
                "RWKV架构",
                "频率空间感知",
                "解剖结构保真"
            ],
            "_index": 74
        },
        {
            "title": "Progressive Uncertainty-Guided Evidential U-KAN for Trustworthy Medical Image Segmentation",
            "authors": [
                "Zhen Yang",
                "Yansong Ma",
                "Lei Chen"
            ],
            "arxiv_id": "2510.08949v1",
            "summary": "Trustworthy medical image segmentation aims at deliver accurate and reliable\nresults for clinical decision-making. Most existing methods adopt the evidence\ndeep learning (EDL) paradigm due to its computational efficiency and\ntheoretical robustness. However, the EDL-based methods often neglect leveraging\nuncertainty maps rich in attention cues to refine ambiguous boundary\nsegmentation. To address this, we propose a progressive evidence uncertainty\nguided attention (PEUA) mechanism to guide the model to focus on the feature\nrepresentation learning of hard regions. Unlike conventional approaches, PEUA\nprogressively refines attention using uncertainty maps while employing low-rank\nlearning to denoise attention weights, enhancing feature learning for\nchallenging regions. Concurrently, standard EDL methods suppress evidence of\nincorrect class indiscriminately via Kullback-Leibler (KL) regularization,\nimpairing the uncertainty assessment in ambiguous areas and consequently\ndistorts the corresponding attention guidance. We thus introduce a\nsemantic-preserving evidence learning (SAEL) strategy, integrating a\nsemantic-smooth evidence generator and a fidelity-enhancing regularization term\nto retain critical semantics. Finally, by embedding PEUA and SAEL with the\nstate-of-the-art U-KAN, we proposes Evidential U-KAN, a novel solution for\ntrustworthy medical image segmentation. Extensive experiments on 4 datasets\ndemonstrate superior accuracy and reliability over the competing methods. The\ncode is available at\n\\href{https://anonymous.4open.science/r/Evidence-U-KAN-BBE8}{github}.",
            "headline_zh": "提出Evidential U-KAN以解决可信医学图像分割中边界模糊和不确定性评估问题",
            "intro_zh": [
                "核心问题：现有证据深度学习方法忽略不确定性图对边界分割的指导，且KL正则化损害不确定性评估",
                "方法要点：引入渐进不确定性引导注意机制和语义保留证据学习策略，嵌入U-KAN框架",
                "实验或效果：在4个数据集上实验显示，准确性和可靠性优于现有方法"
            ],
            "tags_zh": [
                "医学图像分割",
                "证据深度学习",
                "不确定性引导",
                "注意力机制",
                "语义保留学习",
                "U-KAN框架"
            ],
            "_index": 75
        },
        {
            "title": "Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning",
            "authors": [
                "Zhen Yang",
                "Yansong Ma",
                "Lei Chen"
            ],
            "arxiv_id": "2510.08938v1",
            "summary": "Traditional Evidence Deep Learning (EDL) methods rely on static\nhyperparameter for uncertainty calibration, limiting their adaptability in\ndynamic data distributions, which results in poor calibration and\ngeneralization in high-risk decision-making tasks. To address this limitation,\nwe propose the Meta-Policy Controller (MPC), a dynamic meta-learning framework\nthat adjusts the KL divergence coefficient and Dirichlet prior strengths for\noptimal uncertainty modeling. Specifically, MPC employs a bi-level optimization\napproach: in the inner loop, model parameters are updated through a dynamically\nconfigured loss function that adapts to the current training state; in the\nouter loop, a policy network optimizes the KL divergence coefficient and\nclass-specific Dirichlet prior strengths based on multi-objective rewards\nbalancing prediction accuracy and uncertainty quality. Unlike previous methods\nwith fixed priors, our learnable Dirichlet prior enables flexible adaptation to\nclass distributions and training dynamics. Extensive experimental results show\nthat MPC significantly enhances the reliability and calibration of model\npredictions across various tasks, improving uncertainty calibration, prediction\naccuracy, and performance retention after confidence-based sample rejection.",
            "headline_zh": "提出元策略控制器以解决动态数据分布下不确定性校准问题",
            "intro_zh": [
                "传统EDL方法依赖静态超参数，导致动态数据分布下校准和泛化能力差",
                "采用双层优化：内层动态调整损失函数，外层优化KL系数和Dirichlet先验强度",
                "实验显示显著提升不确定性校准、预测准确性和置信度样本拒绝后性能"
            ],
            "tags_zh": [
                "元学习",
                "不确定性校准",
                "证据深度学习",
                "双层优化",
                "Dirichlet先验"
            ],
            "_index": 76
        },
        {
            "title": "RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos",
            "authors": [
                "Zixi Yang",
                "Jiapeng Li",
                "Muxi Diao",
                "Yinuo Jing",
                "Kongming Liang"
            ],
            "arxiv_id": "2510.08936v1",
            "summary": "Recently, Multi-modal Large Language Models (MLLMs) have demonstrated\nsignificant performance across various video understanding tasks. However,\ntheir robustness, particularly when faced with manipulated video content,\nremains largely unexplored. In this paper, we introduce Ro-Bench, the first\nbenchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)\ncounterfactual video test sets. Ro-Bench incorporates high-quality, diverse and\ntemporally relevant video data, by editing Style, Object, Background and their\ncompositions. We evaluated eight recent video MLLMs and found that current\nmodels exhibit substantial performance degradation on Ro-Bench when exposed to\ncounterfactual video content. Furthermore, we demonstrate that fine-tuning\nMLLMs with counterfactual data enhances robustness, achieving a 21.73%\nperformance increase on Ro-Bench and a 12.78% improvement across 20 tasks in\nthe MVBench dataset. These findings underscore the effectiveness of\ncounterfactual data in enhancing the video understanding ability of MLLMs. The\ncode and data will be released shortly.",
            "headline_zh": "提出Ro-Bench基准以评估多模态大语言模型在反事实视频中的鲁棒性",
            "intro_zh": [
                "多模态大语言模型在视频理解任务中表现优异，但面对操纵视频内容时鲁棒性未知",
                "通过编辑风格、对象、背景及其组合，构建动态分布外反事实视频测试集",
                "评估显示模型性能显著下降，反事实数据微调可提升鲁棒性，在Ro-Bench和MVBench上分别提高21.73%和12.78%"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视频理解",
                "鲁棒性评估",
                "反事实数据",
                "分布外测试",
                "基准构建"
            ],
            "_index": 77
        },
        {
            "title": "Defense against Unauthorized Distillation in Image Restoration via Feature Space Perturbation",
            "authors": [
                "Han Hu",
                "Zhuoran Zheng",
                "Chen Lyu"
            ],
            "arxiv_id": "2510.08925v1",
            "summary": "Knowledge distillation (KD) attacks pose a significant threat to deep model\nintellectual property by enabling adversaries to train student networks using a\nteacher model's outputs. While recent defenses in image classification have\nsuccessfully disrupted KD by perturbing output probabilities, extending these\nmethods to image restoration is difficult. Unlike classification, restoration\nis a generative task with continuous, high-dimensional outputs that depend on\nspatial coherence and fine details. Minor perturbations are often insufficient,\nas students can still learn the underlying mapping.To address this, we propose\nAdaptive Singular Value Perturbation (ASVP), a runtime defense tailored for\nimage restoration models. ASVP operates on internal feature maps of the teacher\nusing singular value decomposition (SVD). It amplifies the topk singular values\nto inject structured, high-frequency perturbations, disrupting the alignment\nneeded for distillation. This hinders student learning while preserving the\nteacher's output quality.We evaluate ASVP across five image restoration tasks:\nsuper-resolution, low-light enhancement, underwater enhancement, dehazing, and\nderaining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by\n60-75%, with negligible impact on the teacher's performance. Compared to prior\nmethods, ASVP offers a stronger and more consistent defense.Our approach\nprovides a practical solution to protect open-source restoration models from\nunauthorized knowledge distillation.",
            "headline_zh": "提出自适应奇异值扰动以防御图像恢复模型的知识蒸馏攻击",
            "intro_zh": [
                "核心问题：图像恢复模型易受知识蒸馏攻击，传统扰动方法在连续高维输出中效果不佳",
                "方法要点：在特征空间使用奇异值分解，放大顶部奇异值注入高频扰动",
                "实验或效果：在多个恢复任务中，显著降低学生模型性能，教师模型输出质量几乎不变"
            ],
            "tags_zh": [
                "图像恢复",
                "知识蒸馏防御",
                "奇异值分解",
                "特征扰动",
                "模型保护"
            ],
            "_index": 78
        },
        {
            "title": "SegTrans: Transferable Adversarial Examples for Segmentation Models",
            "authors": [
                "Yufei Song",
                "Ziqi Zhou",
                "Qi Lu",
                "Hangtao Zhang",
                "Yifan Hu",
                "Lulu Xue",
                "Shengshan Hu",
                "Minghui Li",
                "Leo Yu Zhang"
            ],
            "arxiv_id": "2510.08922v1",
            "summary": "Segmentation models exhibit significant vulnerability to adversarial examples\nin white-box settings, but existing adversarial attack methods often show poor\ntransferability across different segmentation models. While some researchers\nhave explored transfer-based adversarial attack (i.e., transfer attack) methods\nfor segmentation models, the complex contextual dependencies within these\nmodels and the feature distribution gaps between surrogate and target models\nresult in unsatisfactory transfer success rates. To address these issues, we\npropose SegTrans, a novel transfer attack framework that divides the input\nsample into multiple local regions and remaps their semantic information to\ngenerate diverse enhanced samples. These enhanced samples replace the original\nones for perturbation optimization, thereby improving the transferability of\nadversarial examples across different segmentation models. Unlike existing\nmethods, SegTrans only retains local semantic information from the original\ninput, rather than using global semantic information to optimize perturbations.\nExtensive experiments on two benchmark datasets, PASCAL VOC and Cityscapes,\nfour different segmentation models, and three backbone networks show that\nSegTrans significantly improves adversarial transfer success rates without\nintroducing additional computational overhead. Compared to the current\nstate-of-the-art methods, SegTrans achieves an average increase of 8.55% in\ntransfer attack success rate and improves computational efficiency by more than\n100%.",
            "headline_zh": "提出SegTrans框架以提升分割模型间对抗样本的可迁移性",
            "intro_zh": [
                "分割模型在对抗攻击中可迁移性差，源于上下文依赖和特征分布差异",
                "方法将输入分区域并重映射语义信息，生成增强样本优化扰动",
                "实验在多个数据集和模型上显示，迁移成功率平均提升8.55%，效率提高超100%"
            ],
            "tags_zh": [
                "分割模型",
                "对抗攻击",
                "可迁移性",
                "语义重映射",
                "扰动优化"
            ],
            "_index": 79
        },
        {
            "title": "PHyCLIP: $\\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning",
            "authors": [
                "Daiki Yoshikawa",
                "Takashi Matsubara"
            ],
            "arxiv_id": "2510.08919v1",
            "summary": "Vision-language models have achieved remarkable success in multi-modal\nrepresentation learning from large-scale pairs of visual scenes and linguistic\ndescriptions. However, they still struggle to simultaneously express two\ndistinct types of semantic structures: the hierarchy within a concept family\n(e.g., dog $\\preceq$ mammal $\\preceq$ animal) and the compositionality across\ndifferent concept families (e.g., \"a dog in a car\" $\\preceq$ dog, car). Recent\nworks have addressed this challenge by employing hyperbolic space, which\nefficiently captures tree-like hierarchy, yet its suitability for representing\ncompositionality remains unclear. To resolve this dilemma, we propose PHyCLIP,\nwhich employs an $\\ell_1$-Product metric on a Cartesian product of Hyperbolic\nfactors. With our design, intra-family hierarchies emerge within individual\nhyperbolic factors, and cross-family composition is captured by the\n$\\ell_1$-product metric, analogous to a Boolean algebra. Experiments on\nzero-shot classification, retrieval, hierarchical classification, and\ncompositional understanding tasks demonstrate that PHyCLIP outperforms existing\nsingle-space approaches and offers more interpretable structures in the\nembedding space.",
            "headline_zh": "提出PHyCLIP以统一视觉语言表示中的层次性和组合性",
            "intro_zh": [
                "视觉语言模型难以同时表达概念族内层次和跨族组合性",
                "使用双曲空间ℓ₁-积度量，分别捕获层次和组合结构",
                "在零样本分类等任务中优于单空间方法，嵌入空间更可解释"
            ],
            "tags_zh": [
                "视觉语言模型",
                "双曲空间",
                "层次表示",
                "组合表示",
                "零样本学习",
                "嵌入学习"
            ],
            "_index": 80
        },
        {
            "title": "Modeling Time-Lapse Trajectories to Characterize Cranberry Growth",
            "authors": [
                "Ronan John",
                "Anis Chihoub",
                "Ryan Meegan",
                "Gina Sidelli",
                "Jeffery Neyhart",
                "Peter Oudemans",
                "Kristin Dana"
            ],
            "arxiv_id": "2510.08901v1",
            "summary": "Change monitoring is an essential task for cranberry farming as it provides\nboth breeders and growers with the ability to analyze growth, predict yield,\nand make treatment decisions. However, this task is often done manually,\nrequiring significant time on the part of a cranberry grower or breeder. Deep\nlearning based change monitoring holds promise, despite the caveat of\nhard-to-interpret high dimensional features and hand-annotations for\nfine-tuning. To address this gap, we introduce a method for modeling crop\ngrowth based on fine-tuning vision transformers (ViTs) using a self-supervised\napproach that avoids tedious image annotations. We use a two-fold pretext task\n(time regression and class prediction) to learn a latent space for the\ntime-lapse evolution of plant and fruit appearance. The resulting 2D temporal\ntracks provide an interpretable time-series model of crop growth that can be\nused to: 1) predict growth over time and 2) distinguish temporal differences of\ncranberry varieties. We also provide a novel time-lapse dataset of cranberry\nfruit featuring eight distinct varieties, observed 52 times over the growing\nseason (span of around four months), annotated with information about fungicide\napplication, yield, and rot. Our approach is general and can be applied to\nother crops and applications (code and dataset can be found at https://github.\ncom/ronan-39/tlt/).",
            "headline_zh": "提出自监督视觉变换器方法以建模蔓越莓生长轨迹，解决手动监测问题。",
            "intro_zh": [
                "核心问题：蔓越莓生长监测依赖手动，耗时且需精细标注。",
                "方法要点：使用时间回归和类别预测的自监督ViT，学习时间序列潜在空间。",
                "实验或效果：构建新数据集，模型可预测生长和区分品种差异。"
            ],
            "tags_zh": [
                "自监督学习",
                "视觉变换器",
                "时间序列建模",
                "作物监测",
                "蔓越莓生长"
            ],
            "_index": 81
        }
    ]
}