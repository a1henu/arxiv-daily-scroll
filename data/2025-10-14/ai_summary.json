{
    "papers": [
        {
            "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
            "authors": [
                "Kartik Narayan",
                "Yang Xu",
                "Tian Cao",
                "Kavya Nerella",
                "Vishal M. Patel",
                "Navid Shiee",
                "Peter Grasch",
                "Chao Jia",
                "Yinfei Yang",
                "Zhe Gan"
            ],
            "arxiv_id": "2510.12801v1",
            "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.",
            "headline_zh": "提出DeepMMSearch-R1以解决多模态LLM在动态网络搜索中的效率问题",
            "intro_zh": [
                "现有方法存在管道僵化、搜索调用过多和查询构建不佳等问题",
                "采用两阶段训练：监督微调与在线强化学习，支持多轮图像和文本搜索",
                "在知识密集型基准测试中表现优越，提供多模态网络搜索洞见"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "检索增强生成",
                "网络搜索",
                "强化学习",
                "多模态VQA数据集",
                "图像搜索优化"
            ],
            "_index": 0
        },
        {
            "title": "Detect Anything via Next Point Prediction",
            "authors": [
                "Qing Jiang",
                "Junan Huo",
                "Xingyu Chen",
                "Yuda Xiong",
                "Zhaoyang Zeng",
                "Yihao Chen",
                "Tianhe Ren",
                "Junzhi Yu",
                "Lei Zhang"
            ],
            "arxiv_id": "2510.12798v1",
            "summary": "Object detection has long been dominated by traditional coordinate\nregression-based models, such as YOLO, DETR, and Grounding DINO. Although\nrecent efforts have attempted to leverage MLLMs to tackle this task, they face\nchallenges like low recall rate, duplicate predictions, coordinate\nmisalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a\n3B-scale MLLM that achieves state-of-the-art object perception performance. On\nbenchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or\nexceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot\nsetting. This is enabled by three key designs: 1) Task Formulation: we use\nspecial tokens to represent quantized coordinates from 0 to 999, reducing the\nmodel's learning difficulty and improving token efficiency for coordinate\nprediction; 2) Data Engines: we construct multiple data engines to generate\nhigh-quality grounding, referring, and pointing data, providing semantically\nrich supervision for training; \\3) Training Pipelines: we employ a two-stage\ntraining process, combining supervised fine-tuning on 22 million data with\nGRPO-based reinforcement post-training. This RL post-training leverages\ngeometry-aware rewards to effectively bridge the discrete-to-continuous\ncoordinate prediction gap, improve box accuracy, and mitigate undesirable\nbehaviors like duplicate predictions that stem from the teacher-guided nature\nof the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent\nlanguage understanding enables versatile capabilities such as object referring,\npointing, visual prompting, GUI grounding, spatial referring, OCR and\nkey-pointing, all systematically evaluated on dedicated benchmarks. We believe\nthat Rex-Omni paves the way for more versatile and language-aware visual\nperception systems.",
            "headline_zh": "提出Rex-Omni模型，通过下一点预测实现零样本物体检测与多任务感知",
            "intro_zh": [
                "传统回归模型在物体检测中存在召回率低、重复预测等问题，MLLMs应用受限。",
                "采用量化坐标表示、多数据引擎生成和两阶段训练，提升坐标预测精度与效率。",
                "在COCO和LVIS基准上零样本性能媲美回归模型，并支持多种语言感知任务。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "物体检测",
                "零样本学习",
                "坐标量化",
                "强化学习训练",
                "视觉语言任务"
            ],
            "_index": 1
        },
        {
            "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
            "authors": [
                "Yingyan Li",
                "Shuyao Shang",
                "Weisong Liu",
                "Bing Zhan",
                "Haochen Wang",
                "Yuqi Wang",
                "Yuntao Chen",
                "Xiaoman Wang",
                "Yasong An",
                "Chufeng Tang",
                "Lu Hou",
                "Lue Fan",
                "Zhaoxiang Zhang"
            ],
            "arxiv_id": "2510.12796v1",
            "summary": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a\npromising path to achieving a more generalized driving intelligence. However,\nVLA models are limited by a ``supervision deficit'': the vast model capacity is\nsupervised by sparse, low-dimensional actions, leaving much of their\nrepresentational power underutilized. To remedy this, we propose\n\\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to\npredict future images. This task generates a dense, self-supervised signal that\ncompels the model to learn the underlying dynamics of the driving environment.\nWe showcase the paradigm's versatility by instantiating it for two dominant VLA\narchetypes: an autoregressive world model for VLAs that use discrete visual\ntokens, and a diffusion world model for those operating on continuous visual\nfeatures. Building on the rich representations learned from world modeling, we\nintroduce a lightweight action expert to address the inference latency for\nreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a\n680x larger in-house dataset demonstrate that DriveVLA-W0 significantly\noutperforms BEV and VLA baselines. Crucially, it amplifies the data scaling\nlaw, showing that performance gains accelerate as the training dataset size\nincreases.",
            "headline_zh": "提出DriveVLA-W0训练范式，利用世界模型预测未来图像以增强自动驾驶VLA模型泛化能力。",
            "intro_zh": [
                "核心问题：VLA模型存在监督不足，模型容量大但仅由稀疏动作监督，表示能力未充分利用。",
                "方法要点：采用世界建模预测未来图像，提供密集自监督信号，学习驾驶环境动态。",
                "实验效果：在NAVSIM基准和内部数据集上显著超越基线，并放大数据缩放定律。"
            ],
            "tags_zh": [
                "自动驾驶",
                "视觉语言动作模型",
                "世界建模",
                "自监督学习",
                "数据缩放定律"
            ],
            "_index": 2
        },
        {
            "title": "CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations",
            "authors": [
                "Caner Korkmaz",
                "Brighton Nuwagira",
                "Barış Coşkunuzer",
                "Tolga Birdal"
            ],
            "arxiv_id": "2510.12795v1",
            "summary": "We present CuMPerLay, a novel differentiable vectorization layer that enables\nthe integration of Cubical Multiparameter Persistence (CMP) into deep learning\npipelines. While CMP presents a natural and powerful way to topologically work\nwith images, its use is hindered by the complexity of multifiltration\nstructures as well as the vectorization of CMP. In face of these challenges, we\nintroduce a new algorithm for vectorizing MP homologies of cubical complexes.\nOur CuMPerLay decomposes the CMP into a combination of individual, learnable\nsingle-parameter persistence, where the bifiltration functions are jointly\nlearned. Thanks to the differentiability, its robust topological feature\nvectors can be seamlessly used within state-of-the-art architectures such as\nSwin Transformers. We establish theoretical guarantees for the stability of our\nvectorization under generalized Wasserstein metrics. Our experiments on\nbenchmark medical imaging and computer vision datasets show the benefit\nCuMPerLay on classification and segmentation performance, particularly in\nlimited-data scenarios. Overall, CuMPerLay offers a promising direction for\nintegrating global structural information into deep networks for structured\nimage analysis.",
            "headline_zh": "提出CuMPerLay以将立方多参数持久性集成到深度学习管道中",
            "intro_zh": [
                "立方多参数持久性向量化复杂，阻碍其在图像拓扑分析中的应用",
                "CuMPerLay将多参数持久性分解为可学习的单参数持久性组合",
                "实验显示在医学影像和计算机视觉任务中提升分类和分割性能"
            ],
            "tags_zh": [
                "多参数持久性",
                "深度学习集成",
                "拓扑数据分析",
                "图像分析",
                "可微分层"
            ],
            "_index": 3
        },
        {
            "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
            "authors": [
                "Long Cui",
                "Weiyun Wang",
                "Jie Shao",
                "Zichen Wen",
                "Gen Luo",
                "Linfeng Zhang",
                "Yanting Zhang",
                "Yu Qiao",
                "Wenhai Wang"
            ],
            "arxiv_id": "2510.12793v1",
            "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.",
            "headline_zh": "提出ViCO训练策略，通过语义感知动态调整视觉令牌数量以降低MLLM推理成本",
            "intro_zh": [
                "核心问题：多模态大语言模型因图像输入增加视觉令牌，导致推理成本上升",
                "方法要点：使用多MLP连接器基于语义复杂度压缩视觉令牌，并最小化KL散度",
                "实验或效果：减少视觉令牌达50%，保持感知、推理和OCR能力"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视觉令牌压缩",
                "语义复杂度",
                "动态高分辨率",
                "推理效率优化"
            ],
            "_index": 4
        },
        {
            "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
            "authors": [
                "Kevin Li",
                "Manuel Brack",
                "Sudeep Katakol",
                "Hareesh Ravi",
                "Ajinkya Kale"
            ],
            "arxiv_id": "2510.12789v1",
            "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.",
            "headline_zh": "提出UniFusion，利用冻结VLM作为统一编码器，提升扩散模型的跨模态生成与编辑能力。",
            "intro_zh": [
                "现有扩散模型依赖独立图像和文本编码器，限制跨模态推理和知识迁移。",
                "UniFusion引入Layerwise Attention Pooling机制，从冻结VLM提取多层级语义和细节，条件化扩散生成。",
                "实验显示，LAP在文本-图像对齐和视觉信息迁移上优于浅层融合，并零样本泛化到多图像编辑任务。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "扩散模型",
                "跨模态生成",
                "图像编辑",
                "统一编码器",
                "零样本泛化"
            ],
            "_index": 5
        },
        {
            "title": "Efficient Real-World Deblurring using Single Images: AIM 2025 Challenge Report",
            "authors": [
                "Daniel Feijoo",
                "Paula Garrido-Mellado",
                "Marcos V. Conde",
                "Jaesung Rim",
                "Alvaro Garcia",
                "Sunghyun Cho",
                "Radu Timofte"
            ],
            "arxiv_id": "2510.12788v1",
            "summary": "This paper reviews the AIM 2025 Efficient Real-World Deblurring using Single\nImages Challenge, which aims to advance in efficient real-blur restoration. The\nchallenge is based on a new test set based on the well known RSBlur dataset.\nPairs of blur and degraded images in this dataset are captured using a\ndouble-camera system. Participant were tasked with developing solutions to\neffectively deblur these type of images while fulfilling strict efficiency\nconstraints: fewer than 5 million model parameters and a computational budget\nunder 200 GMACs. A total of 71 participants registered, with 4 teams finally\nsubmitting valid solutions. The top-performing approach achieved a PSNR of\n31.1298 dB, showcasing the potential of efficient methods in this domain. This\npaper provides a comprehensive overview of the challenge, compares the proposed\nsolutions, and serves as a valuable reference for researchers in efficient\nreal-world image deblurring.",
            "headline_zh": "报告AIM 2025高效真实世界单图像去模糊挑战，评估方法在效率约束下的性能",
            "intro_zh": [
                "核心问题：在真实世界图像中高效去模糊，需满足参数少于500万和计算量低于200 GMACs",
                "方法要点：基于RSBlur数据集，使用双相机系统捕获模糊与退化图像对进行方法开发",
                "实验或效果：最佳方法PSNR达31.1298 dB，4个团队提交有效方案，展示高效去模糊潜力"
            ],
            "tags_zh": [
                "图像去模糊",
                "效率约束",
                "真实世界图像",
                "PSNR评估",
                "挑战报告"
            ],
            "_index": 6
        },
        {
            "title": "MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars",
            "authors": [
                "Felix Taubner",
                "Ruihang Zhang",
                "Mathieu Tuli",
                "Sherwin Bahmani",
                "David B. Lindell"
            ],
            "arxiv_id": "2510.12785v1",
            "summary": "Digital human avatars aim to simulate the dynamic appearance of humans in\nvirtual environments, enabling immersive experiences across gaming, film,\nvirtual reality, and more. However, the conventional process for creating and\nanimating photorealistic human avatars is expensive and time-consuming,\nrequiring large camera capture rigs and significant manual effort from\nprofessional 3D artists. With the advent of capable image and video generation\nmodels, recent methods enable automatic rendering of realistic animated avatars\nfrom a single casually captured reference image of a target subject. While\nthese techniques significantly lower barriers to avatar creation and offer\ncompelling realism, they lack constraints provided by multi-view information or\nan explicit 3D representation. So, image quality and realism degrade when\nrendered from viewpoints that deviate strongly from the reference image. Here,\nwe build a video model that generates animatable multi-view videos of digital\nhumans based on a single reference image and target expressions. Our model,\nMVP4D, is based on a state-of-the-art pre-trained video diffusion model and\ngenerates hundreds of frames simultaneously from viewpoints varying by up to\n360 degrees around a target subject. We show how to distill the outputs of this\nmodel into a 4D avatar that can be rendered in real-time. Our approach\nsignificantly improves the realism, temporal consistency, and 3D consistency of\ngenerated avatars compared to previous methods.",
            "headline_zh": "提出MVP4D模型，基于单参考图像生成可动画4D虚拟人，提升多视角真实感",
            "intro_zh": [
                "核心问题：单图像生成虚拟人时，视角偏离导致质量下降，缺乏多视角约束",
                "方法要点：基于预训练视频扩散模型，生成多视角视频并蒸馏为实时渲染4D虚拟人",
                "实验或效果：相比先前方法，显著改进真实感、时间一致性和3D一致性"
            ],
            "tags_zh": [
                "4D虚拟人",
                "多视角视频生成",
                "视频扩散模型",
                "实时渲染",
                "单图像动画"
            ],
            "_index": 7
        },
        {
            "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
            "authors": [
                "Weiyang Jin",
                "Yuwei Niu",
                "Jiaqi Liao",
                "Chengqi Duan",
                "Aoxue Li",
                "Shenghua Gao",
                "Xihui Liu"
            ],
            "arxiv_id": "2510.12784v1",
            "summary": "Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a \\textbf{global reward}\nensures the correctness of the overall visual semantics and layout, while a\n\\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to \\textbf{88.37} and on T2I-ReasonBench from 43.82\nto \\textbf{46.75}. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding.",
            "headline_zh": "提出SRUM自奖励框架以解决统一多模态模型中视觉理解与生成能力不匹配问题",
            "intro_zh": [
                "核心问题：统一多模态模型中视觉理解能力无法有效迁移到视觉生成，导致生成图像不忠实",
                "方法要点：设计全局-局部双奖励系统，利用理解模块作为内部评估器提供多尺度反馈信号",
                "实验或效果：在T2I-CompBench和T2I-ReasonBench基准上性能显著提升，无需额外人工标注数据"
            ],
            "tags_zh": [
                "统一多模态模型",
                "自奖励学习",
                "视觉生成",
                "理解-生成对齐",
                "多尺度奖励",
                "后训练框架"
            ],
            "_index": 8
        },
        {
            "title": "What If : Understanding Motion Through Sparse Interactions",
            "authors": [
                "Stefan Andreas Baumann",
                "Nick Stracke",
                "Timy Phan",
                "Björn Ommer"
            ],
            "arxiv_id": "2510.12777v1",
            "summary": "Understanding the dynamics of a physical scene involves reasoning about the\ndiverse ways it can potentially change, especially as a result of local\ninteractions. We present the Flow Poke Transformer (FPT), a novel framework for\ndirectly predicting the distribution of local motion, conditioned on sparse\ninteractions termed \"pokes\". Unlike traditional methods that typically only\nenable dense sampling of a single realization of scene dynamics, FPT provides\nan interpretable directly accessible representation of multi-modal scene\nmotion, its dependency on physical interactions and the inherent uncertainties\nof scene dynamics. We also evaluate our model on several downstream tasks to\nenable comparisons with prior methods and highlight the flexibility of our\napproach. On dense face motion generation, our generic pre-trained model\nsurpasses specialized baselines. FPT can be fine-tuned in strongly\nout-of-distribution tasks such as synthetic datasets to enable significant\nimprovements over in-domain methods in articulated object motion estimation.\nAdditionally, predicting explicit motion distributions directly enables our\nmethod to achieve competitive performance on tasks like moving part\nsegmentation from pokes which further demonstrates the versatility of our FPT.\nCode and models are publicly available at\nhttps://compvis.github.io/flow-poke-transformer.",
            "headline_zh": "提出Flow Poke Transformer以通过稀疏交互预测多模态场景运动分布",
            "intro_zh": [
                "核心问题：理解物理场景动态变化，特别是基于局部稀疏交互的多模态运动预测。",
                "方法要点：使用Flow Poke Transformer直接预测局部运动分布，条件于稀疏poke交互。",
                "实验或效果：在密集人脸运动生成和关节物体运动估计等任务中超越基线方法。"
            ],
            "tags_zh": [
                "场景动态理解",
                "稀疏交互",
                "运动分布预测",
                "Transformer模型",
                "多模态预测"
            ],
            "_index": 9
        },
        {
            "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction",
            "authors": [
                "Fengzhi Guo",
                "Chih-Chuan Hsu",
                "Sihao Ding",
                "Cheng Zhang"
            ],
            "arxiv_id": "2510.12768v1",
            "summary": "Reconstructing dynamic 3D scenes from monocular input is fundamentally\nunder-constrained, with ambiguities arising from occlusion and extreme novel\nviews. While dynamic Gaussian Splatting offers an efficient representation,\nvanilla models optimize all Gaussian primitives uniformly, ignoring whether\nthey are well or poorly observed. This limitation leads to motion drifts under\nocclusion and degraded synthesis when extrapolating to unseen views. We argue\nthat uncertainty matters: Gaussians with recurring observations across views\nand time act as reliable anchors to guide motion, whereas those with limited\nvisibility are treated as less reliable. To this end, we introduce USplat4D, a\nnovel Uncertainty-aware dynamic Gaussian Splatting framework that propagates\nreliable motion cues to enhance 4D reconstruction. Our key insight is to\nestimate time-varying per-Gaussian uncertainty and leverages it to construct a\nspatio-temporal graph for uncertainty-aware optimization. Experiments on\ndiverse real and synthetic datasets show that explicitly modeling uncertainty\nconsistently improves dynamic Gaussian Splatting models, yielding more stable\ngeometry under occlusion and high-quality synthesis at extreme viewpoints.",
            "headline_zh": "提出USplat4D框架，通过不确定性建模提升单目4D动态重建质量",
            "intro_zh": [
                "核心问题：单目动态3D重建存在遮挡和极端视角下的模糊性，导致运动漂移和合成质量下降。",
                "方法要点：引入时间变化的高斯不确定性估计，构建时空图进行不确定性感知优化。",
                "实验或效果：在真实和合成数据集上验证，模型在遮挡下几何更稳定，极端视角合成质量高。"
            ],
            "tags_zh": [
                "动态高斯泼溅",
                "不确定性建模",
                "单目4D重建",
                "时空图优化",
                "运动漂移抑制"
            ],
            "_index": 10
        },
        {
            "title": "Efficient Perceptual Image Super Resolution: AIM 2025 Study and Benchmark",
            "authors": [
                "Bruno Longarela",
                "Marcos V. Conde",
                "Alvaro Garcia",
                "Radu Timofte"
            ],
            "arxiv_id": "2510.12765v1",
            "summary": "This paper presents a comprehensive study and benchmark on Efficient\nPerceptual Super-Resolution (EPSR). While significant progress has been made in\nefficient PSNR-oriented super resolution, approaches focusing on perceptual\nquality metrics remain relatively inefficient. Motivated by this gap, we aim to\nreplicate or improve the perceptual results of Real-ESRGAN while meeting strict\nefficiency constraints: a maximum of 5M parameters and 2000 GFLOPs, calculated\nfor an input size of 960x540 pixels. The proposed solutions were evaluated on a\nnovel dataset consisting of 500 test images of 4K resolution, each degraded\nusing multiple degradation types, without providing the original high-quality\ncounterparts. This design aims to reflect realistic deployment conditions and\nserves as a diverse and challenging benchmark. The top-performing approach\nmanages to outperform Real-ESRGAN across all benchmark datasets, demonstrating\nthe potential of efficient methods in the perceptual domain. This paper\nestablishes the modern baselines for efficient perceptual super resolution.",
            "headline_zh": "提出高效感知超分辨率方法，在严格效率约束下超越Real-ESRGAN。",
            "intro_zh": [
                "核心问题：感知质量导向的超分辨率方法效率低下，需在5M参数和2000 GFLOPs内优化。",
                "方法要点：设计高效模型，复制或改进Real-ESRGAN的感知结果，满足资源限制。",
                "实验或效果：在500张4K退化图像数据集上评估，最优方法在所有基准中表现更优。"
            ],
            "tags_zh": [
                "高效感知超分辨率",
                "参数约束优化",
                "4K图像基准",
                "Real-ESRGAN比较"
            ],
            "_index": 11
        },
        {
            "title": "AnyUp: Universal Feature Upsampling",
            "authors": [
                "Thomas Wimmer",
                "Prune Truong",
                "Marie-Julie Rakotosaona",
                "Michael Oechsle",
                "Federico Tombari",
                "Bernt Schiele",
                "Jan Eric Lenssen"
            ],
            "arxiv_id": "2510.12764v1",
            "summary": "We introduce AnyUp, a method for feature upsampling that can be applied to\nany vision feature at any resolution, without encoder-specific training.\nExisting learning-based upsamplers for features like DINO or CLIP need to be\nre-trained for every feature extractor and thus do not generalize to different\nfeature types at inference time. In this work, we propose an inference-time\nfeature-agnostic upsampling architecture to alleviate this limitation and\nimprove upsampling quality. In our experiments, AnyUp sets a new state of the\nart for upsampled features, generalizes to different feature types, and\npreserves feature semantics while being efficient and easy to apply to a wide\nrange of downstream tasks.",
            "headline_zh": "提出AnyUp方法，实现通用特征上采样，无需特定编码器训练。",
            "intro_zh": [
                "现有基于学习的特征上采样方法需为每个特征提取器重新训练，缺乏通用性。",
                "AnyUp采用推理时特征无关的上采样架构，提升上采样质量和语义保持。",
                "实验显示AnyUp在多种特征类型上达到新SOTA，高效适用于下游任务。"
            ],
            "tags_zh": [
                "特征上采样",
                "通用方法",
                "推理时优化",
                "计算机视觉",
                "语义保持"
            ],
            "_index": 12
        },
        {
            "title": "PET Head Motion Estimation Using Supervised Deep Learning with Attention",
            "authors": [
                "Zhuotong Cai",
                "Tianyi Zeng",
                "Jiazhen Zhang",
                "Eléonore V. Lieffrig",
                "Kathryn Fontaine",
                "Chenyu You",
                "Enette Mae Revilla",
                "James S. Duncan",
                "Jingmin Xin",
                "Yihuan Lu",
                "John A. Onofrey"
            ],
            "arxiv_id": "2510.12758v1",
            "summary": "Head movement poses a significant challenge in brain positron emission\ntomography (PET) imaging, resulting in image artifacts and tracer uptake\nquantification inaccuracies. Effective head motion estimation and correction\nare crucial for precise quantitative image analysis and accurate diagnosis of\nneurological disorders. Hardware-based motion tracking (HMT) has limited\napplicability in real-world clinical practice. To overcome this limitation, we\npropose a deep-learning head motion correction approach with cross-attention\n(DL-HMC++) to predict rigid head motion from one-second 3D PET raw data.\nDL-HMC++ is trained in a supervised manner by leveraging existing dynamic PET\nscans with gold-standard motion measurements from external HMT. We evaluate\nDL-HMC++ on two PET scanners (HRRT and mCT) and four radiotracers (18F-FDG,\n18F-FPEB, 11C-UCB-J, and 11C-LSN3172176) to demonstrate the effectiveness and\ngeneralization of the approach in large cohort PET studies. Quantitative and\nqualitative results demonstrate that DL-HMC++ consistently outperforms\nstate-of-the-art data-driven motion estimation methods, producing motion-free\nimages with clear delineation of brain structures and reduced motion artifacts\nthat are indistinguishable from gold-standard HMT. Brain region of interest\nstandard uptake value analysis exhibits average difference ratios between\nDL-HMC++ and gold-standard HMT to be 1.2 plus-minus 0.5% for HRRT and 0.5\nplus-minus 0.2% for mCT. DL-HMC++ demonstrates the potential for data-driven\nPET head motion correction to remove the burden of HMT, making motion\ncorrection accessible to clinical populations beyond research settings. The\ncode is available at https://github.com/maxxxxxxcai/DL-HMC-TMI.",
            "headline_zh": "提出基于注意力机制的深度学习模型DL-HMC++，用于PET头部运动估计与校正。",
            "intro_zh": [
                "头部运动导致PET成像伪影和定量误差，影响神经疾病诊断。",
                "使用监督学习和交叉注意力，从3D PET原始数据预测刚性头部运动。",
                "在多种扫描器和示踪剂上验证，性能优于现有方法，接近金标准。"
            ],
            "tags_zh": [
                "PET成像",
                "头部运动校正",
                "深度学习",
                "注意力机制",
                "监督学习",
                "医学图像分析"
            ],
            "_index": 13
        },
        {
            "title": "E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization",
            "authors": [
                "Wenpu Li",
                "Bangyan Liao",
                "Yi Zhou",
                "Qi Xu",
                "Pian Wan",
                "Peidong Liu"
            ],
            "arxiv_id": "2510.12753v1",
            "summary": "The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in\n3D vision, has typically been addressed independently. For neuromorphic vision\n(e.g., event cameras), however, the lack of robust data association makes\nsolving the two problems separately an ill-posed challenge, especially in the\nabsence of supervision via ground truth. Existing works mitigate this\nill-posedness by either enforcing the smoothness of the flow field via an\nexplicit variational regularizer or leveraging explicit structure-and-motion\npriors in the parametrization to improve event alignment. The former notably\nintroduces bias in results and computational overhead, while the latter, which\nparametrizes the optical flow in terms of the scene depth and the camera\nmotion, often converges to suboptimal local minima. To address these issues, we\npropose an unsupervised framework that jointly optimizes egomotion and optical\nflow via implicit spatial-temporal and geometric regularization. First, by\nmodeling camera's egomotion as a continuous spline and optical flow as an\nimplicit neural representation, our method inherently embeds spatial-temporal\ncoherence through inductive biases. Second, we incorporate structure-and-motion\npriors through differential geometric constraints, bypassing explicit depth\nestimation while maintaining rigorous geometric consistency. As a result, our\nframework (called E-MoFlow) unifies egomotion and optical flow estimation via\nimplicit regularization under a fully unsupervised paradigm. Experiments\ndemonstrate its versatility to general 6-DoF motion scenarios, achieving\nstate-of-the-art performance among unsupervised methods and competitive even\nwith supervised approaches.",
            "headline_zh": "提出E-MoFlow框架，通过隐式正则化从事件数据联合学习自运动和光流",
            "intro_zh": [
                "核心问题：事件相机中自运动和光流独立估计因缺乏数据关联而病态，现有方法有偏差或易陷局部最优",
                "方法要点：建模自运动为连续样条、光流为隐式神经表示，嵌入时空一致性和微分几何约束",
                "实验或效果：在无监督方法中达到SOTA，对一般6-DoF运动场景有效，与有监督方法竞争"
            ],
            "tags_zh": [
                "事件相机",
                "自运动估计",
                "光流估计",
                "隐式正则化",
                "无监督学习",
                "神经表示"
            ],
            "_index": 14
        },
        {
            "title": "VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage",
            "authors": [
                "A. Alfarano",
                "L. Venturoli",
                "D. Negueruela del Castillo"
            ],
            "arxiv_id": "2510.12750v1",
            "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\ncapabilities in joint visual and linguistic tasks. However, existing Visual\nQuestion Answering (VQA) benchmarks often fail to evaluate deep semantic\nunderstanding, particularly in complex domains like visual art analysis.\nConfined to simple syntactic structures and surface-level attributes, these\nquestions fail to capture the diversity and depth of human visual inquiry. This\nlimitation incentivizes models to exploit statistical shortcuts rather than\nengage in visual reasoning. To address this gap, we introduce VQArt-Bench, a\nnew, large-scale VQA benchmark for the cultural heritage domain. This benchmark\nis constructed using a novel multi-agent pipeline where specialized agents\ncollaborate to generate nuanced, validated, and linguistically diverse\nquestions. The resulting benchmark is structured along relevant visual\nunderstanding dimensions that probe a model's ability to interpret symbolic\nmeaning, narratives, and complex visual relationships. Our evaluation of 14\nstate-of-the-art MLLMs on this benchmark reveals significant limitations in\ncurrent models, including a surprising weakness in simple counting tasks and a\nclear performance gap between proprietary and open-source models.",
            "headline_zh": "提出VQArt-Bench基准以评估文化遗产领域的视觉语义理解",
            "intro_zh": [
                "现有VQA基准缺乏深度语义评估，尤其在艺术分析中依赖表面属性",
                "采用多智能体管道生成多样、验证的问题，覆盖符号意义和复杂关系",
                "评估14个MLLM显示模型在计数任务和开源与专有模型间存在性能差距"
            ],
            "tags_zh": [
                "视觉问答基准",
                "文化遗产分析",
                "多模态大语言模型",
                "语义理解评估",
                "多智能体生成"
            ],
            "_index": 15
        },
        {
            "title": "SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding",
            "authors": [
                "Zhiliu Yang",
                "Jinyu Dai",
                "Jianyuan Zhang",
                "Zhu Yang"
            ],
            "arxiv_id": "2510.12749v1",
            "summary": "The scene perception, understanding, and simulation are fundamental\ntechniques for embodied-AI agents, while existing solutions are still prone to\nsegmentation deficiency, dynamic objects' interference, sensor data sparsity,\nand view-limitation problems. This paper proposes a novel framework, named\nSPORTS, for holistic scene understanding via tightly integrating Video Panoptic\nSegmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into\nan iterative and unified perspective. Firstly, VPS designs an adaptive\nattention-based geometric fusion mechanism to align cross-frame features via\nenrolling the pose, depth, and optical flow modality, which automatically\nadjust feature maps for different decoding stages. And a post-matching strategy\nis integrated to improve identities tracking. In VO, panoptic segmentation\nresults from VPS are combined with the optical flow map to improve the\nconfidence estimation of dynamic objects, which enhances the accuracy of the\ncamera pose estimation and completeness of the depth map generation via the\nlearning-based paradigm. Furthermore, the point-based rendering of SR is\nbeneficial from VO, transforming sparse point clouds into neural fields to\nsynthesize high-fidelity RGB views and twin panoptic views. Extensive\nexperiments on three public datasets demonstrate that our attention-based\nfeature fusion outperforms most existing state-of-the-art methods on the\nodometry, tracking, segmentation, and novel view synthesis tasks.",
            "headline_zh": "提出SPORTS框架以解决城市场景理解中的分割缺陷和动态干扰问题",
            "intro_zh": [
                "核心问题：现有方法存在分割缺陷、动态对象干扰、数据稀疏和视角限制",
                "方法要点：集成视频全景分割、视觉里程计和场景渲染，采用自适应注意力特征融合",
                "实验或效果：在三个公开数据集上，在里程计、跟踪、分割和新视图合成任务中表现优异"
            ],
            "tags_zh": [
                "视频全景分割",
                "视觉里程计",
                "场景渲染",
                "城市场景理解",
                "自适应注意力融合"
            ],
            "_index": 16
        },
        {
            "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution",
            "authors": [
                "Junhao Zhuang",
                "Shi Guo",
                "Xin Cai",
                "Xiaohui Li",
                "Yihao Liu",
                "Chun Yuan",
                "Tianfan Xue"
            ],
            "arxiv_id": "2510.12747v1",
            "summary": "Diffusion models have recently advanced video restoration, but applying them\nto real-world video super-resolution (VSR) remains challenging due to high\nlatency, prohibitive computation, and poor generalization to ultra-high\nresolutions. Our goal in this work is to make diffusion-based VSR practical by\nachieving efficiency, scalability, and real-time performance. To this end, we\npropose FlashVSR, the first diffusion-based one-step streaming framework\ntowards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408\nvideos on a single A100 GPU by combining three complementary innovations: (i) a\ntrain-friendly three-stage distillation pipeline that enables streaming\nsuper-resolution, (ii) locality-constrained sparse attention that cuts\nredundant computation while bridging the train-test resolution gap, and (iii) a\ntiny conditional decoder that accelerates reconstruction without sacrificing\nquality. To support large-scale training, we also construct VSR-120K, a new\ndataset with 120k videos and 180k images. Extensive experiments show that\nFlashVSR scales reliably to ultra-high resolutions and achieves\nstate-of-the-art performance with up to 12x speedup over prior one-step\ndiffusion VSR models. We will release the code, pretrained models, and dataset\nto foster future research in efficient diffusion-based VSR.",
            "headline_zh": "提出FlashVSR以实现实时扩散视频超分辨率",
            "intro_zh": [
                "扩散模型在视频超分辨率中面临高延迟、计算量大和泛化差问题",
                "采用三阶段蒸馏、稀疏注意力和微小解码器提升效率与实时性",
                "实验显示在A100 GPU上达17 FPS，性能领先且速度提升12倍"
            ],
            "tags_zh": [
                "视频超分辨率",
                "扩散模型",
                "实时处理",
                "蒸馏训练",
                "稀疏注意力",
                "高效解码"
            ],
            "_index": 17
        },
        {
            "title": "Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare",
            "authors": [
                "Adam Tupper",
                "Christian Gagné"
            ],
            "arxiv_id": "2510.12741v1",
            "summary": "Foundation models open up new possibilities for the use of AI in healthcare.\nHowever, even when pre-trained on health data, they still need to be fine-tuned\nfor specific downstream tasks. Furthermore, although foundation models reduce\nthe amount of training data required to achieve good performance, obtaining\nsufficient data is still a challenge. This is due, in part, to restrictions on\nsharing and aggregating data from different sources to protect patients'\nprivacy. One possible solution to this is to fine-tune foundation models via\nfederated learning across multiple participating clients (i.e., hospitals,\nclinics, etc.). In this work, we propose a new personalized federated\nfine-tuning method that learns orthogonal LoRA adapters to disentangle general\nand client-specific knowledge, enabling each client to fully exploit both their\nown data and the data of others. Our preliminary results on real-world\nfederated medical imaging tasks demonstrate that our approach is competitive\nagainst current federated fine-tuning methods.",
            "headline_zh": "提出个性化联邦微调方法，利用正交LoRA适配器解决医疗数据隐私下的模型适应问题",
            "intro_zh": [
                "核心问题：医疗数据隐私限制共享，导致基础模型微调数据不足",
                "方法要点：通过联邦学习学习正交LoRA适配器，分离通用与客户端特定知识",
                "实验或效果：在真实联邦医疗成像任务中，与现有方法竞争性表现"
            ],
            "tags_zh": [
                "联邦学习",
                "基础模型微调",
                "LoRA适配器",
                "医疗成像",
                "个性化学习",
                "数据隐私"
            ],
            "_index": 18
        },
        {
            "title": "HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions",
            "authors": [
                "Hang Yu",
                "Julian Jordan",
                "Julian Schmidt",
                "Silvan Lindner",
                "Alessandro Canevaro",
                "Wilhelm Stork"
            ],
            "arxiv_id": "2510.12733v1",
            "summary": "Safe and interpretable motion planning in complex urban environments needs to\nreason about bidirectional multi-agent interactions. This reasoning requires to\nestimate the costs of potential ego driving maneuvers. Many existing planners\ngenerate initial trajectories with sampling-based methods and refine them by\noptimizing on learned predictions of future environment states, which requires\na cost function that encodes the desired vehicle behavior. Designing such a\ncost function can be very challenging, especially if a wide range of complex\nurban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego\nproposal-conditioned predictions, a planner that integrates multimodal\ntrajectory proposals from a learned proposal model as heuristic priors into a\nMonte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,\nwe introduce an ego-conditioned occupancy prediction model, enabling\nconsistent, scene-aware reasoning. Our design significantly simplifies cost\nfunction design in refinement by considering proposal-driven guidance,\nrequiring only minimalistic grid-based cost terms. Evaluations on large-scale\nreal-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves\nstate-of-the-art performance, especially in safety and adaptability.",
            "headline_zh": "提出HYPE混合规划器，通过自我提案条件预测解决复杂城市环境中的安全运动规划问题",
            "intro_zh": [
                "核心问题：复杂城市环境中双向多智能体交互的安全运动规划，需估计自我驾驶操作成本",
                "方法要点：集成学习提案模型的多模态轨迹到MCTS精炼，使用自我条件占用预测建模交互",
                "实验或效果：在nuPlan和DeepUrban基准上实现先进性能，尤其在安全性和适应性方面"
            ],
            "tags_zh": [
                "运动规划",
                "多智能体交互",
                "蒙特卡洛树搜索",
                "自我条件预测",
                "城市驾驶",
                "成本函数简化"
            ],
            "_index": 19
        },
        {
            "title": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping",
            "authors": [
                "Xin Fei",
                "Zhixuan Xu",
                "Huaicong Fang",
                "Tianrui Zhang",
                "Lin Shao"
            ],
            "arxiv_id": "2510.12724v1",
            "summary": "Dexterous grasping remains a central challenge in robotics due to the\ncomplexity of its high-dimensional state and action space. We introduce T(R,O)\nGrasp, a diffusion-based framework that efficiently generates accurate and\ndiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,\na unified representation that models spatial transformations between robotic\nhands and objects while encoding their geometric properties. A graph diffusion\nmodel, coupled with an efficient inverse kinematics solver, supports both\nunconditioned and conditioned grasp synthesis. Extensive experiments on a\ndiverse set of dexterous hands show that T(R,O) Grasp achieves average success\nrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per\nsecond on an NVIDIA A100 40GB GPU, substantially outperforming existing\nbaselines. In addition, our approach is robust and generalizable across\nembodiments while significantly reducing memory consumption. More importantly,\nthe high inference speed enables closed-loop dexterous manipulation,\nunderscoring the potential of T(R,O) Grasp to scale into a foundation model for\ndexterous grasping.",
            "headline_zh": "提出T(R,O) Grasp框架，通过图扩散模型高效生成跨具身灵巧抓取。",
            "intro_zh": [
                "灵巧抓取面临高维状态和动作空间的复杂性挑战。",
                "使用T(R,O)图统一表示手-物空间变换，结合图扩散模型和逆运动学求解器。",
                "实验显示平均成功率94.83%，推理速度0.21秒，优于现有基线。"
            ],
            "tags_zh": [
                "灵巧抓取",
                "图扩散模型",
                "机器人具身",
                "逆运动学",
                "跨具身泛化"
            ],
            "_index": 20
        },
        {
            "title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception",
            "authors": [
                "Ziyang Ma",
                "Ruiyang Xu",
                "Zhenghao Xing",
                "Yunfei Chu",
                "Yuxuan Wang",
                "Jinzheng He",
                "Jin Xu",
                "Pheng-Ann Heng",
                "Kai Yu",
                "Junyang Lin",
                "Eng Siong Chng",
                "Xie Chen"
            ],
            "arxiv_id": "2510.12720v1",
            "summary": "Fine-grained perception of multimodal information is critical for advancing\nhuman-AI interaction. With recent progress in audio-visual technologies, Omni\nLanguage Models (OLMs), capable of processing audio and video signals in\nparallel, have emerged as a promising paradigm for achieving richer\nunderstanding and reasoning. However, their capacity to capture and describe\nfine-grained details remains limited explored. In this work, we present a\nsystematic and comprehensive investigation of omni detailed perception from the\nperspectives of the data pipeline, models, and benchmark. We first identify an\ninherent \"co-growth\" between detail and hallucination in current OLMs. To\naddress this, we propose Omni-Detective, an agentic data generation pipeline\nintegrating tool-calling, to autonomously produce highly detailed yet minimally\nhallucinatory multimodal data. Based on the data generated with Omni-Detective,\nwe train two captioning models: Audio-Captioner for audio-only detailed\nperception, and Omni-Captioner for audio-visual detailed perception. Under the\ncascade evaluation protocol, Audio-Captioner achieves the best performance on\nMMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and\ndelivering performance comparable to Gemini 2.5 Pro. On existing detailed\ncaptioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and\nachieves the best trade-off between detail and hallucination on the\nvideo-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni\ndetailed perception, we design Omni-Cloze, a novel cloze-style evaluation for\ndetailed audio, visual, and audio-visual captioning that ensures stable,\nefficient, and reliable assessment. Experimental results and analysis\ndemonstrate the effectiveness of Omni-Detective in generating high-quality\ndetailed captions, as well as the superiority of Omni-Cloze in evaluating such\ndetailed captions.",
            "headline_zh": "提出Omni-Captioner数据管道与模型，以解决全模态细粒度感知中的细节与幻觉问题。",
            "intro_zh": [
                "核心问题：当前全模态语言模型在细粒度感知中存在细节与幻觉共增长问题。",
                "方法要点：开发Omni-Detective数据生成管道，自动产生高细节、低幻觉的多模态数据。",
                "实验或效果：Omni-Captioner在多个基准测试中达到最优或最佳平衡性能。"
            ],
            "tags_zh": [
                "全模态感知",
                "细粒度描述",
                "数据生成管道",
                "音频-视觉模型",
                "基准评估"
            ],
            "_index": 21
        },
        {
            "title": "Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control",
            "authors": [
                "Se Hwan Jeon",
                "Ho Jae Lee",
                "Seungwoo Hong",
                "Sangbae Kim"
            ],
            "arxiv_id": "2510.12717v1",
            "summary": "Model Predictive Control (MPC) provides interpretable, tunable locomotion\ncontrollers grounded in physical models, but its robustness depends on frequent\nreplanning and is limited by model mismatch and real-time computational\nconstraints. Reinforcement Learning (RL), by contrast, can produce highly\nrobust behaviors through stochastic training but often lacks interpretability,\nsuffers from out-of-distribution failures, and requires intensive reward\nengineering. This work presents a GPU-parallelized residual architecture that\ntightly integrates MPC and RL by blending their outputs at the torque-control\nlevel. We develop a kinodynamic whole-body MPC formulation evaluated across\nthousands of agents in parallel at 100 Hz for RL training. The residual policy\nlearns to make targeted corrections to the MPC outputs, combining the\ninterpretability and constraint handling of model-based control with the\nadaptability of RL. The model-based control prior acts as a strong bias,\ninitializing and guiding the policy towards desirable behavior with a simple\nset of rewards. Compared to standalone MPC or end-to-end RL, our approach\nachieves higher sample efficiency, converges to greater asymptotic rewards,\nexpands the range of trackable velocity commands, and enables zero-shot\nadaptation to unseen gaits and uneven terrain.",
            "headline_zh": "提出残差MPC架构，融合强化学习与模型预测控制以提升机器人运动控制性能。",
            "intro_zh": [
                "核心问题：模型预测控制受限于模型失配和实时计算，强化学习缺乏可解释性和泛化性。",
                "方法要点：在扭矩控制层融合MPC与RL输出，利用GPU并行化实现高效训练。",
                "实验效果：相比单独方法，样本效率更高，适应未见步态和地形，提升奖励和速度跟踪范围。"
            ],
            "tags_zh": [
                "模型预测控制",
                "强化学习",
                "残差架构",
                "GPU并行化",
                "机器人运动控制",
                "扭矩控制"
            ],
            "_index": 22
        },
        {
            "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning",
            "authors": [
                "Xingang Guo",
                "Utkarsh Tyagi",
                "Advait Gosai",
                "Paula Vergara",
                "Ernesto Gabriel Hernández Montoya",
                "Chen Bo Calvin Zhang",
                "Bin Hu",
                "Yunzhong He",
                "Bing Liu",
                "Rakshith Sharma Srinivasa"
            ],
            "arxiv_id": "2510.12712v1",
            "summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce IRIS,\nan Interactive Reasoning with Images and Systems that evaluates MLLMs' ability\nto perceive, transform, and reason across complex visual-textual tasks under\nthe think with images paradigm. IRIS comprises 1,204 challenging, open-ended\nvision tasks (603 single-turn, 601 multi-turn) spanning across five diverse\ndomains, each paired with detailed rubrics to enable systematic evaluation. Our\nevaluation shows that current MLLMs struggle with tasks requiring effective\nintegration of vision and general-purpose tools. Even the strongest model\n(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent\ntool-use behaviors, with OpenAI models benefiting from diverse image\nmanipulations while Gemini-2.5-pro shows no improvement. By introducing the\nfirst benchmark centered on think with images, IRIS offers critical insights\nfor advancing visual intelligence in MLLMs.",
            "headline_zh": "提出IRIS基准以评估多模态大语言模型在工具辅助图像感知、变换与推理中的能力",
            "intro_zh": [
                "核心问题：现有基准多将图像视为静态输入，未充分探索MLLMs在动态图像变换与工具集成中的能力。",
                "方法要点：引入IRIS基准，包含1,204个开放视觉任务，涵盖感知、变换和推理，支持多轮交互。",
                "实验或效果：当前MLLMs表现不佳，最强模型仅达18.68%通过率，工具使用行为存在差异。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "图像交互推理",
                "工具集成",
                "基准评估",
                "视觉变换",
                "开放任务"
            ],
            "_index": 23
        },
        {
            "title": "Reflection-Based Task Adaptation for Self-Improving VLA",
            "authors": [
                "Baicheng Li",
                "Dong Wu",
                "Zike Yan",
                "Xinchen Liu",
                "Zecui Zeng",
                "Lusong Li",
                "Hongbin Zha"
            ],
            "arxiv_id": "2510.12710v1",
            "summary": "Pre-trained Vision-Language-Action (VLA) models represent a major leap\ntowards general-purpose robots, yet efficiently adapting them to novel,\nspecific tasks in-situ remains a significant hurdle. While reinforcement\nlearning (RL) is a promising avenue for such adaptation, the process often\nsuffers from low efficiency, hindering rapid task mastery. We introduce\nReflective Self-Adaptation, a framework for rapid, autonomous task adaptation\nwithout human intervention. Our framework establishes a self-improving loop\nwhere the agent learns from its own experience to enhance both strategy and\nexecution.\n  The core of our framework is a dual-pathway architecture that addresses the\nfull adaptation lifecycle. First, a Failure-Driven Reflective RL pathway\nenables rapid learning by using the VLM's causal reasoning to automatically\nsynthesize a targeted, dense reward function from failure analysis. This\nprovides a focused learning signal that significantly accelerates policy\nexploration. However, optimizing such proxy rewards introduces a potential risk\nof \"reward hacking,\" where the agent masters the reward function but fails the\nactual task. To counteract this, our second pathway, Success-Driven\nQuality-Guided SFT, grounds the policy in holistic success. It identifies and\nselectively imitates high-quality successful trajectories, ensuring the agent\nremains aligned with the ultimate task goal. This pathway is strengthened by a\nconditional curriculum mechanism to aid initial exploration.\n  We conduct experiments in challenging manipulation tasks. The results\ndemonstrate that our framework achieves faster convergence and higher final\nsuccess rates compared to representative baselines. Our work presents a robust\nsolution for creating self-improving agents that can efficiently and reliably\nadapt to new environments.",
            "headline_zh": "提出反射式自适应框架以解决VLA模型在机器人任务中适应效率低的问题",
            "intro_zh": [
                "核心问题：预训练VLA模型在适应新任务时效率低，强化学习收敛慢。",
                "方法要点：采用双路径架构，结合失败驱动反思RL和成功驱动SFT，防止奖励黑客。",
                "实验或效果：在操作任务中实现更快收敛和更高成功率，优于基线方法。"
            ],
            "tags_zh": [
                "视觉语言动作模型",
                "任务自适应",
                "强化学习",
                "反思学习",
                "模仿学习",
                "机器人操作"
            ],
            "_index": 24
        },
        {
            "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
            "authors": [
                "Lin Lin",
                "Jiefeng Long",
                "Zhihe Wan",
                "Yuchi Wang",
                "Dingkang Yang",
                "Shuang Yang",
                "Yueyang Yao",
                "Xu Chen",
                "Zirui Guo",
                "Shengqiang Li",
                "Weiran Li",
                "Hanyu Li",
                "Yaling Mou",
                "Yan Qiu",
                "Haiyang Yu",
                "Xiao Liang",
                "Hongsheng Li",
                "Chao Feng"
            ],
            "arxiv_id": "2510.12709v1",
            "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the\nDouyin-Selected scenario. For the Douyin feed rank model, the match features\nproduced by SAIL-Embedding yield a +0.08% AUC gain.",
            "headline_zh": "提出SAIL-Embedding全模态嵌入基础模型以解决多模态支持不足和工业领域差距问题",
            "intro_zh": [
                "核心问题：现有模型在多模态支持、训练稳定性和工业领域差距方面存在挑战",
                "方法要点：采用多阶段训练策略，包括内容感知渐进训练和推荐增强训练",
                "实验或效果：在检索任务中达到SOTA，推荐场景中提升Lifetime和AUC指标"
            ],
            "tags_zh": [
                "全模态嵌入",
                "多阶段训练",
                "推荐系统",
                "跨模态检索",
                "知识蒸馏"
            ],
            "_index": 25
        },
        {
            "title": "Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray Diagnosis",
            "authors": [
                "Shelley Zixin Shu",
                "Haozhe Luo",
                "Alexander Poellinger",
                "Mauricio Reyes"
            ],
            "arxiv_id": "2510.12704v1",
            "summary": "Transformer-based deep learning models have demonstrated exceptional\nperformance in medical imaging by leveraging attention mechanisms for feature\nrepresentation and interpretability. However, these models are prone to\nlearning spurious correlations, leading to biases and limited generalization.\nWhile human-AI attention alignment can mitigate these issues, it often depends\non costly manual supervision. In this work, we propose a Hybrid\nExplanation-Guided Learning (H-EGL) framework that combines self-supervised and\nhuman-guided constraints to enhance attention alignment and improve\ngeneralization. The self-supervised component of H-EGL leverages\nclass-distinctive attention without relying on restrictive priors, promoting\nrobustness and flexibility. We validate our approach on chest X-ray\nclassification using the Vision Transformer (ViT), where H-EGL outperforms two\nstate-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating\nsuperior classification accuracy and generalization capability. Additionally,\nit produces attention maps that are better aligned with human expertise.",
            "headline_zh": "提出混合解释引导学习框架，以提升基于Transformer的胸部X光诊断的泛化能力",
            "intro_zh": [
                "Transformer模型在医学影像中易学伪相关，导致偏差和泛化受限",
                "结合自监督和人工引导约束，增强注意力对齐，无需高成本监督",
                "在胸部X光分类中优于现有方法，提高准确性和注意力图对齐度"
            ],
            "tags_zh": [
                "Transformer模型",
                "胸部X光诊断",
                "解释引导学习",
                "注意力对齐",
                "自监督学习",
                "泛化能力"
            ],
            "_index": 26
        },
        {
            "title": "DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization",
            "authors": [
                "Danial Hosseintabar",
                "Fan Chen",
                "Giannis Daras",
                "Antonio Torralba",
                "Constantinos Daskalakis"
            ],
            "arxiv_id": "2510.12691v1",
            "summary": "Diffusion models have emerged as powerful generative priors for\nhigh-dimensional inverse problems, yet learning them when only corrupted or\nnoisy observations are available remains challenging. In this work, we propose\na new method for training diffusion models with Expectation-Maximization (EM)\nfrom corrupted data. Our proposed method, DiffEM, utilizes conditional\ndiffusion models to reconstruct clean data from observations in the E-step, and\nthen uses the reconstructed data to refine the conditional diffusion model in\nthe M-step. Theoretically, we provide monotonic convergence guarantees for the\nDiffEM iteration, assuming appropriate statistical conditions. We demonstrate\nthe effectiveness of our approach through experiments on various image\nreconstruction tasks.",
            "headline_zh": "提出DiffEM方法以从损坏数据中训练扩散模型",
            "intro_zh": [
                "核心问题：扩散模型在仅有损坏或噪声观测数据时训练困难",
                "方法要点：使用期望最大化算法，E步重建干净数据，M步优化模型",
                "实验或效果：在多种图像重建任务中验证方法有效性"
            ],
            "tags_zh": [
                "扩散模型",
                "期望最大化",
                "图像重建",
                "损坏数据学习",
                "条件扩散模型"
            ],
            "_index": 27
        },
        {
            "title": "EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels",
            "authors": [
                "Kunyu Peng",
                "Di Wen",
                "Kailun Yang",
                "Jia Fu",
                "Yufan Chen",
                "Ruiping Liu",
                "Jiamin Wu",
                "Junwei Zheng",
                "M. Saquib Sarfraz",
                "Luc Van Gool",
                "Danda Pani Paudel",
                "Rainer Stiefelhagen"
            ],
            "arxiv_id": "2510.12687v1",
            "summary": "Open-Set Domain Generalization (OSDG) aims to enable deep learning models to\nrecognize unseen categories in new domains, which is crucial for real-world\napplications. Label noise hinders open-set domain generalization by corrupting\nsource-domain knowledge, making it harder to recognize known classes and reject\nunseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL)\nusing hyperbolic prototype-guided meta-learning, they struggle to bridge domain\ngaps, especially with limited clean labeled data. In this paper, we propose\nEvidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first\nintroduce an unsupervised two-stage evidential loss clustering method to\npromote label reliability awareness. Then, we propose a residual flow matching\nmechanism that models structured domain- and category-conditioned residuals,\nenabling diverse and uncertainty-aware transfer paths beyond\ninterpolation-based augmentation. During this meta-learning process, the model\nis optimized such that the update direction on the clean set maximizes the loss\ndecrease on the noisy set, using pseudo labels derived from the most confident\npredicted class for supervision. Experimental results show that EReLiFM\noutperforms existing methods on OSDG-NL, achieving state-of-the-art\nperformance. The source code is available at\nhttps://github.com/KPeng9510/ERELIFM.",
            "headline_zh": "提出EReLiFM方法以解决带噪声标签的开放集域泛化问题",
            "intro_zh": [
                "开放集域泛化在噪声标签下难以识别已知类和拒绝未知类",
                "使用证据损失聚类和残差流匹配机制增强域间迁移能力",
                "实验显示在OSDG-NL任务中性能优于现有方法"
            ],
            "tags_zh": [
                "开放集域泛化",
                "噪声标签处理",
                "元学习",
                "证据学习",
                "残差流匹配"
            ],
            "_index": 28
        },
        {
            "title": "Autonomous Legged Mobile Manipulation for Lunar Surface Operations via Constrained Reinforcement Learning",
            "authors": [
                "Alvaro Belmonte-Baeza",
                "Miguel Cazorla",
                "Gabriel J. García",
                "Carlos J. Pérez-Del-Pulgar",
                "Jorge Pomares"
            ],
            "arxiv_id": "2510.12684v1",
            "summary": "Robotics plays a pivotal role in planetary science and exploration, where\nautonomous and reliable systems are crucial due to the risks and challenges\ninherent to space environments. The establishment of permanent lunar bases\ndemands robotic platforms capable of navigating and manipulating in the harsh\nlunar terrain. While wheeled rovers have been the mainstay for planetary\nexploration, their limitations in unstructured and steep terrains motivate the\nadoption of legged robots, which offer superior mobility and adaptability. This\npaper introduces a constrained reinforcement learning framework designed for\nautonomous quadrupedal mobile manipulators operating in lunar environments. The\nproposed framework integrates whole-body locomotion and manipulation\ncapabilities while explicitly addressing critical safety constraints, including\ncollision avoidance, dynamic stability, and power efficiency, in order to\nensure robust performance under lunar-specific conditions, such as reduced\ngravity and irregular terrain. Experimental results demonstrate the framework's\neffectiveness in achieving precise 6D task-space end-effector pose tracking,\nachieving an average positional accuracy of 4 cm and orientation accuracy of\n8.1 degrees. The system consistently respects both soft and hard constraints,\nexhibiting adaptive behaviors optimized for lunar gravity conditions. This work\neffectively bridges adaptive learning with essential mission-critical safety\nrequirements, paving the way for advanced autonomous robotic explorers for\nfuture lunar missions.",
            "headline_zh": "提出约束强化学习框架以解决月球四足移动机器人的自主操作问题",
            "intro_zh": [
                "核心问题：月球环境中的机器人需在崎岖地形和低重力下实现安全自主移动与操作",
                "方法要点：集成全身运动与操作，通过约束强化学习确保避障、稳定性和能效",
                "实验或效果：在6D任务空间跟踪中，位置精度4厘米，方向精度8.1度，约束得到满足"
            ],
            "tags_zh": [
                "约束强化学习",
                "四足机器人",
                "移动操作",
                "月球探索",
                "自主系统",
                "安全约束"
            ],
            "_index": 29
        },
        {
            "title": "MCOP: Multi-UAV Collaborative Occupancy Prediction",
            "authors": [
                "Zefu Lin",
                "Wenbo Chen",
                "Xiaojuan Jin",
                "Yuran Yang",
                "Lue Fan",
                "Yixin Zhang",
                "Yufeng Zhang",
                "Zhaoxiang Zhang"
            ],
            "arxiv_id": "2510.12679v1",
            "summary": "Unmanned Aerial Vehicle (UAV) swarm systems necessitate efficient\ncollaborative perception mechanisms for diverse operational scenarios. Current\nBird's Eye View (BEV)-based approaches exhibit two main limitations:\nbounding-box representations fail to capture complete semantic and geometric\ninformation of the scene, and their performance significantly degrades when\nencountering undefined or occluded objects. To address these limitations, we\npropose a novel multi-UAV collaborative occupancy prediction framework. Our\nframework effectively preserves 3D spatial structures and semantics through\nintegrating a Spatial-Aware Feature Encoder and Cross-Agent Feature\nIntegration. To enhance efficiency, we further introduce Altitude-Aware Feature\nReduction to compactly represent scene information, along with a Dual-Mask\nPerceptual Guidance mechanism to adaptively select features and reduce\ncommunication overhead. Due to the absence of suitable benchmark datasets, we\nextend three datasets for evaluation: two virtual datasets (Air-to-Pred-Occ and\nUAV3D-Occ) and one real-world dataset (GauUScene-Occ). Experiments results\ndemonstrate that our method achieves state-of-the-art accuracy, significantly\noutperforming existing collaborative methods while reducing communication\noverhead to only a fraction of previous approaches.",
            "headline_zh": "提出多无人机协作占用预测框架以解决BEV方法语义几何信息缺失与性能下降问题",
            "intro_zh": [
                "BEV方法因边界框表示无法捕捉完整语义几何信息，且对未定义或遮挡对象性能显著下降",
                "集成空间感知特征编码与跨代理特征整合，保留3D结构与语义；引入高度感知特征压缩与双掩码感知指导以提升效率",
                "扩展三个数据集评估，实验显示方法在精度上达到最优，同时通信开销大幅降低"
            ],
            "tags_zh": [
                "多无人机协作",
                "占用预测",
                "3D感知",
                "特征整合",
                "通信优化",
                "无人机群系统"
            ],
            "_index": 30
        },
        {
            "title": "TerraCodec: Compressing Earth Observations",
            "authors": [
                "Julen Costa-Watanabe",
                "Isabelle Wittmann",
                "Benedikt Blumenstiel",
                "Konrad Schindler"
            ],
            "arxiv_id": "2510.12670v1",
            "summary": "Earth observation (EO) satellites produce massive streams of multispectral\nimage time series, posing pressing challenges for storage and transmission.\nYet, learned EO compression remains fragmented, lacking publicly available\npretrained models and misaligned with advances in compression for natural\nimagery. Image codecs overlook temporal redundancy, while video codecs rely on\nmotion priors that fail to capture the radiometric evolution of largely static\nscenes. We introduce TerraCodec (TEC), a family of learned codecs tailored to\nEO. TEC includes efficient image-based variants adapted to multispectral\ninputs, as well as a Temporal Transformer model (TEC-TT) that leverages\ndependencies across time. To overcome the fixed-rate setting of today's neural\ncodecs, we present Latent Repacking, a novel method for training flexible-rate\ntransformer models that operate on varying rate-distortion settings. Trained on\nSentinel-2 data, TerraCodec outperforms classical codecs, achieving 3-10x\nstronger compression at equivalent image quality. Beyond compression, TEC-TT\nenables zero-shot cloud inpainting, surpassing state-of-the-art methods on the\nAllClear benchmark. Our results establish bespoke, learned compression\nalgorithms as a promising direction for Earth observation. Code and model\nweights will be released under a permissive license.",
            "headline_zh": "提出TerraCodec以压缩地球观测数据，提升存储与传输效率",
            "intro_zh": [
                "地球观测卫星产生海量多光谱图像时间序列，存储与传输面临挑战",
                "开发图像和时序Transformer模型，利用时间依赖性和潜在重打包实现灵活率失真",
                "在Sentinel-2数据上优于经典编解码器，压缩比提升3-10倍，并支持零样本云修复"
            ],
            "tags_zh": [
                "地球观测压缩",
                "多光谱图像",
                "时序Transformer",
                "潜在重打包",
                "零样本修复"
            ],
            "_index": 31
        },
        {
            "title": "Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning with Humans in the Loop",
            "authors": [
                "Oz Gitelson",
                "Satya Prakash Nayak",
                "Ritam Raha",
                "Anne-Kathrin Schmuck"
            ],
            "arxiv_id": "2510.12662v1",
            "summary": "We present a novel framework for human-robot \\emph{logical} interaction that\nenables robots to reliably satisfy (infinite horizon) temporal logic tasks\nwhile effectively collaborating with humans who pursue independent and unknown\ntasks. The framework combines two key capabilities: (i) \\emph{maximal\nadaptation} enables the robot to adjust its strategy \\emph{online} to exploit\nhuman behavior for cooperation whenever possible, and (ii) \\emph{minimal\ntunable feedback} enables the robot to request cooperation by the human online\nonly when necessary to guarantee progress. This balance minimizes human-robot\ninterference, preserves human autonomy, and ensures persistent robot task\nsatisfaction even under conflicting human goals. We validate the approach in a\nreal-world block-manipulation task with a Franka Emika Panda robotic arm and in\nthe Overcooked-AI benchmark, demonstrating that our method produces rich,\n\\emph{emergent} cooperative behaviors beyond the reach of existing approaches,\nwhile maintaining strong formal guarantees.",
            "headline_zh": "提出人机逻辑交互框架，实现机器人在无限时域任务中与人类协作。",
            "intro_zh": [
                "核心问题：机器人在未知人类任务下如何可靠满足时序逻辑任务并最小化干扰。",
                "方法要点：结合最大适应性和最小可调反馈，在线调整策略并仅在必要时请求合作。",
                "实验或效果：在真实机器人任务和Overcooked-AI基准中验证，产生涌现合作行为并保持形式保证。"
            ],
            "tags_zh": [
                "人机交互",
                "时序逻辑任务",
                "机器人规划",
                "在线适应",
                "协作机器人",
                "形式保证"
            ],
            "_index": 32
        },
        {
            "title": "On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation",
            "authors": [
                "Shuhei Tarashima",
                "Yushan Wang",
                "Norio Tagawa"
            ],
            "arxiv_id": "2510.12660v1",
            "summary": "In this work, we aim to develop simple and efficient models for human mesh\nrecovery (HMR) and its predecessor task, human pose estimation (HPE).\nState-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large,\nnon-hierarchical vision transformers as encoders, which are inherited from the\ncorresponding HPE models like ViTPose. To establish baselines across varying\ncomputational budgets, we first construct three lightweight HMR2.0 variants by\nadapting the corresponding ViTPose models. In addition, we propose leveraging\nthe early stages of hierarchical vision foundation models (VFMs), including\nSwin Transformer, GroupMixFormer, and VMamba, as encoders. This design is\nmotivated by the observation that intermediate stages of hierarchical VFMs\nproduce feature maps with resolutions comparable to or higher than those of\nnon-hierarchical counterparts. We conduct a comprehensive evaluation of 27\nhierarchical-VFM-based HMR and HPE models, demonstrating that using only the\nfirst two or three stages achieves performance on par with full-stage models.\nMoreover, we show that the resulting truncated models exhibit better trade-offs\nbetween accuracy and computational efficiency compared to existing lightweight\nalternatives.",
            "headline_zh": "提出利用分层视觉基础模型早期阶段构建高效人体网格恢复与姿态估计模型",
            "intro_zh": [
                "核心问题：现有HMR方法依赖大型非分层视觉变换器，计算成本高，需轻量化方案。",
                "方法要点：采用Swin Transformer等分层VFM的前两或三阶段作为编码器，保持特征图高分辨率。",
                "实验效果：27个模型评估显示，截断模型在精度与效率间优于现有轻量方法。"
            ],
            "tags_zh": [
                "人体网格恢复",
                "人体姿态估计",
                "分层视觉基础模型",
                "轻量化模型",
                "计算效率优化"
            ],
            "_index": 33
        },
        {
            "title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency Consistency",
            "authors": [
                "Yanlin Jiang",
                "Yuchen Liu",
                "Mingren Liu"
            ],
            "arxiv_id": "2510.12646v1",
            "summary": "Zero-shot denoisers address the dataset dependency of deep-learning-based\ndenoisers, enabling the denoising of unseen single images. Nonetheless,\nexisting zero-shot methods suffer from long training times and rely on the\nassumption of noise independence and a zero-mean property, limiting their\neffectiveness in real-world denoising scenarios where noise characteristics are\nmore complicated. This paper proposes an efficient and effective method for\nreal-world denoising, the Zero-Shot denoiser based on Cross-Frequency\nConsistency (ZSCFC), which enables training and denoising with a single noisy\nimage and does not rely on assumptions about noise distribution. Specifically,\nimage textures exhibit position similarity and content consistency across\ndifferent frequency bands, while noise does not. Based on this property, we\ndeveloped cross-frequency consistency loss and an ultralight network to realize\nimage denoising. Experiments on various real-world image datasets demonstrate\nthat our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of\ncomputational efficiency and denoising performance.",
            "headline_zh": "提出基于跨频一致性的零样本去噪方法，以高效处理真实世界图像噪声",
            "intro_zh": [
                "现有零样本去噪方法训练时间长且依赖噪声独立性假设，限制真实场景应用",
                "利用图像纹理在频带间的一致性，设计跨频一致性损失和轻量网络实现去噪",
                "实验显示在真实数据集上，该方法在计算效率和去噪性能上优于其他先进方法"
            ],
            "tags_zh": [
                "零样本去噪",
                "跨频一致性",
                "真实世界图像",
                "轻量网络",
                "噪声分布未知"
            ],
            "_index": 34
        },
        {
            "title": "Designing Tools with Control Confidence",
            "authors": [
                "Ajith Anil Meera",
                "Abian Torres",
                "Pablo Lanillos"
            ],
            "arxiv_id": "2510.12630v1",
            "summary": "Prehistoric humans invented stone tools for specialized tasks by not just\nmaximizing the tool's immediate goal-completion accuracy, but also increasing\ntheir confidence in the tool for later use under similar settings. This factor\ncontributed to the increased robustness of the tool, i.e., the least\nperformance deviations under environmental uncertainties. However, the current\nautonomous tool design frameworks solely rely on performance optimization,\nwithout considering the agent's confidence in tool use for repeated use. Here,\nwe take a step towards filling this gap by i) defining an optimization\nframework for task-conditioned autonomous hand tool design for robots, where\nii) we introduce a neuro-inspired control confidence term into the optimization\nroutine that helps the agent to design tools with higher robustness. Through\nrigorous simulations using a robotic arm, we show that tools designed with\ncontrol confidence as the objective function are more robust to environmental\nuncertainties during tool use than a pure accuracy-driven objective. We further\nshow that adding control confidence to the objective function for tool design\nprovides a balance between the robustness and goal accuracy of the designed\ntools under control perturbations. Finally, we show that our CMAES-based\nevolutionary optimization strategy for autonomous tool design outperforms other\nstate-of-the-art optimizers by designing the optimal tool within the fewest\niterations. Code: https://github.com/ajitham123/Tool_design_control_confidence.",
            "headline_zh": "提出控制置信度优化框架以增强机器人工具在环境不确定性下的鲁棒性",
            "intro_zh": [
                "当前自主工具设计框架仅优化性能，忽略代理对工具重复使用的置信度问题",
                "引入神经启发控制置信度项，结合CMAES进化优化策略设计任务条件化工具",
                "仿真显示控制置信度目标函数提升工具鲁棒性，并在控制扰动下平衡准确性与鲁棒性"
            ],
            "tags_zh": [
                "自主工具设计",
                "控制置信度",
                "鲁棒性优化",
                "进化算法",
                "机器人仿真"
            ],
            "_index": 35
        },
        {
            "title": "Learning Robust Agile Flight Control with Stability Guarantees",
            "authors": [
                "Lukas Pries",
                "Markus Ryll"
            ],
            "arxiv_id": "2510.12611v1",
            "summary": "In the evolving landscape of high-speed agile quadrotor flight, achieving\nprecise trajectory tracking at the platform's operational limits is paramount.\nControllers must handle actuator constraints, exhibit robustness to\ndisturbances, and remain computationally efficient for safety-critical\napplications. In this work, we present a novel neural-augmented feedback\ncontroller for agile flight control. The controller addresses individual\nlimitations of existing state-of-the-art control paradigms and unifies their\nstrengths. We demonstrate the controller's capabilities, including the accurate\ntracking of highly aggressive trajectories that surpass the feasibility of the\nactuators. Notably, the controller provides universal stability guarantees,\nenhancing its robustness and tracking performance even in exceedingly\ndisturbance-prone settings. Its nonlinear feedback structure is highly\nefficient enabling fast computation at high update rates. Moreover, the\nlearning process in simulation is both fast and stable, and the controller's\ninherent robustness allows direct deployment to real-world platforms without\nthe need for training augmentations or fine-tuning.",
            "headline_zh": "提出神经增强反馈控制器以解决高速敏捷四旋翼飞行中的轨迹跟踪问题",
            "intro_zh": [
                "核心问题：高速敏捷四旋翼飞行需在操作极限下精确跟踪轨迹，同时处理执行器约束和扰动。",
                "方法要点：结合现有控制范式优势，设计非线性反馈结构，提供通用稳定性保证。",
                "实验或效果：在仿真中快速稳定学习，无需调优即可部署到真实平台，提升鲁棒性和跟踪性能。"
            ],
            "tags_zh": [
                "敏捷飞行控制",
                "神经增强控制器",
                "稳定性保证",
                "轨迹跟踪",
                "鲁棒控制"
            ],
            "_index": 36
        },
        {
            "title": "WaterFlow: Explicit Physics-Prior Rectified Flow for Underwater Saliency Mask Generation",
            "authors": [
                "Runting Li",
                "Shijie Lian",
                "Hua Li",
                "Yutong Li",
                "Wenhui Wu",
                "Sam Kwong"
            ],
            "arxiv_id": "2510.12605v1",
            "summary": "Underwater Salient Object Detection (USOD) faces significant challenges,\nincluding underwater image quality degradation and domain gaps. Existing\nmethods tend to ignore the physical principles of underwater imaging or simply\ntreat degradation phenomena in underwater images as interference factors that\nmust be eliminated, failing to fully exploit the valuable information they\ncontain. We propose WaterFlow, a rectified flow-based framework for underwater\nsalient object detection that innovatively incorporates underwater physical\nimaging information as explicit priors directly into the network training\nprocess and introduces temporal dimension modeling, significantly enhancing the\nmodel's capability for salient object identification. On the USOD10K dataset,\nWaterFlow achieves a 0.072 gain in S_m, demonstrating the effectiveness and\nsuperiority of our method. The code will be published after the acceptance.",
            "headline_zh": "提出WaterFlow框架，利用显式物理先验和整流流解决水下显著目标检测问题",
            "intro_zh": [
                "核心问题：水下图像质量退化和领域差距，现有方法忽略物理成像原理或简单消除干扰",
                "方法要点：引入水下物理成像信息作为显式先验，结合整流流框架和时序维度建模",
                "实验或效果：在USOD10K数据集上S_m指标提升0.072，显示方法有效性和优越性"
            ],
            "tags_zh": [
                "水下显著目标检测",
                "整流流框架",
                "物理先验",
                "时序建模",
                "图像质量退化"
            ],
            "_index": 37
        },
        {
            "title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space",
            "authors": [
                "Chao Chen",
                "Zhixin Ma",
                "Yongqi Li",
                "Yupeng Hu",
                "Yinwei Wei",
                "Wenjie Li",
                "Liqiang Nie"
            ],
            "arxiv_id": "2510.12603v1",
            "summary": "Multimodal reasoning aims to enhance the capabilities of MLLMs by\nincorporating intermediate reasoning steps before reaching the final answer. It\nhas evolved from text-only reasoning to the integration of visual information,\nenabling the thought process to be conveyed through both images and text.\nDespite its effectiveness, current multimodal reasoning methods depend on\nexplicit reasoning steps that require labor-intensive vision-text annotations\nand inherently introduce significant inference latency. To address these\nissues, we introduce multimodal latent reasoning with the advantages of\nmultimodal representation, reduced annotation, and inference efficiency. To\nfacilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),\nwhich injects both visual and textual information in the reasoning process\nwithin the latent space. Specifically, IVT-LR represents each reasoning step by\ncombining two implicit parts: latent text (the hidden states from the previous\nstep) and latent vision (a set of selected image embeddings). We further\nintroduce a progressive multi-stage training strategy to enable MLLMs to\nperform the above multimodal latent reasoning steps. Experiments on M3CoT and\nScienceQA demonstrate that our IVT-LR method achieves an average performance\nincrease of 5.45% in accuracy, while simultaneously achieving a speed increase\nof over 5 times compared to existing approaches. Code available at\nhttps://github.com/FYYDCC/IVT-LR.",
            "headline_zh": "提出IVT-LR方法以解决多模态推理中显式步骤依赖导致的标注成本高和推理延迟问题",
            "intro_zh": [
                "核心问题：当前多模态推理方法依赖显式推理步骤，需要大量视觉-文本标注并引入高推理延迟",
                "方法要点：在潜在空间中注入视觉和文本信息，结合隐式文本和视觉表示进行推理",
                "实验或效果：在M3CoT和ScienceQA数据集上平均准确率提升5.45%，推理速度提升5倍以上"
            ],
            "tags_zh": [
                "多模态推理",
                "潜在空间推理",
                "推理效率优化",
                "隐式表示学习",
                "多模态大语言模型"
            ],
            "_index": 38
        },
        {
            "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training",
            "authors": [
                "Jiachen Lei",
                "Keli Liu",
                "Julius Berner",
                "Haiming Yu",
                "Hongkai Zheng",
                "Jiahong Wu",
                "Xiangxiang Chu"
            ],
            "arxiv_id": "2510.12586v1",
            "summary": "Pixel-space generative models are often more difficult to train and generally\nunderperform compared to their latent-space counterparts, leaving a persistent\nperformance and efficiency gap. In this paper, we introduce a novel two-stage\ntraining framework that closes this gap for pixel-space diffusion and\nconsistency models. In the first stage, we pre-train encoders to capture\nmeaningful semantics from clean images while aligning them with points along\nthe same deterministic sampling trajectory, which evolves points from the prior\nto the data distribution. In the second stage, we integrate the encoder with a\nrandomly initialized decoder and fine-tune the complete model end-to-end for\nboth diffusion and consistency models. Our training framework demonstrates\nstrong empirical performance on ImageNet dataset. Specifically, our diffusion\nmodel reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75\nnumber of function evaluations (NFE), surpassing prior pixel-space methods by a\nlarge margin in both generation quality and efficiency while rivaling leading\nVAE-based models at comparable training cost. Furthermore, on ImageNet-256, our\nconsistency model achieves an impressive FID of 8.82 in a single sampling step,\nsignificantly surpassing its latent-space counterpart. To the best of our\nknowledge, this marks the first successful training of a consistency model\ndirectly on high-resolution images without relying on pre-trained VAEs or\ndiffusion models.",
            "headline_zh": "提出两阶段训练框架以提升像素空间生成模型性能",
            "intro_zh": [
                "像素空间生成模型训练困难且性能低于潜在空间模型",
                "采用自监督预训练编码器，结合端到端微调解码器",
                "在ImageNet上实现高FID分数，超越现有像素空间方法"
            ],
            "tags_zh": [
                "像素空间生成模型",
                "自监督预训练",
                "扩散模型",
                "一致性模型",
                "端到端训练",
                "图像生成"
            ],
            "_index": 39
        },
        {
            "title": "LayerSync: Self-aligning Intermediate Layers",
            "authors": [
                "Yasaman Haghighi",
                "Bastien van Delft",
                "Mariam Hassan",
                "Alexandre Alahi"
            ],
            "arxiv_id": "2510.12581v1",
            "summary": "We propose LayerSync, a domain-agnostic approach for improving the generation\nquality and the training efficiency of diffusion models. Prior studies have\nhighlighted the connection between the quality of generation and the\nrepresentations learned by diffusion models, showing that external guidance on\nmodel intermediate representations accelerates training. We reconceptualize\nthis paradigm by regularizing diffusion models with their own intermediate\nrepresentations. Building on the observation that representation quality varies\nacross diffusion model layers, we show that the most semantically rich\nrepresentations can act as an intrinsic guidance for weaker ones, reducing the\nneed for external supervision. Our approach, LayerSync, is a self-sufficient,\nplug-and-play regularizer term with no overhead on diffusion model training and\ngeneralizes beyond the visual domain to other modalities. LayerSync requires no\npretrained models nor additional data. We extensively evaluate the method on\nimage generation and demonstrate its applicability to other domains such as\naudio, video, and motion generation. We show that it consistently improves the\ngeneration quality and the training efficiency. For example, we speed up the\ntraining of flow-based transformer by over 8.75x on ImageNet dataset and\nimproved the generation quality by 23.6%. The code is available at\nhttps://github.com/vita-epfl/LayerSync.",
            "headline_zh": "提出LayerSync自对齐中间层方法，提升扩散模型生成质量与训练效率。",
            "intro_zh": [
                "核心问题：扩散模型中间层表示质量不均，影响生成质量与训练效率。",
                "方法要点：利用语义丰富层指导弱层，无需外部监督，作为即插即用正则项。",
                "实验或效果：在图像生成中加速训练8.75倍，提升质量23.6%，适用于多模态。"
            ],
            "tags_zh": [
                "扩散模型",
                "中间层正则化",
                "自对齐学习",
                "多模态生成",
                "训练加速"
            ],
            "_index": 40
        },
        {
            "title": "Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence",
            "authors": [
                "Simon Ravé",
                "Jean-Christophe Lombardo",
                "Pejman Rasti",
                "Alexis Joly",
                "David Rousseau"
            ],
            "arxiv_id": "2510.12579v1",
            "summary": "We present a zero-shot segmentation approach for agricultural imagery that\nleverages Plantnet, a large-scale plant classification model, in conjunction\nwith its DinoV2 backbone and the Segment Anything Model (SAM). Rather than\ncollecting and annotating new datasets, our method exploits Plantnet's\nspecialized plant representations to identify plant regions and produce coarse\nsegmentation masks. These masks are then refined by SAM to yield detailed\nsegmentations. We evaluate on four publicly available datasets of various\ncomplexity in terms of contrast including some where the limited size of the\ntraining data and complex field conditions often hinder purely supervised\nmethods. Our results show consistent performance gains when using\nPlantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by the\nJaccard Index (IoU). These findings highlight the potential of combining\nfoundation models with specialized plant-centric models to alleviate the\nannotation bottleneck and enable effective segmentation in diverse agricultural\nscenarios.",
            "headline_zh": "提出零样本植物分割方法，结合PlantNet与SAM解决农业图像分割问题",
            "intro_zh": [
                "核心问题：农业图像分割中标注数据稀缺，监督方法在复杂场景下性能受限",
                "方法要点：利用PlantNet模型生成粗分割掩码，再通过SAM进行精细化处理",
                "实验或效果：在多个数据集上评估，PlantNet微调DinoV2相比基础模型IoU提升"
            ],
            "tags_zh": [
                "零样本分割",
                "植物图像分析",
                "农业计算机视觉",
                "基础模型融合",
                "掩码细化"
            ],
            "_index": 41
        },
        {
            "title": "Learning Human Motion with Temporally Conditional Mamba",
            "authors": [
                "Quang Nguyen",
                "Tri Le",
                "Baoru Huang",
                "Minh Nhat Vu",
                "Ngan Le",
                "Thieu Vo",
                "Anh Nguyen"
            ],
            "arxiv_id": "2510.12573v1",
            "summary": "Learning human motion based on a time-dependent input signal presents a\nchallenging yet impactful task with various applications. The goal of this task\nis to generate or estimate human movement that consistently reflects the\ntemporal patterns of conditioning inputs. Existing methods typically rely on\ncross-attention mechanisms to fuse the condition with motion. However, this\napproach primarily captures global interactions and struggles to maintain\nstep-by-step temporal alignment. To address this limitation, we introduce\nTemporally Conditional Mamba, a new mamba-based model for human motion\ngeneration. Our approach integrates conditional information into the recurrent\ndynamics of the Mamba block, enabling better temporally aligned motion. To\nvalidate the effectiveness of our method, we evaluate it on a variety of human\nmotion tasks. Extensive experiments demonstrate that our model significantly\nimproves temporal alignment, motion realism, and condition consistency over\nstate-of-the-art approaches. Our project page is available at\nhttps://zquang2202.github.io/TCM.",
            "headline_zh": "提出Temporally Conditional Mamba以解决时间条件人类运动生成中的对齐问题",
            "intro_zh": [
                "核心问题：现有方法依赖跨注意力机制，难以保持时间步对齐。",
                "方法要点：将条件信息集成到Mamba块的循环动态中，提升时间对齐。",
                "实验或效果：在多种任务中验证，显著改进对齐、真实性和条件一致性。"
            ],
            "tags_zh": [
                "人类运动生成",
                "时间条件建模",
                "Mamba模型",
                "时间对齐",
                "运动估计"
            ],
            "_index": 42
        },
        {
            "title": "MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking",
            "authors": [
                "Tianhao Li",
                "Tingfa Xu",
                "Ying Wang",
                "Haolin Qin",
                "Xu Lin",
                "Jianan Li"
            ],
            "arxiv_id": "2510.12565v1",
            "summary": "Drone-based multi-object tracking is essential yet highly challenging due to\nsmall targets, severe occlusions, and cluttered backgrounds. Existing RGB-based\ntracking algorithms heavily depend on spatial appearance cues such as color and\ntexture, which often degrade in aerial views, compromising reliability.\nMultispectral imagery, capturing pixel-level spectral reflectance, provides\ncrucial cues that enhance object discriminability under degraded spatial\nconditions. However, the lack of dedicated multispectral UAV datasets has\nhindered progress in this domain. To bridge this gap, we introduce MMOT, the\nfirst challenging benchmark for drone-based multispectral multi-object\ntracking. It features three key characteristics: (i) Large Scale - 125 video\nsequences with over 488.8K annotations across eight categories; (ii)\nComprehensive Challenges - covering diverse conditions such as extreme small\ntargets, high-density scenarios, severe occlusions, and complex motion; and\n(iii) Precise Oriented Annotations - enabling accurate localization and reduced\nambiguity under aerial perspectives. To better extract spectral features and\nleverage oriented annotations, we further present a multispectral and\norientation-aware MOT scheme adapting existing methods, featuring: (i) a\nlightweight Spectral 3D-Stem integrating spectral features while preserving\ncompatibility with RGB pretraining; (ii) an orientation-aware Kalman filter for\nprecise state estimation; and (iii) an end-to-end orientation-adaptive\ntransformer. Extensive experiments across representative trackers consistently\nshow that multispectral input markedly improves tracking performance over RGB\nbaselines, particularly for small and densely packed objects. We believe our\nwork will advance drone-based multispectral multi-object tracking research. Our\nMMOT, code, and benchmarks are publicly available at\nhttps://github.com/Annzstbl/MMOT.",
            "headline_zh": "提出MMOT基准与多光谱方案，以提升无人机多目标跟踪在复杂场景下的性能。",
            "intro_zh": [
                "核心问题：无人机多目标跟踪因目标小、遮挡严重和背景杂乱而不可靠，RGB方法依赖空间外观易退化。",
                "方法要点：引入大规模多光谱数据集MMOT，并设计光谱3D-Stem和方向感知跟踪方案增强特征提取。",
                "实验或效果：多光谱输入显著优于RGB基线，尤其在小目标和密集场景中提升跟踪性能。"
            ],
            "tags_zh": [
                "多光谱多目标跟踪",
                "无人机基准",
                "方向感知跟踪",
                "光谱特征提取",
                "小目标跟踪"
            ],
            "_index": 43
        },
        {
            "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving",
            "authors": [
                "Xiaoji Zheng",
                "Ziyuan Yang",
                "Yanhao Chen",
                "Yuhang Peng",
                "Yuanrong Tang",
                "Gengyuan Liu",
                "Bokui Chen",
                "Jiangtao Gong"
            ],
            "arxiv_id": "2510.12560v1",
            "summary": "End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
            "headline_zh": "提出CoIRL-AD框架，结合模仿与强化学习提升自动驾驶泛化能力",
            "intro_zh": [
                "端到端自动驾驶模型仅用模仿学习泛化能力差，强化学习样本效率低且不稳定",
                "采用竞争性双策略框架，让模仿与强化学习代理在训练中交互，避免梯度冲突",
                "在nuScenes数据集上碰撞率降低18%，泛化能力和长尾场景性能提升"
            ],
            "tags_zh": [
                "自动驾驶",
                "模仿学习",
                "强化学习",
                "双策略框架",
                "泛化能力",
                "长尾场景"
            ],
            "_index": 44
        },
        {
            "title": "VISaGE: Understanding Visual Generics and Exceptions",
            "authors": [
                "Stella Frank",
                "Emily Allaway"
            ],
            "arxiv_id": "2510.12548v1",
            "summary": "While Vision Language Models (VLMs) learn conceptual representations, in the\nform of generalized knowledge, during training, they are typically used to\nanalyze individual instances. When evaluation instances are atypical, this\nparadigm results in tension between two priors in the model. The first is a\npragmatic prior that the textual and visual input are both relevant, arising\nfrom VLM finetuning on congruent inputs; the second is a semantic prior that\nthe conceptual representation is generally true for instances of the category.\nIn order to understand how VLMs trade off these priors, we introduce a new\nevaluation dataset, VISaGE, consisting of both typical and exceptional images.\nIn carefully balanced experiments, we show that conceptual understanding\ndegrades when the assumption of congruency underlying the pragmatic prior is\nviolated with incongruent images. This effect is stronger than the effect of\nthe semantic prior when querying about individual instances.",
            "headline_zh": "提出VISaGE数据集以评估视觉语言模型在典型与异常图像上的概念理解",
            "intro_zh": [
                "核心问题：视觉语言模型在异常实例上存在语用先验与语义先验的冲突",
                "方法要点：构建平衡数据集，包含典型和异常图像，用于系统实验",
                "实验或效果：语用先验违反时概念理解退化，影响强于语义先验"
            ],
            "tags_zh": [
                "视觉语言模型",
                "概念理解",
                "语用先验",
                "语义先验",
                "异常图像",
                "评估数据集"
            ],
            "_index": 45
        },
        {
            "title": "Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion",
            "authors": [
                "David Björkstrand",
                "Tiesheng Wang",
                "Lars Bretzner",
                "Josephine Sullivan"
            ],
            "arxiv_id": "2510.12537v1",
            "summary": "Recent work has explored a range of model families for human motion\ngeneration, including Variational Autoencoders (VAEs), Generative Adversarial\nNetworks (GANs), and diffusion-based models. Despite their differences, many\nmethods rely on over-parameterized input features and auxiliary losses to\nimprove empirical results. These strategies should not be strictly necessary\nfor diffusion models to match the human motion distribution. We show that on\npar with state-of-the-art results in unconditional human motion generation are\nachievable with a score-based diffusion model using only careful feature-space\nnormalization and analytically derived weightings for the standard L2\nscore-matching loss, while generating both motion and shape directly, thereby\navoiding slow post hoc shape recovery from joints. We build the method step by\nstep, with a clear theoretical motivation for each component, and provide\ntargeted ablations demonstrating the effectiveness of each proposed addition in\nisolation.",
            "headline_zh": "提出基于平衡分数扩散的无条件人体运动与形状生成方法，避免过参数化输入",
            "intro_zh": [
                "核心问题：现有方法依赖过参数化输入和辅助损失，可能不必要",
                "方法要点：使用分数扩散模型，仅需特征归一化和分析权重L2损失",
                "实验或效果：在无条件人体运动生成中达到先进水平，直接生成运动与形状"
            ],
            "tags_zh": [
                "人体运动生成",
                "扩散模型",
                "分数匹配",
                "形状生成",
                "无条件生成"
            ],
            "_index": 46
        },
        {
            "title": "Two-stream network-driven vision-based tactile sensor for object feature extraction and fusion perception",
            "authors": [
                "Muxing Huang",
                "Zibin Chen",
                "Weiliang Xu",
                "Zilan Li",
                "Yuanzhi Zhou",
                "Guoyuan Zhou",
                "Wenjing Chen",
                "Xinming Li"
            ],
            "arxiv_id": "2510.12528v1",
            "summary": "Tactile perception is crucial for embodied intelligent robots to recognize\nobjects. Vision-based tactile sensors extract object physical attributes\nmultidimensionally using high spatial resolution; however, this process\ngenerates abundant redundant information. Furthermore, single-dimensional\nextraction, lacking effective fusion, fails to fully characterize object\nattributes. These challenges hinder the improvement of recognition accuracy. To\naddress this issue, this study introduces a two-stream network feature\nextraction and fusion perception strategy for vision-based tactile systems.\nThis strategy employs a distributed approach to extract internal and external\nobject features. It obtains depth map information through three-dimensional\nreconstruction while simultaneously acquiring hardness information by measuring\ncontact force data. After extracting features with a convolutional neural\nnetwork (CNN), weighted fusion is applied to create a more informative and\neffective feature representation. In standard tests on objects of varying\nshapes and hardness, the force prediction error is 0.06 N (within a 12 N\nrange). Hardness recognition accuracy reaches 98.0%, and shape recognition\naccuracy reaches 93.75%. With fusion algorithms, object recognition accuracy in\nactual grasping scenarios exceeds 98.5%. Focused on object physical attributes\nperception, this method enhances the artificial tactile system ability to\ntransition from perception to cognition, enabling its use in embodied\nperception applications.",
            "headline_zh": "提出双流网络特征提取与融合策略，以提升视觉触觉传感器在物体识别中的准确性。",
            "intro_zh": [
                "核心问题：视觉触觉传感器产生冗余信息，单维特征提取缺乏有效融合，限制识别精度提升。",
                "方法要点：采用双流网络分别提取物体内外特征，结合深度图和硬度信息，经CNN提取后加权融合。",
                "实验或效果：在标准测试中，硬度识别准确率98.0%，形状识别93.75%，实际抓取场景识别超98.5%。"
            ],
            "tags_zh": [
                "视觉触觉传感器",
                "双流网络",
                "特征融合",
                "物体识别",
                "卷积神经网络",
                "多模态感知"
            ],
            "_index": 47
        },
        {
            "title": "Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points",
            "authors": [
                "Jiayi Kong",
                "Chen Zong",
                "Junkai Deng",
                "Xuhui Chen",
                "Fei Hou",
                "Shiqing Xin",
                "Junhui Hou",
                "Chen Qian",
                "Ying He"
            ],
            "arxiv_id": "2510.12524v1",
            "summary": "Unsigned Distance Fields (UDFs) provide a flexible representation for 3D\nshapes with arbitrary topology, including open and closed surfaces, orientable\nand non-orientable geometries, and non-manifold structures. While recent neural\napproaches have shown promise in learning UDFs, they often suffer from\nnumerical instability, high computational cost, and limited controllability. We\npresent a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD),\nfor computing UDFs directly from unoriented point clouds. Our approach begins\nby assigning bi-directional normals to input points, guided by two\nVoronoi-based geometric criteria encoded in an energy function for optimal\nalignment. The aligned normals are then diffused to form an approximate UDF\ngradient field, which is subsequently integrated to recover the final UDF.\nExperiments demonstrate that VAD robustly handles watertight and open surfaces,\nas well as complex non-manifold and non-orientable geometries, while remaining\ncomputationally efficient and stable.",
            "headline_zh": "提出Voronoi辅助扩散方法以从无定向点云计算无符号距离场",
            "intro_zh": [
                "核心问题：现有神经方法计算无符号距离场时存在数值不稳定、高计算成本和可控性差的问题",
                "方法要点：使用Voronoi几何准则对齐双向法向量，扩散后积分恢复无符号距离场",
                "实验或效果：方法高效稳定，能处理封闭、开放、非流形和非定向几何体"
            ],
            "tags_zh": [
                "无符号距离场",
                "点云处理",
                "Voronoi图",
                "扩散方法",
                "几何计算"
            ],
            "_index": 48
        },
        {
            "title": "Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot Manipulators: Addressing the Behavior Planning Challenge",
            "authors": [
                "Gaoyuan Liu",
                "Bas Boom",
                "Naftali Slob",
                "Yuri Durodié",
                "Ann Nowé",
                "Bram Vanderborght"
            ],
            "arxiv_id": "2510.12509v1",
            "summary": "Pruning is an essential agricultural practice for orchards. Proper pruning\ncan promote healthier growth and optimize fruit production throughout the\norchard's lifespan. Robot manipulators have been developed as an automated\nsolution for this repetitive task, which typically requires seasonal labor with\nspecialized skills. While previous research has primarily focused on the\nchallenges of perception, the complexities of manipulation are often\noverlooked. These challenges involve planning and control in both joint and\nCartesian spaces to guide the end-effector through intricate, obstructive\nbranches. Our work addresses the behavior planning challenge for a robotic\npruning system, which entails a multi-level planning problem in environments\nwith complex collisions. In this paper, we formulate the planning problem for a\nhigh-dimensional robotic arm in a pruning scenario, investigate the system's\nintrinsic redundancies, and propose a comprehensive pruning workflow that\nintegrates perception, modeling, and holistic planning. In our experiments, we\ndemonstrate that more comprehensive planning methods can significantly enhance\nthe performance of the robotic manipulator. Finally, we implement the proposed\nworkflow on a real-world robot. As a result, this work complements previous\nefforts on robotic pruning and motivates future research and development in\nplanning for pruning applications.",
            "headline_zh": "提出自动化行为规划方法以解决果树修剪中冗余机器人操作器的规划挑战",
            "intro_zh": [
                "核心问题：机器人修剪面临复杂碰撞环境中的多级行为规划难题，常被忽视。",
                "方法要点：利用机器人内在冗余性，集成感知、建模和整体规划于修剪工作流。",
                "实验或效果：实验显示全面规划方法显著提升机器人性能，并在真实机器人上实现。"
            ],
            "tags_zh": [
                "机器人修剪",
                "行为规划",
                "冗余操作器",
                "碰撞环境",
                "整体规划"
            ],
            "_index": 49
        },
        {
            "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring",
            "authors": [
                "An Zhao",
                "Piaopiao Yu",
                "Zhe Zhu",
                "Mingqiang Wei"
            ],
            "arxiv_id": "2510.12493v1",
            "summary": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene\nreconstruction.However, reconstructing high-quality 3D scenes from\nmotion-blurred images caused by camera motion poses a significant challenge.The\nperformance of existing 3DGS-based deblurring methods are limited due to their\ninherent mechanisms, such as extreme dependence on the accuracy of camera poses\nand inability to effectively control erroneous Gaussian primitives\ndensification caused by motion blur.To solve these problems, we introduce a\nnovel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D\nscenes from motion-blurred images.BSGS contains two stages. First, Camera Pose\nRefinement roughly optimizes camera poses to reduce motion-induced distortions.\nSecond, with fixed rough camera poses, Global RigidTransformation further\ncorrects motion-induced blur distortions.To alleviate multi-subframe gradient\nconflicts, we propose a subframe gradient aggregation strategy to optimize both\nstages.Furthermore, a space-time bi-stage optimization strategy is introduced\nto dynamically adjust primitive densification thresholds and prevent premature\nnoisy Gaussian generation in blurred regions. Comprehensive experiments verify\nthe effectiveness of our proposed deblurring method and show its superiority\nover the state of the arts.",
            "headline_zh": "提出双阶段3D高斯泼溅方法以解决相机运动模糊下的3D场景重建问题",
            "intro_zh": [
                "核心问题：现有3DGS方法在相机运动模糊下重建质量受限，依赖精确相机位姿且易产生错误高斯基元。",
                "方法要点：采用双阶段优化，先粗略优化相机位姿，再全局刚性变换校正模糊失真。",
                "实验效果：综合实验验证方法有效性，优于现有先进技术，提升重建质量。"
            ],
            "tags_zh": [
                "3D高斯泼溅",
                "相机运动去模糊",
                "双阶段优化",
                "梯度聚合",
                "场景重建",
                "刚性变换"
            ],
            "_index": 50
        },
        {
            "title": "Fast Visuomotor Policy for Robotic Manipulation",
            "authors": [
                "Jingkai Jia",
                "Tong Yang",
                "Xueyao Chen",
                "Chenhuan Liu",
                "Wenqiang Zhang"
            ],
            "arxiv_id": "2510.12483v1",
            "summary": "We present a fast and effective policy framework for robotic manipulation,\nnamed Energy Policy, designed for high-frequency robotic tasks and\nresource-constrained systems. Unlike existing robotic policies, Energy Policy\nnatively predicts multimodal actions in a single forward pass, enabling\nhigh-precision manipulation at high speed. The framework is built upon two core\ncomponents. First, we adopt the energy score as the learning objective to\nfacilitate multimodal action modeling. Second, we introduce an energy MLP to\nimplement the proposed objective while keeping the architecture simple and\nefficient. We conduct comprehensive experiments in both simulated environments\nand real-world robotic tasks to evaluate the effectiveness of Energy Policy.\nThe results show that Energy Policy matches or surpasses the performance of\nstate-of-the-art manipulation methods while significantly reducing\ncomputational overhead. Notably, on the MimicGen benchmark, Energy Policy\nachieves superior performance with at a faster inference compared to existing\napproaches.",
            "headline_zh": "提出Energy Policy框架，用于高速机器人操作任务，实现高效多模态动作预测。",
            "intro_zh": [
                "核心问题：现有机器人策略在高频任务中计算开销大，难以实现快速多模态动作预测。",
                "方法要点：采用能量分数作为学习目标，引入能量MLP简化架构，单次前向预测多模态动作。",
                "实验或效果：在模拟和真实任务中，性能匹配或超越先进方法，显著降低计算成本。"
            ],
            "tags_zh": [
                "机器人操作",
                "多模态动作预测",
                "能量学习",
                "高效策略",
                "高频任务"
            ],
            "_index": 51
        },
        {
            "title": "A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation",
            "authors": [
                "Shurong Chai",
                "Rahul Kumar JAIN",
                "Rui Xu",
                "Shaocong Mo",
                "Ruibo Hou",
                "Shiyu Teng",
                "Jiaqing Liu",
                "Lanfen Lin",
                "Yen-Wei Chen"
            ],
            "arxiv_id": "2510.12482v1",
            "summary": "Deep learning relies heavily on data augmentation to mitigate limited data,\nespecially in medical imaging. Recent multimodal learning integrates text and\nimages for segmentation, known as referring or text-guided image segmentation.\nHowever, common augmentations like rotation and flipping disrupt spatial\nalignment between image and text, weakening performance. To address this, we\npropose an early fusion framework that combines text and visual features before\naugmentation, preserving spatial consistency. We also design a lightweight\ngenerator that projects text embeddings into visual space, bridging semantic\ngaps. Visualization of generated pseudo-images shows accurate region\nlocalization. Our method is evaluated on three medical imaging tasks and four\nsegmentation frameworks, achieving state-of-the-art results. Code is publicly\navailable on GitHub: https://github.com/11yxk/MedSeg_EarlyFusion.",
            "headline_zh": "提出早期融合框架以解决医学图像分割中文本-图像空间对齐问题",
            "intro_zh": [
                "核心问题：传统数据增强破坏文本与图像空间对齐，影响分割性能",
                "方法要点：早期融合文本与视觉特征，设计轻量生成器投影文本嵌入",
                "实验或效果：在三个医学任务和四个框架上实现最优结果，代码开源"
            ],
            "tags_zh": [
                "医学图像分割",
                "文本引导分割",
                "早期融合",
                "数据增强",
                "多模态学习"
            ],
            "_index": 52
        },
        {
            "title": "A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot Cooperation",
            "authors": [
                "Gaoyuan Liu",
                "Joris de Winter",
                "Kelly Merckaert",
                "Denis Steckelmacher",
                "Ann Nowe",
                "Bram Vanderborght"
            ],
            "arxiv_id": "2510.12477v1",
            "summary": "In a Human-Robot Cooperation (HRC) environment, safety and efficiency are the\ntwo core properties to evaluate robot performance. However, safety mechanisms\nusually hinder task efficiency since human intervention will cause backup\nmotions and goal failures of the robot. Frequent motion replanning will\nincrease the computational load and the chance of failure. In this paper, we\npresent a hybrid Reinforcement Learning (RL) planning framework which is\ncomprised of an interactive motion planner and a RL task planner. The RL task\nplanner attempts to choose statistically safe and efficient task sequences\nbased on the feedback from the motion planner, while the motion planner keeps\nthe task execution process collision-free by detecting human arm motions and\ndeploying new paths when the previous path is not valid anymore. Intuitively,\nthe RL agent will learn to avoid dangerous tasks, while the motion planner\nensures that the chosen tasks are safe. The proposed framework is validated on\nthe cobot in both simulation and the real world, we compare the planner with\nhard-coded task motion planning methods. The results show that our planning\nframework can 1) react to uncertain human motions at both joint and task\nlevels; 2) reduce the times of repeating failed goal commands; 3) reduce the\ntotal number of replanning requests.",
            "headline_zh": "提出混合强化学习任务运动规划框架，以提升人机协作的安全性与效率",
            "intro_zh": [
                "核心问题：人机协作中安全机制常降低任务效率，频繁运动重规划增加计算负担与失败风险",
                "方法要点：结合强化学习任务规划器与交互式运动规划器，学习安全任务序列并实时避障",
                "实验或效果：在仿真与真实机器人验证，减少重规划次数与失败命令重复，提升反应能力"
            ],
            "tags_zh": [
                "人机协作",
                "强化学习规划",
                "运动规划",
                "安全机器人",
                "任务效率"
            ],
            "_index": 53
        },
        {
            "title": "MS-GAGA: Metric-Selective Guided Adversarial Generation Attack",
            "authors": [
                "Dion J. X. Ho",
                "Gabriel Lee Jun Rong",
                "Niharika Shrivastava",
                "Harshavardhan Abichandani",
                "Pai Chet Ng",
                "Xiaoxiao Miao"
            ],
            "arxiv_id": "2510.12468v1",
            "summary": "We present MS-GAGA (Metric-Selective Guided Adversarial Generation Attack), a\ntwo-stage framework for crafting transferable and visually imperceptible\nadversarial examples against deepfake detectors in black-box settings. In Stage\n1, a dual-stream attack module generates adversarial candidates: MNTD-PGD\napplies enhanced gradient calculations optimized for small perturbation\nbudgets, while SG-PGD focuses perturbations on visually salient regions. This\ncomplementary design expands the adversarial search space and improves\ntransferability across unseen models. In Stage 2, a metric-aware selection\nmodule evaluates candidates based on both their success against black-box\nmodels and their structural similarity (SSIM) to the original image. By jointly\noptimizing transferability and imperceptibility, MS-GAGA achieves up to 27%\nhigher misclassification rates on unseen detectors compared to state-of-the-art\nattacks.",
            "headline_zh": "提出MS-GAGA框架以在黑盒设置下生成可转移且视觉不可察觉的对抗样本攻击深度伪造检测器",
            "intro_zh": [
                "核心问题：在黑盒设置下生成可转移且视觉不可察觉的对抗样本攻击深度伪造检测器",
                "方法要点：采用两阶段框架，第一阶段生成对抗候选，第二阶段基于指标选择优化",
                "实验或效果：在未见检测器上实现高达27%的误分类率提升"
            ],
            "tags_zh": [
                "对抗攻击",
                "深度伪造检测",
                "黑盒设置",
                "可转移性",
                "视觉不可察觉性",
                "两阶段框架"
            ],
            "_index": 54
        },
        {
            "title": "A Function Centric Perspective On Flat and Sharp Minima",
            "authors": [
                "Israel Mason-Williams",
                "Gabryel Mason-Williams",
                "Helen Yannakoudakis"
            ],
            "arxiv_id": "2510.12451v1",
            "summary": "Flat minima are widely believed to correlate with improved generalisation in\ndeep neural networks. However, this connection has proven more nuanced in\nrecent studies, with both theoretical counterexamples and empirical exceptions\nemerging in the literature. In this paper, we revisit the role of sharpness in\nmodel performance, proposing that sharpness is better understood as a\nfunction-dependent property rather than a reliable indicator of poor\ngeneralisation. We conduct extensive empirical studies, from single-objective\noptimisation to modern image classification tasks, showing that sharper minima\noften emerge when models are regularised (e.g., via SAM, weight decay, or data\naugmentation), and that these sharp minima can coincide with better\ngeneralisation, calibration, robustness, and functional consistency. Across a\nrange of models and datasets, we find that baselines without regularisation\ntend to converge to flatter minima yet often perform worse across all safety\nmetrics. Our findings demonstrate that function complexity, rather than\nflatness alone, governs the geometry of solutions, and that sharper minima can\nreflect more appropriate inductive biases (especially under regularisation),\ncalling for a function-centric reappraisal of loss landscape geometry.",
            "headline_zh": "提出函数中心视角，重新评估平坦与尖锐极小值在深度学习泛化中的作用。",
            "intro_zh": [
                "核心问题：平坦极小值与泛化性能的关联在理论和实践中存在例外，需要更深入理解。",
                "方法要点：将尖锐性视为函数依赖属性，通过正则化实验探索其对泛化的影响。",
                "实验或效果：正则化下尖锐极小值常伴随更好泛化、校准、鲁棒性和功能一致性。"
            ],
            "tags_zh": [
                "深度学习泛化",
                "损失景观几何",
                "正则化方法",
                "尖锐极小值",
                "函数复杂度"
            ],
            "_index": 55
        },
        {
            "title": "A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation",
            "authors": [
                "Shaoyang Zhou",
                "Yingshu Li",
                "Yunyi Liu",
                "Lingqiao Liu",
                "Lei Wang",
                "Luping Zhou"
            ],
            "arxiv_id": "2510.12444v1",
            "summary": "Chest Xray imaging is a widely used diagnostic tool in modern medicine, and\nits high utilization creates substantial workloads for radiologists. To\nalleviate this burden, vision language models are increasingly applied to\nautomate Chest Xray radiology report generation (CXRRRG), aiming for clinically\naccurate descriptions while reducing manual effort. Conventional approaches,\nhowever, typically rely on single images, failing to capture the longitudinal\ncontext necessary for producing clinically faithful comparison statements.\nRecently, growing attention has been directed toward incorporating longitudinal\ndata into CXR RRG, enabling models to leverage historical studies in ways that\nmirror radiologists diagnostic workflows. Nevertheless, existing surveys\nprimarily address single image CXRRRG and offer limited guidance for\nlongitudinal settings, leaving researchers without a systematic framework for\nmodel design. To address this gap, this survey provides the first comprehensive\nreview of longitudinal radiology report generation (LRRG). Specifically, we\nexamine dataset construction strategies, report generation architectures\nalongside longitudinally tailored designs, and evaluation protocols\nencompassing both longitudinal specific measures and widely used benchmarks. We\nfurther summarize LRRG methods performance, alongside analyses of different\nablation studies, which collectively highlight the critical role of\nlongitudinal information and architectural design choices in improving model\nperformance. Finally, we summarize five major limitations of current research\nand outline promising directions for future development, aiming to lay a\nfoundation for advancing this emerging field.",
            "headline_zh": "综述纵向放射学报告生成，涵盖数据集、方法和评估，以提升临床准确性。",
            "intro_zh": [
                "核心问题：传统方法依赖单张图像，无法捕捉纵向上下文，影响报告临床准确性。",
                "方法要点：整合纵向数据，设计专用架构，模拟放射科医生诊断工作流程。",
                "实验或效果：分析性能与消融研究，强调纵向信息和架构设计对模型改进的关键作用。"
            ],
            "tags_zh": [
                "纵向放射学报告生成",
                "数据集构建",
                "模型架构设计",
                "性能评估",
                "消融研究",
                "临床准确性"
            ],
            "_index": 56
        },
        {
            "title": "Tensor Completion via Monotone Inclusion: Generalized Low-Rank Priors Meet Deep Denoisers",
            "authors": [
                "Peng Chen",
                "Deliang Wei",
                "Jiale Yao",
                "Fang Li"
            ],
            "arxiv_id": "2510.12425v1",
            "summary": "Missing entries in multi dimensional data pose significant challenges for\ndownstream analysis across diverse real world applications. These data are\nnaturally modeled as tensors, and recent completion methods integrating global\nlow rank priors with plug and play denoisers have demonstrated strong empirical\nperformance. However, these approaches often rely on empirical convergence\nalone or unrealistic assumptions, such as deep denoisers acting as proximal\noperators of implicit regularizers, which generally does not hold. To address\nthese limitations, we propose a novel tensor completion framework grounded in\nthe monotone inclusion paradigm, which unifies generalized low rank priors with\ndeep pseudo contractive denoisers and extends beyond traditional convex\noptimization. Building on the Davis Yin splitting scheme, we develop the GTCTV\nDPC algorithm and rigorously establish its global convergence. Extensive\nexperiments demonstrate that GTCTV DPC consistently outperforms existing\nmethods in both quantitative metrics and visual quality, particularly at low\nsampling rates.",
            "headline_zh": "提出基于单调包含范式的张量补全框架，融合广义低秩先验与深度伪压缩去噪器",
            "intro_zh": [
                "核心问题：多维数据缺失条目影响下游分析，现有方法依赖经验收敛或不现实假设",
                "方法要点：采用单调包含范式统一广义低秩先验与深度伪压缩去噪器，开发GTCTV DPC算法",
                "实验或效果：在低采样率下，GTCTV DPC在定量指标和视觉质量上优于现有方法"
            ],
            "tags_zh": [
                "张量补全",
                "单调包含范式",
                "广义低秩先验",
                "深度去噪器",
                "全局收敛分析",
                "低采样率性能"
            ],
            "_index": 57
        },
        {
            "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding",
            "authors": [
                "Jialong Zuo",
                "Yongtai Deng",
                "Lingdong Kong",
                "Jingkang Yang",
                "Rui Jin",
                "Yiwei Zhang",
                "Nong Sang",
                "Liang Pan",
                "Ziwei Liu",
                "Changxin Gao"
            ],
            "arxiv_id": "2510.12422v1",
            "summary": "Recent studies have shown that agent-based systems leveraging large language\nmodels (LLMs) for key information retrieval and integration have emerged as a\npromising approach for long video understanding. However, these systems face\ntwo major challenges. First, they typically perform modeling and reasoning on\nindividual frames, struggling to capture the temporal context of consecutive\nframes. Second, to reduce the cost of dense frame-level captioning, they adopt\nsparse frame sampling, which risks discarding crucial information. To overcome\nthese limitations, we propose VideoLucy, a deep memory backtracking framework\nfor long video understanding. Inspired by the human recollection process from\ncoarse to fine, VideoLucy employs a hierarchical memory structure with\nprogressive granularity. This structure explicitly defines the detail level and\ntemporal scope of memory at different hierarchical depths. Through an\nagent-based iterative backtracking mechanism, VideoLucy systematically mines\nvideo-wide, question-relevant deep memories until sufficient information is\ngathered to provide a confident answer. This design enables effective temporal\nunderstanding of consecutive frames while preserving critical details. In\naddition, we introduce EgoMem, a new benchmark for long video understanding.\nEgoMem is designed to comprehensively evaluate a model's ability to understand\ncomplex events that unfold over time and capture fine-grained details in\nextremely long videos. Extensive experiments demonstrate the superiority of\nVideoLucy. Built on open-source models, VideoLucy significantly outperforms\nstate-of-the-art methods on multiple long video understanding benchmarks,\nachieving performance even surpassing the latest proprietary models such as\nGPT-4o. Our code and dataset will be made publicly at\nhttps://videolucy.github.io",
            "headline_zh": "提出VideoLucy框架以解决长视频理解中的时序上下文和关键信息丢失问题",
            "intro_zh": [
                "核心问题：现有基于LLM的代理系统难以捕捉连续帧的时序上下文，且稀疏采样易丢失关键信息",
                "方法要点：采用分层记忆结构和迭代回溯机制，从粗到细挖掘视频全局的深度记忆",
                "实验或效果：在多个基准测试中显著优于现有方法，性能超越GPT-4o等专有模型"
            ],
            "tags_zh": [
                "长视频理解",
                "分层记忆结构",
                "迭代回溯机制",
                "EgoMem基准",
                "代理系统",
                "时序上下文建模"
            ],
            "_index": 58
        },
        {
            "title": "M3D-skin: Multi-material 3D-printed Tactile Sensor with Hierarchical Infill Structures for Pressure Sensing",
            "authors": [
                "Shunnosuke Yoshimura",
                "Kento Kawaharazuka",
                "Kei Okada"
            ],
            "arxiv_id": "2510.12419v1",
            "summary": "Tactile sensors have a wide range of applications, from utilization in\nrobotic grippers to human motion measurement. If tactile sensors could be\nfabricated and integrated more easily, their applicability would further\nexpand. In this study, we propose a tactile sensor-M3D-skin-that can be easily\nfabricated with high versatility by leveraging the infill patterns of a\nmulti-material fused deposition modeling (FDM) 3D printer as the sensing\nprinciple. This method employs conductive and non-conductive flexible filaments\nto create a hierarchical structure with a specific infill pattern. The flexible\nhierarchical structure deforms under pressure, leading to a change in\nelectrical resistance, enabling the acquisition of tactile information. We\nmeasure the changes in characteristics of the proposed tactile sensor caused by\nmodifications to the hierarchical structure. Additionally, we demonstrate the\nfabrication and use of a multi-tile sensor. Furthermore, as applications, we\nimplement motion pattern measurement on the sole of a foot, integration with a\nrobotic hand, and tactile-based robotic operations. Through these experiments,\nwe validate the effectiveness of the proposed tactile sensor.",
            "headline_zh": "提出多材料3D打印触觉传感器M3D-skin，利用填充结构实现压力感知，应用于机器人抓取和人体运动测量。",
            "intro_zh": [
                "核心问题：触觉传感器制造和集成困难，限制了其广泛应用。",
                "方法要点：使用多材料FDM 3D打印，构建柔性分层填充结构，通过电阻变化感知压力。",
                "实验或效果：验证传感器特性变化，并演示在足底运动测量和机器人操作中的应用。"
            ],
            "tags_zh": [
                "触觉传感器",
                "多材料3D打印",
                "分层填充结构",
                "压力感知",
                "机器人集成",
                "柔性电子"
            ],
            "_index": 59
        },
        {
            "title": "Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model",
            "authors": [
                "Huu Tien Nguyen",
                "Ahmed Karam Eldaly"
            ],
            "arxiv_id": "2510.12408v1",
            "summary": "This paper introduces a novel framework for image quality transfer based on\nconditional flow matching (CFM). Unlike conventional generative models that\nrely on iterative sampling or adversarial objectives, CFM learns a continuous\nflow between a noise distribution and target data distributions through the\ndirect regression of an optimal velocity field. We evaluate this approach in\nthe context of low-field magnetic resonance imaging (LF-MRI), a rapidly\nemerging modality that offers affordable and portable scanning but suffers from\ninherently low signal-to-noise ratio and reduced diagnostic quality. Our\nframework is designed to reconstruct high-field-like MR images from their\ncorresponding low-field inputs, thereby bridging the quality gap without\nrequiring expensive infrastructure. Experiments demonstrate that CFM not only\nachieves state-of-the-art performance, but also generalizes robustly to both\nin-distribution and out-of-distribution data. Importantly, it does so while\nutilizing significantly fewer parameters than competing deep learning methods.\nThese results underline the potential of CFM as a powerful and scalable tool\nfor MRI reconstruction, particularly in resource-limited clinical environments.",
            "headline_zh": "提出条件流匹配模型以增强低场磁共振图像质量",
            "intro_zh": [
                "低场磁共振成像信号噪声比低，诊断质量差",
                "使用条件流匹配直接回归最优速度场，学习噪声到目标的连续流",
                "实验显示性能领先，泛化强，参数少，适用于资源有限环境"
            ],
            "tags_zh": [
                "条件流匹配",
                "磁共振成像重建",
                "图像质量增强",
                "低场MRI",
                "生成模型"
            ],
            "_index": 60
        },
        {
            "title": "Robot Learning: A Tutorial",
            "authors": [
                "Francesco Capuano",
                "Caroline Pascal",
                "Adil Zouitine",
                "Thomas Wolf",
                "Michel Aractingi"
            ],
            "arxiv_id": "2510.12403v1",
            "summary": "Robot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\n$\\texttt{lerobot}$.",
            "headline_zh": "提供机器人学习教程，涵盖从基础强化学习到通用语言条件模型。",
            "intro_zh": [
                "核心问题：机器人学习从基于模型方法转向数据驱动范式，以提升自主系统能力。",
                "方法要点：介绍强化学习、行为克隆和语言条件模型等关键方法。",
                "实验或效果：包含使用lerobot的实用示例，帮助读者快速上手。"
            ],
            "tags_zh": [
                "机器人学习",
                "强化学习",
                "行为克隆",
                "语言条件模型",
                "数据驱动方法",
                "lerobot库"
            ],
            "_index": 61
        },
        {
            "title": "Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda",
            "authors": [
                "André Torneiro",
                "Diogo Monteiro",
                "Paulo Novais",
                "Pedro Rangel Henriques",
                "Nuno F. Rodrigues"
            ],
            "arxiv_id": "2510.12400v1",
            "summary": "Urban monitoring of public infrastructure (such as waste bins, road signs,\nvegetation, sidewalks, and construction sites) poses significant challenges due\nto the diversity of objects, environments, and contextual conditions involved.\nCurrent state-of-the-art approaches typically rely on a combination of IoT\nsensors and manual inspections, which are costly, difficult to scale, and often\nmisaligned with citizens' perception formed through direct visual observation.\nThis raises a critical question: Can machines now \"see\" like citizens and infer\ninformed opinions about the condition of urban infrastructure? Vision-Language\nModels (VLMs), which integrate visual understanding with natural language\nreasoning, have recently demonstrated impressive capabilities in processing\ncomplex visual information, turning them into a promising technology to address\nthis challenge. This systematic review investigates the role of VLMs in urban\nmonitoring, with particular emphasis on zero-shot applications. Following the\nPRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021\nand 2025 to address four core research questions: (1) What urban monitoring\ntasks have been effectively addressed using VLMs? (2) Which VLM architectures\nand frameworks are most commonly used and demonstrate superior performance? (3)\nWhat datasets and resources support this emerging field? (4) How are VLM-based\napplications evaluated, and what performance levels have been reported?",
            "headline_zh": "综述视觉语言模型在城市监测中的应用，评估零样本性能并提出研究议程",
            "intro_zh": [
                "核心问题：城市基础设施监测因对象多样性和环境复杂性面临挑战，需低成本可扩展方案",
                "方法要点：利用视觉语言模型结合视觉理解和语言推理，支持零样本应用",
                "实验或效果：分析32项研究，评估任务、架构、数据集和性能，显示VLM潜力"
            ],
            "tags_zh": [
                "城市监测",
                "视觉语言模型",
                "零样本学习",
                "系统综述",
                "基础设施评估"
            ],
            "_index": 62
        },
        {
            "title": "Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking",
            "authors": [
                "Junhyuk So",
                "Chiwoong Lee",
                "Shinyoung Lee",
                "Jungseul Ok",
                "Eunhyeok Park"
            ],
            "arxiv_id": "2510.12392v1",
            "summary": "Generative Behavior Cloning (GBC) is a simple yet effective framework for\nrobot learning, particularly in multi-task settings. Recent GBC methods often\nemploy diffusion policies with open-loop (OL) control, where actions are\ngenerated via a diffusion process and executed in multi-step chunks without\nreplanning. While this approach has demonstrated strong success rates and\ngeneralization, its inherent stochasticity can result in erroneous action\nsampling, occasionally leading to unexpected task failures. Moreover, OL\ncontrol suffers from delayed responses, which can degrade performance in noisy\nor dynamic environments. To address these limitations, we propose two novel\ntechniques to enhance the consistency and reactivity of diffusion policies: (1)\nself-guidance, which improves action fidelity by leveraging past observations\nand implicitly promoting future-aware behavior; and (2) adaptive chunking,\nwhich selectively updates action sequences when the benefits of reactivity\noutweigh the need for temporal consistency. Extensive experiments show that our\napproach substantially improves GBC performance across a wide range of\nsimulated and real-world robotic manipulation tasks. Our code is available at\nhttps://github.com/junhyukso/SGAC",
            "headline_zh": "提出自引导与自适应分块以提升生成行为克隆在机器人学习中的一致性与反应性",
            "intro_zh": [
                "核心问题：扩散策略在开环控制中因随机性导致动作错误和延迟响应，影响任务性能。",
                "方法要点：引入自引导利用过去观察提升动作保真度，自适应分块根据需求选择性更新动作序列。",
                "实验或效果：在模拟和真实机器人操作任务中显著提升性能，代码已开源。"
            ],
            "tags_zh": [
                "生成行为克隆",
                "扩散策略",
                "自引导",
                "自适应分块",
                "机器人学习",
                "多任务学习"
            ],
            "_index": 63
        },
        {
            "title": "Scene Coordinate Reconstruction Priors",
            "authors": [
                "Wenjing Bian",
                "Axel Barroso-Laguna",
                "Tommaso Cavallari",
                "Victor Adrian Prisacariu",
                "Eric Brachmann"
            ],
            "arxiv_id": "2510.12387v1",
            "summary": "Scene coordinate regression (SCR) models have proven to be powerful implicit\nscene representations for 3D vision, enabling visual relocalization and\nstructure-from-motion. SCR models are trained specifically for one scene. If\ntraining images imply insufficient multi-view constraints SCR models\ndegenerate. We present a probabilistic reinterpretation of training SCR models,\nwhich allows us to infuse high-level reconstruction priors. We investigate\nmultiple such priors, ranging from simple priors over the distribution of\nreconstructed depth values to learned priors over plausible scene coordinate\nconfigurations. For the latter, we train a 3D point cloud diffusion model on a\nlarge corpus of indoor scans. Our priors push predicted 3D scene points towards\nplausible geometry at each training step to increase their likelihood. On three\nindoor datasets our priors help learning better scene representations,\nresulting in more coherent scene point clouds, higher registration rates and\nbetter camera poses, with a positive effect on down-stream tasks such as novel\nview synthesis and camera relocalization.",
            "headline_zh": "提出场景坐标重建先验以提升SCR模型在训练图像不足时的性能",
            "intro_zh": [
                "核心问题：SCR模型在训练图像多视角约束不足时退化，导致场景表示不准确",
                "方法要点：引入概率重解释，融合深度分布和点云扩散先验，优化预测场景点",
                "实验或效果：在室内数据集上提升点云一致性、注册率和相机位姿，改善下游任务"
            ],
            "tags_zh": [
                "场景坐标回归",
                "重建先验",
                "点云扩散模型",
                "视觉重定位",
                "室内场景",
                "多视角约束"
            ],
            "_index": 64
        },
        {
            "title": "Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling",
            "authors": [
                "Tim J. Schoonbeek",
                "Shao-Hsuan Hung",
                "Dan Lehman",
                "Hans Onvlee",
                "Jacek Kustra",
                "Peter H. N. de With",
                "Fons van der Sommen"
            ],
            "arxiv_id": "2510.12385v1",
            "summary": "Procedure step recognition (PSR) aims to identify all correctly completed\nsteps and their sequential order in videos of procedural tasks. The existing\nstate-of-the-art models rely solely on detecting assembly object states in\nindividual video frames. By neglecting temporal features, model robustness and\naccuracy are limited, especially when objects are partially occluded. To\novercome these limitations, we propose Spatio-Temporal Occlusion-Resilient\nModeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework\nfor PSR that leverages both spatial and temporal features. The assembly state\ndetection stream operates effectively with unobstructed views of the object,\nwhile the spatio-temporal stream captures both spatial and temporal features to\nrecognize step completions even under partial occlusion. This stream includes a\nspatial encoder, pre-trained using a novel weakly supervised approach to\ncapture meaningful spatial representations, and a transformer-based temporal\nencoder that learns how these spatial features relate over time. STORM-PSR is\nevaluated on the MECCANO and IndustReal datasets, reducing the average delay\nbetween actual and predicted assembly step completions by 11.2% and 26.1%,\nrespectively, compared to prior methods. We demonstrate that this reduction in\ndelay is driven by the spatio-temporal stream, which does not rely on\nunobstructed views of the object to infer completed steps. The code for\nSTORM-PSR, along with the newly annotated MECCANO labels, is made publicly\navailable at https://timschoonbeek.github.io/stormpsr .",
            "headline_zh": "提出STORM-PSR双流框架，通过时空建模提升第一人称装配视频中步骤识别的鲁棒性。",
            "intro_zh": [
                "核心问题：现有方法仅依赖单帧物体状态检测，忽略时序特征，导致在部分遮挡时识别精度受限。",
                "方法要点：结合空间流和时空流，空间编码器预训练捕获空间特征，时序编码器基于Transformer建模时间关系。",
                "实验或效果：在MECCANO和IndustReal数据集上，平均延迟分别降低11.2%和26.1%，优于先前方法。"
            ],
            "tags_zh": [
                "步骤识别",
                "时空建模",
                "第一人称视频",
                "遮挡鲁棒性",
                "Transformer编码器",
                "弱监督学习"
            ],
            "_index": 65
        },
        {
            "title": "Deep Attention-guided Adaptive Subsampling",
            "authors": [
                "Sharath M Shankaranarayana",
                "Soumava Kumar Roy",
                "Prasad Sudhakar",
                "Chandan Aladahalli"
            ],
            "arxiv_id": "2510.12376v1",
            "summary": "Although deep neural networks have provided impressive gains in performance,\nthese improvements often come at the cost of increased computational complexity\nand expense. In many cases, such as 3D volume or video classification tasks,\nnot all slices or frames are necessary due to inherent redundancies. To address\nthis issue, we propose a novel learnable subsampling framework that can be\nintegrated into any neural network architecture. Subsampling, being a\nnondifferentiable operation, poses significant challenges for direct adaptation\ninto deep learning models. While some works, have proposed solutions using the\nGumbel-max trick to overcome the problem of non-differentiability, they fall\nshort in a crucial aspect: they are only task-adaptive and not inputadaptive.\nOnce the sampling mechanism is learned, it remains static and does not adjust\nto different inputs, making it unsuitable for real-world applications. To this\nend, we propose an attention-guided sampling module that adapts to inputs even\nduring inference. This dynamic adaptation results in performance gains and\nreduces complexity in deep neural network models. We demonstrate the\neffectiveness of our method on 3D medical imaging datasets from MedMNIST3D as\nwell as two ultrasound video datasets for classification tasks, one of them\nbeing a challenging in-house dataset collected under real-world clinical\nconditions.",
            "headline_zh": "提出注意力引导自适应子采样框架，以降低3D医学影像和视频分类的计算复杂度。",
            "intro_zh": [
                "核心问题：深度神经网络计算复杂度高，且现有子采样方法无法根据输入动态调整。",
                "方法要点：使用注意力机制实现输入自适应子采样，克服不可微操作挑战。",
                "实验或效果：在MedMNIST3D和超声视频数据集上验证，提升性能并减少计算量。"
            ],
            "tags_zh": [
                "自适应子采样",
                "注意力机制",
                "3D医学影像分类",
                "视频分类",
                "计算复杂度优化"
            ],
            "_index": 66
        },
        {
            "title": "Controlling Intent Expressiveness in Robot Motion with Diffusion Models",
            "authors": [
                "Wenli Shi",
                "Clemence Grislain",
                "Olivier Sigaud",
                "Mohamed Chetouani"
            ],
            "arxiv_id": "2510.12370v1",
            "summary": "Legibility of robot motion is critical in human-robot interaction, as it\nallows humans to quickly infer a robot's intended goal. Although traditional\ntrajectory generation methods typically prioritize efficiency, they often fail\nto make the robot's intentions clear to humans. Meanwhile, existing approaches\nto legible motion usually produce only a single \"most legible\" trajectory,\noverlooking the need to modulate intent expressiveness in different contexts.\nIn this work, we propose a novel motion generation framework that enables\ncontrollable legibility across the full spectrum, from highly legible to highly\nambiguous motions. We introduce a modeling approach based on an Information\nPotential Field to assign continuous legibility scores to trajectories, and\nbuild upon it with a two-stage diffusion framework that first generates paths\nat specified legibility levels and then translates them into executable robot\nactions. Experiments in both 2D and 3D reaching tasks demonstrate that our\napproach produces diverse and controllable motions with varying degrees of\nlegibility, while achieving performance comparable to SOTA. Code and project\npage: https://legibility-modulator.github.io.",
            "headline_zh": "提出基于扩散模型的机器人运动生成框架，实现意图表达可控性",
            "intro_zh": [
                "核心问题：传统机器人运动方法效率优先，但意图可读性不足，且现有方法仅生成单一最可读轨迹",
                "方法要点：使用信息势场模型评估轨迹可读性，结合两阶段扩散框架生成不同可读性水平的路径和动作",
                "实验或效果：在2D和3D到达任务中验证，生成多样可控运动，性能接近SOTA"
            ],
            "tags_zh": [
                "机器人运动生成",
                "意图可读性",
                "扩散模型",
                "信息势场",
                "人机交互"
            ],
            "_index": 67
        },
        {
            "title": "Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control",
            "authors": [
                "Jiale Fan",
                "Andrei Cramariuc",
                "Tifanny Portela",
                "Marco Hutter"
            ],
            "arxiv_id": "2510.12363v1",
            "summary": "The pretraining-finetuning paradigm has facilitated numerous transformative\nadvancements in artificial intelligence research in recent years. However, in\nthe domain of reinforcement learning (RL) for robot motion control, individual\nskills are often learned from scratch despite the high likelihood that some\ngeneralizable knowledge is shared across all task-specific policies belonging\nto a single robot embodiment. This work aims to define a paradigm for\npretraining neural network models that encapsulate such knowledge and can\nsubsequently serve as a basis for warm-starting the RL process in classic\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin\nwith a task-agnostic exploration-based data collection algorithm to gather\ndiverse, dynamic transition data, which is then used to train a Proprioceptive\nInverse Dynamics Model (PIDM) through supervised learning. The pretrained\nweights are loaded into both the actor and critic networks to warm-start the\npolicy optimization of actual tasks. We systematically validated our proposed\nmethod on seven distinct robot motion control tasks, showing significant\nbenefits to this initialization strategy. Our proposed approach on average\nimproves sample efficiency by 40.1% and task performance by 7.5%, compared to\nrandom initialization. We further present key ablation studies and empirical\nanalyses that shed light on the mechanisms behind the effectiveness of our\nmethod.",
            "headline_zh": "提出基于预训练的演员-评论家强化学习方法，以提升机器人运动控制的样本效率和性能",
            "intro_zh": [
                "核心问题：机器人运动控制中强化学习常从零开始学习，缺乏跨任务通用知识共享。",
                "方法要点：通过任务无关探索收集数据，训练PIDM模型，预训练权重用于初始化演员-评论家网络。",
                "实验或效果：在七项任务中，样本效率平均提升40.1%，任务性能平均提升7.5%。"
            ],
            "tags_zh": [
                "强化学习",
                "机器人运动控制",
                "预训练",
                "演员-评论家算法",
                "样本效率",
                "PIDM模型"
            ],
            "_index": 68
        },
        {
            "title": "CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion",
            "authors": [
                "Jinzhou Lin",
                "Jie Zhou",
                "Wenhao Xu",
                "Rongtao Xu",
                "Changwei Wang",
                "Shunpeng Chen",
                "Kexue Fu",
                "Yihua Shao",
                "Li Guo",
                "Shibiao Xu"
            ],
            "arxiv_id": "2510.12362v1",
            "summary": "Semantic Scene Completion (SSC) aims to infer complete 3D geometry and\nsemantics from monocular images, serving as a crucial capability for\ncamera-based perception in autonomous driving. However, existing SSC methods\nrelying on temporal stacking or depth projection often lack explicit motion\nreasoning and struggle with occlusions and noisy depth supervision. We propose\nCurriFlow, a novel semantic occupancy prediction framework that integrates\noptical flow-based temporal alignment with curriculum-guided depth fusion.\nCurriFlow employs a multi-level fusion strategy to align segmentation, visual,\nand depth features across frames using pre-trained optical flow, thereby\nimproving temporal consistency and dynamic object understanding. To enhance\ngeometric robustness, a curriculum learning mechanism progressively transitions\nfrom sparse yet accurate LiDAR depth to dense but noisy stereo depth during\ntraining, ensuring stable optimization and seamless adaptation to real-world\ndeployment. Furthermore, semantic priors from the Segment Anything Model (SAM)\nprovide category-agnostic supervision, strengthening voxel-level semantic\nlearning and spatial consistency. Experiments on the SemanticKITTI benchmark\ndemonstrate that CurriFlow achieves state-of-the-art performance with a mean\nIoU of 16.9, validating the effectiveness of our motion-guided and\ncurriculum-aware design for camera-based 3D semantic scene completion.",
            "headline_zh": "提出CurriFlow框架，通过光流时序对齐和课程深度融合解决自动驾驶中3D语义场景补全问题",
            "intro_zh": [
                "核心问题：现有方法缺乏显式运动推理，难以处理遮挡和噪声深度监督",
                "方法要点：集成光流时序对齐与课程学习，从稀疏LiDAR深度过渡到密集立体深度",
                "实验效果：在SemanticKITTI基准上达到16.9 mIoU，验证运动引导和课程感知设计的有效性"
            ],
            "tags_zh": [
                "3D语义场景补全",
                "光流时序对齐",
                "课程学习",
                "深度融合",
                "自动驾驶感知",
                "语义占用预测"
            ],
            "_index": 69
        },
        {
            "title": "A Unidirectionally Connected FAS Approach for 6-DOF Quadrotor Control",
            "authors": [
                "Weijie Ren",
                "Haowen Liu",
                "Guang-Ren Duan"
            ],
            "arxiv_id": "2510.12360v1",
            "summary": "This paper proposes a unidirectionally connected fully actuated system\n(UC-FAS) approach for the sub-stabilization and tracking control of 6-DOF\nquadrotors, tackling limitations both in state-space and FAS framework to some\nextent. The framework systematically converts underactuated quadrotor dynamics\ninto a UC-FAS model, unifying the existing different FAS transformation ways.\nBy eliminating estimation of the high-order derivatives of control inputs, a\ndrawback of current methods, the UC-FAS model simplifies controller design and\nenables direct eigenstructure assignment for closed-loop dynamics. Simulations\ndemonstrate precise 6-DOF tracking performance. This work bridges theoretical\nFAS approach advancements with practical implementation needs, offering a\nstandardized paradigm for nonlinear quadrotor control.",
            "headline_zh": "提出单向连接全驱动系统方法以改进六自由度四旋翼控制",
            "intro_zh": [
                "解决六自由度四旋翼在状态空间和全驱动系统框架中的控制限制问题",
                "将欠驱动动力学转换为单向连接全驱动模型，简化控制器设计",
                "仿真验证了精确的六自由度跟踪性能，推进理论到实践应用"
            ],
            "tags_zh": [
                "四旋翼控制",
                "全驱动系统",
                "非线性控制",
                "六自由度跟踪",
                "控制器设计"
            ],
            "_index": 70
        },
        {
            "title": "PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing",
            "authors": [
                "Bingquan Li",
                "Ning Wang",
                "Tianwei Zhang",
                "Zhicheng He",
                "Yucong Wu"
            ],
            "arxiv_id": "2510.12346v1",
            "summary": "Recently, biped robot walking technology has been significantly developed,\nmainly in the context of a bland walking scheme. To emulate human walking,\nrobots need to step on the positions they see in unknown spaces accurately. In\nthis paper, we present PolyMap, a perception-based locomotion planning\nframework for humanoid robots to climb stairs. Our core idea is to build a\nreal-time polygonal staircase plane semantic map, followed by a footstep planar\nusing these polygonal plane segments. These plane segmentation and visual\nodometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs). The\nproposed framework is deployed on a NVIDIA Orin, which performs 20-30 Hz\nwhole-body motion planning output. Both indoor and outdoor real-scene\nexperiments indicate that our method is efficient and robust for humanoid robot\nstair climbing.",
            "headline_zh": "提出PolygMap感知运动框架以解决人形机器人楼梯攀爬问题",
            "intro_zh": [
                "核心问题：人形机器人需在未知空间中精确踩踏位置以模拟人类行走",
                "方法要点：构建实时多边形楼梯平面语义地图，并基于此进行脚步规划",
                "实验或效果：在NVIDIA Orin上实现20-30Hz全身运动规划，室内外实验显示高效鲁棒"
            ],
            "tags_zh": [
                "人形机器人",
                "楼梯攀爬",
                "感知运动规划",
                "多边形语义地图",
                "多传感器融合",
                "实时规划"
            ],
            "_index": 71
        },
        {
            "title": "Achieving Meaningful Collaboration: Worker-centered Design of a Physical Human-Robot Collaborative Blending Task",
            "authors": [
                "Nicky Mol",
                "Luka Peternel",
                "Alessandro Ianniello",
                "Denis Zatyagov",
                "Auke Nachenius",
                "Stephan Balvert",
                "J. Micah Prendergast",
                "Sara Muscolo",
                "Olger Siebinga",
                "Eva Verhoef",
                "Deborah Forster",
                "David A. Abbink"
            ],
            "arxiv_id": "2510.12340v1",
            "summary": "The use of robots in industrial settings continues to grow, driven by the\nneed to address complex societal challenges such as labor shortages, aging\npopulations, and ever-increasing production demands. In this abstract, we\nadvocate for (and demonstrate) a transdisciplinary approach when considering\nrobotics in the workplace. Transdisciplinarity emphasizes the integration of\nacademic research with pragmatic expertise and embodied experiential knowledge,\nthat prioritize values such as worker wellbeing and job attractiveness. In the\nfollowing, we describe an ongoing multi-pronged effort to explore the potential\nof collaborative robots in the context of airplane engine repair and\nmaintenance operations.",
            "headline_zh": "提出跨学科方法以优化飞机发动机维修中的人机协作任务",
            "intro_zh": [
                "核心问题：工业机器人应用需应对劳动力短缺和工人福祉等社会挑战",
                "方法要点：采用跨学科方法，整合学术研究与工人实践经验",
                "实验或效果：在飞机发动机维修场景中探索协作机器人潜力"
            ],
            "tags_zh": [
                "人机协作",
                "跨学科方法",
                "工业机器人",
                "工人福祉",
                "飞机维修"
            ],
            "_index": 72
        },
        {
            "title": "Shape-Aware Whole-Body Control for Continuum Robots with Application in Endoluminal Surgical Robotics",
            "authors": [
                "Mohammadreza Kasaei",
                "Mostafa Ghobadi",
                "Mohsen Khadem"
            ],
            "arxiv_id": "2510.12332v1",
            "summary": "This paper presents a shape-aware whole-body control framework for\ntendon-driven continuum robots with direct application to endoluminal surgical\nnavigation. Endoluminal procedures, such as bronchoscopy, demand precise and\nsafe navigation through tortuous, patient-specific anatomy where conventional\ntip-only control often leads to wall contact, tissue trauma, or failure to\nreach distal targets. To address these challenges, our approach combines a\nphysics-informed backbone model with residual learning through an Augmented\nNeural ODE, enabling accurate shape estimation and efficient Jacobian\ncomputation. A sampling-based Model Predictive Path Integral (MPPI) controller\nleverages this representation to jointly optimize tip tracking, backbone\nconformance, and obstacle avoidance under actuation constraints. A task manager\nfurther enhances adaptability by allowing real-time adjustment of objectives,\nsuch as wall clearance or direct advancement, during tele-operation. Extensive\nsimulation studies demonstrate millimeter-level accuracy across diverse\nscenarios, including trajectory tracking, dynamic obstacle avoidance, and\nshape-constrained reaching. Real-robot experiments on a bronchoscopy phantom\nvalidate the framework, showing improved lumen-following accuracy, reduced wall\ncontacts, and enhanced adaptability compared to joystick-only navigation and\nexisting baselines. These results highlight the potential of the proposed\nframework to increase safety, reliability, and operator efficiency in minimally\ninvasive endoluminal surgery, with broader applicability to other confined and\nsafety-critical environments.",
            "headline_zh": "提出形状感知全身控制框架以解决内腔手术中连续机器人的精确导航问题",
            "intro_zh": [
                "核心问题：内腔手术中传统仅尖端控制易导致壁接触和组织损伤，难以到达远端目标。",
                "方法要点：结合物理模型与增强神经ODE，实现形状估计和高效雅可比计算，并采用MPPI控制器优化跟踪与避障。",
                "实验或效果：仿真和真实机器人实验显示毫米级精度，减少壁接触，提高适应性。"
            ],
            "tags_zh": [
                "连续机器人控制",
                "内腔手术导航",
                "形状感知估计",
                "模型预测控制",
                "增强神经ODE",
                "避障优化"
            ],
            "_index": 73
        },
        {
            "title": "Hybrid Gaussian Splatting for Novel Urban View Synthesis",
            "authors": [
                "Mohamed Omran",
                "Farhad Zanjani",
                "Davide Abati",
                "Jens Petersen",
                "Amirhossein Habibian"
            ],
            "arxiv_id": "2510.12308v1",
            "summary": "This paper describes the Qualcomm AI Research solution to the RealADSim-NVS\nchallenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge\nconcerns novel view synthesis in street scenes, and participants are required\nto generate, starting from car-centric frames captured during some training\ntraversals, renders of the same urban environment as viewed from a different\ntraversal (e.g. different street lane or car direction). Our solution is\ninspired by hybrid methods in scene generation and generative simulators\nmerging gaussian splatting and diffusion models, and it is composed of two\nstages: First, we fit a 3D reconstruction of the scene and render novel views\nas seen from the target cameras. Then, we enhance the resulting frames with a\ndedicated single-step diffusion model. We discuss specific choices made in the\ninitialization of gaussian primitives as well as the finetuning of the enhancer\nmodel and its training data curation. We report the performance of our model\ndesign and we ablate its components in terms of novel view quality as measured\nby PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our\nproposal reaches an aggregated score of 0.432, achieving the second place\noverall.",
            "headline_zh": "提出混合高斯溅射与扩散模型方法，用于城市街景新视角合成。",
            "intro_zh": [
                "核心问题：从训练轨迹的车载帧生成不同轨迹（如不同车道）的新视角街景渲染。",
                "方法要点：先拟合3D场景重建并渲染新视角，再用单步扩散模型增强图像质量。",
                "实验或效果：在ICCV 2025挑战中获第二名，PSNR、SSIM和LPIPS指标评估性能。"
            ],
            "tags_zh": [
                "新视角合成",
                "高斯溅射",
                "扩散模型",
                "3D重建",
                "街景渲染"
            ],
            "_index": 74
        },
        {
            "title": "Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector",
            "authors": [
                "Sifan Li",
                "Hongkai Chen",
                "Yujun Cai",
                "Qingwen Ye",
                "Liyang Chen",
                "Junsong Yuan",
                "Yiwei Wang"
            ],
            "arxiv_id": "2510.12287v1",
            "summary": "Vision Language Models (VLMs) have achieved impressive progress in multimodal\nreasoning; yet, they remain vulnerable to hallucinations, where outputs are not\ngrounded in visual evidence. In this paper, we investigate a previously\noverlooked setting: logo hallucination, where models generate brand names or\ntextual content despite logos containing no visible words. Using curated splits\nof pure symbols, hybrids, and text-bearing logos, as well as the challenging\nHard-60 subset, we systematically measure hallucination across leading VLMs. We\nfurther probe robustness through nine structured perturbations and show that\nhallucinations persist even under strong distortions, with occlusion exposing\nthe sharpest weaknesses. Embedding-level analysis with open-weight LLaVA\ndemonstrates that hallucination is tied to a small subset of projector\ndimensions, and targeted ablation substantially reduces errors while preserving\nOCR accuracy. Together, these findings reveal that VLMs often rely on symbolic\npriors rather than genuine glyph perception, particularly for iconic circular\nlogos, and that projector subspaces play a decisive role in this failure mode.\nOur work contributes both a novel diagnostic lens and actionable mitigation\ninsights, highlighting projector disentanglement and OCR-guided decoding as\npromising directions for building more trustworthy multimodal systems.",
            "headline_zh": "揭示视觉语言模型因投影器语义纠缠导致徽标幻觉，并提出缓解方法",
            "intro_zh": [
                "核心问题：视觉语言模型在无文字徽标上产生品牌名称幻觉，依赖符号先验而非真实字形感知。",
                "方法要点：通过嵌入分析和针对性消融，识别并减少投影器维度中的幻觉相关子空间。",
                "实验效果：在扰动和遮挡测试中，幻觉持续存在，消融方法显著降低错误并保持OCR准确性。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "徽标幻觉",
                "投影器分析",
                "语义纠缠",
                "OCR引导解码",
                "多模态鲁棒性"
            ],
            "_index": 75
        },
        {
            "title": "Dual Learning with Dynamic Knowledge Distillation and Soft Alignment for Partially Relevant Video Retrieval",
            "authors": [
                "Jianfeng Dong",
                "Lei Huang",
                "Daizong Liu",
                "Xianke Chen",
                "Xun Yang",
                "Changting Lin",
                "Xun Wang",
                "Meng Wang"
            ],
            "arxiv_id": "2510.12283v1",
            "summary": "Almost all previous text-to-video retrieval works ideally assume that videos\nare pre-trimmed with short durations containing solely text-related content.\nHowever, in practice, videos are typically untrimmed in long durations with\nmuch more complicated background content. Therefore, in this paper, we focus on\nthe more practical yet challenging task of Partially Relevant Video Retrieval\n(PRVR), which aims to retrieve partially relevant untrimmed videos with the\ngiven query. To tackle this task, we propose a novel framework that distills\ngeneralization knowledge from a powerful large-scale vision-language\npre-trained model and transfers it to a lightweight, task-specific PRVR\nnetwork. Specifically, we introduce a Dual Learning framework with Dynamic\nKnowledge Distillation (DL-DKD++), where a large teacher model provides\nsupervision to a compact dual-branch student network. The student model\ncomprises two branches: an inheritance branch that absorbs transferable\nknowledge from the teacher, and an exploration branch that learns task-specific\ninformation from the PRVR dataset to address domain gaps. To further enhance\nlearning, we incorporate a dynamic soft-target construction mechanism. By\nreplacing rigid hard-target supervision with adaptive soft targets that evolve\nduring training, our method enables the model to better capture the\nfine-grained, partial relevance between videos and queries. Experiment results\ndemonstrate that our proposed model achieves state-of-the-art performance on\nTVR, ActivityNet, and Charades-STA datasets for PRVR. The code is available at\nhttps://github.com/HuiGuanLab/DL-DKD.",
            "headline_zh": "提出动态知识蒸馏与软对齐的双学习框架，以解决部分相关长视频检索问题。",
            "intro_zh": [
                "核心问题：长视频中仅部分内容与查询相关，传统方法假设视频已修剪，不适用于实际场景。",
                "方法要点：使用大型教师模型监督轻量学生网络，通过继承和探索分支处理领域差异。",
                "实验效果：在TVR、ActivityNet和Charades-STA数据集上达到最先进性能。"
            ],
            "tags_zh": [
                "部分相关视频检索",
                "知识蒸馏",
                "双学习框架",
                "动态软目标",
                "长视频理解",
                "视觉语言模型"
            ],
            "_index": 76
        },
        {
            "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes",
            "authors": [
                "Ying A",
                "Wenzhang Sun",
                "Chang Zeng",
                "Chunfeng Wang",
                "Hao Li",
                "Jianxun Cui"
            ],
            "arxiv_id": "2510.12282v1",
            "summary": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet\ncurrent methods face a stark trade-off between fidelity and computational cost.\nThis inefficiency stems from their semantically agnostic design, which\nallocates resources uniformly, treating static backgrounds and safety-critical\nobjects with equal importance. To address this, we introduce Priority-Adaptive\nGaussian Splatting (PAGS), a framework that injects task-aware semantic\npriorities directly into the 3D reconstruction and rendering pipeline. PAGS\nintroduces two core contributions: (1) Semantically-Guided Pruning and\nRegularization strategy, which employs a hybrid importance metric to\naggressively simplify non-critical scene elements while preserving fine-grained\ndetails on objects vital for navigation. (2) Priority-Driven Rendering\npipeline, which employs a priority-based depth pre-pass to aggressively cull\noccluded primitives and accelerate the final shading computations. Extensive\nexperiments on the Waymo and KITTI datasets demonstrate that PAGS achieves\nexceptional reconstruction quality, particularly on safety-critical objects,\nwhile significantly reducing training time and boosting rendering speeds to\nover 350 FPS.",
            "headline_zh": "提出PAGS框架，通过语义优先级优化动态驾驶场景的3D重建与渲染效率",
            "intro_zh": [
                "核心问题：动态3D城市场景重建在保真度与计算成本间存在权衡，现有方法资源分配不均",
                "方法要点：引入语义引导剪枝与正则化策略，以及优先级驱动渲染管道，优化资源分配",
                "实验或效果：在Waymo和KITTI数据集上验证，提升重建质量并加速渲染至超过350 FPS"
            ],
            "tags_zh": [
                "3D场景重建",
                "高斯泼溅",
                "语义优先级",
                "动态驾驶场景",
                "渲染加速",
                "计算优化"
            ],
            "_index": 77
        },
        {
            "title": "Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model",
            "authors": [
                "Fuhao Li",
                "Wenxuan Song",
                "Han Zhao",
                "Jingbo Wang",
                "Pengxiang Ding",
                "Donglin Wang",
                "Long Zeng",
                "Haoang Li"
            ],
            "arxiv_id": "2510.12276v1",
            "summary": "Vision-language-action (VLA) models have recently shown strong potential in\nenabling robots to follow language instructions and execute precise actions.\nHowever, most VLAs are built upon vision-language models pretrained solely on\n2D data, which lack accurate spatial awareness and hinder their ability to\noperate in the 3D physical world. Existing solutions attempt to incorporate\nexplicit 3D sensor inputs such as depth maps or point clouds, but these\napproaches face challenges due to sensor noise, hardware heterogeneity, and\nincomplete depth coverage in existing datasets. Alternative methods that\nestimate 3D cues from 2D images also suffer from the limited performance of\ndepth estimators.We propose Spatial Forcing (SF), a simple yet effective\nalignment strategy that implicitly forces VLA models to develop spatial\ncomprehension capabilities without relying on explicit 3D inputs or depth\nestimators. SF aligns intermediate visual embeddings of VLAs with geometric\nrepresentations produced by pretrained 3D foundation models. By enforcing\nalignment at intermediate layers, SF guides VLAs to encode richer spatial\nrepresentations that enhance action precision.Extensive experiments in\nsimulation and real-world environments demonstrate that SF achieves\nstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further\naccelerates training by up to 3.8x and improves data efficiency across diverse\nrobotic tasks. Project page is at https://spatial-forcing.github.io/",
            "headline_zh": "提出Spatial Forcing以增强视觉-语言-动作模型在3D空间中的动作精度",
            "intro_zh": [
                "核心问题：基于2D数据的VLA模型缺乏空间感知，影响3D物理世界操作",
                "方法要点：通过中间层嵌入对齐，强制VLA模型学习隐式空间表示",
                "实验或效果：在仿真和真实环境中实现SOTA，加速训练并提升数据效率"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "空间表示对齐",
                "3D感知",
                "机器人控制",
                "嵌入对齐",
                "训练加速"
            ],
            "_index": 78
        },
        {
            "title": "SpineBench: Benchmarking Multimodal LLMs for Spinal Pathology Analysis",
            "authors": [
                "Chenghanyu Zhang",
                "Zekun Li",
                "Peipei Li",
                "Xing Cui",
                "Shuhan Xia",
                "Weixiang Yan",
                "Yiqiao Zhang",
                "Qianyu Zhuang"
            ],
            "arxiv_id": "2510.12267v1",
            "summary": "With the increasing integration of Multimodal Large Language Models (MLLMs)\ninto the medical field, comprehensive evaluation of their performance in\nvarious medical domains becomes critical. However, existing benchmarks\nprimarily assess general medical tasks, inadequately capturing performance in\nnuanced areas like the spine, which relies heavily on visual input. To address\nthis, we introduce SpineBench, a comprehensive Visual Question Answering (VQA)\nbenchmark designed for fine-grained analysis and evaluation of MLLMs in the\nspinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images,\ncovering 11 spinal diseases through two critical clinical tasks: spinal disease\ndiagnosis and spinal lesion localization, both in multiple-choice format.\nSpineBench is built by integrating and standardizing image-label pairs from\nopen-source spinal disease datasets, and samples challenging hard negative\noptions for each VQA pair based on visual similarity (similar but not the same\ndisease), simulating real-world challenging scenarios. We evaluate 12 leading\nMLLMs on SpineBench. The results reveal that these models exhibit poor\nperformance in spinal tasks, highlighting limitations of current MLLM in the\nspine domain and guiding future improvements in spinal medicine applications.\nSpineBench is publicly available at\nhttps://zhangchenghanyu.github.io/SpineBench.github.io/.",
            "headline_zh": "提出SpineBench基准以评估多模态大语言模型在脊柱病理分析中的性能",
            "intro_zh": [
                "现有基准在脊柱等依赖视觉输入的领域评估不足，无法捕捉细粒度性能。",
                "构建包含64,878个问答对的VQA基准，覆盖11种脊柱疾病，模拟真实挑战场景。",
                "评估12个领先MLLM，结果显示模型在脊柱任务中表现不佳，指导未来改进。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "脊柱病理分析",
                "视觉问答基准",
                "医学图像评估",
                "疾病诊断",
                "病灶定位"
            ],
            "_index": 79
        },
        {
            "title": "AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion",
            "authors": [
                "Xiaopeng Liu",
                "Yupei Lin",
                "Sen Zhang",
                "Xiao Wang",
                "Yukai Shi",
                "Liang Lin"
            ],
            "arxiv_id": "2510.12260v1",
            "summary": "Visible-infrared image fusion is crucial in key applications such as\nautonomous driving and nighttime surveillance. Its main goal is to integrate\nmultimodal information to produce enhanced images that are better suited for\ndownstream tasks. Although deep learning based fusion methods have made\nsignificant progress, mainstream unsupervised approaches still face serious\nchallenges in practical applications. Existing methods mostly rely on manually\ndesigned loss functions to guide the fusion process. However, these loss\nfunctions have obvious limitations. On one hand, the reference images\nconstructed by existing methods often lack details and have uneven brightness.\nOn the other hand, the widely used gradient losses focus only on gradient\nmagnitude. To address these challenges, this paper proposes an angle-based\nperception framework for spatial-sensitive image fusion (AngularFuse). At\nfirst, we design a cross-modal complementary mask module to force the network\nto learn complementary information between modalities. Then, a fine-grained\nreference image synthesis strategy is introduced. By combining Laplacian edge\nenhancement with adaptive histogram equalization, reference images with richer\ndetails and more balanced brightness are generated. Last but not least, we\nintroduce an angle-aware loss, which for the first time constrains both\ngradient magnitude and direction simultaneously in the gradient domain.\nAngularFuse ensures that the fused images preserve both texture intensity and\ncorrect edge orientation. Comprehensive experiments on the MSRS, RoadScene, and\nM3FD public datasets show that AngularFuse outperforms existing mainstream\nmethods with clear margin. Visual comparisons further confirm that our method\nproduces sharper and more detailed results in challenging scenes, demonstrating\nsuperior fusion capability.",
            "headline_zh": "提出AngularFuse框架，通过角度感知损失解决可见光-红外图像融合中的细节和亮度问题。",
            "intro_zh": [
                "核心问题：现有无监督融合方法依赖手工损失函数，导致细节缺失和亮度不均。",
                "方法要点：引入跨模态互补掩码模块和角度感知损失，同时约束梯度幅值和方向。",
                "实验或效果：在MSRS等数据集上优于主流方法，生成图像更锐利和详细。"
            ],
            "tags_zh": [
                "图像融合",
                "角度感知损失",
                "跨模态互补",
                "无监督学习",
                "梯度约束"
            ],
            "_index": 80
        },
        {
            "title": "Local Background Features Matter in Out-of-Distribution Detection",
            "authors": [
                "Jinlun Ye",
                "Zhuohao Sun",
                "Yiqiao Qiu",
                "Qiu Li",
                "Zhijun Tan",
                "Ruixuan Wang"
            ],
            "arxiv_id": "2510.12259v1",
            "summary": "Out-of-distribution (OOD) detection is crucial when deploying deep neural\nnetworks in the real world to ensure the reliability and safety of their\napplications. One main challenge in OOD detection is that neural network models\noften produce overconfident predictions on OOD data. While some methods using\nauxiliary OOD datasets or generating fake OOD images have shown promising OOD\ndetection performance, they are limited by the high costs of data collection\nand training. In this study, we propose a novel and effective OOD detection\nmethod that utilizes local background features as fake OOD features for model\ntraining. Inspired by the observation that OOD images generally share similar\nbackground regions with ID images, the background features are extracted from\nID images as simulated OOD visual representations during training based on the\nlocal invariance of convolution. Through being optimized to reduce the\n$L_2$-norm of these background features, the neural networks are able to\nalleviate the overconfidence issue on OOD data. Extensive experiments on\nmultiple standard OOD detection benchmarks confirm the effectiveness of our\nmethod and its wide combinatorial compatibility with existing post-hoc methods,\nwith new state-of-the-art performance achieved from our method.",
            "headline_zh": "提出利用局部背景特征作为伪OOD特征以缓解OOD检测中的过自信问题",
            "intro_zh": [
                "核心问题：神经网络在OOD数据上产生过自信预测，影响部署可靠性。",
                "方法要点：从ID图像提取局部背景特征作为伪OOD特征，通过优化降低其L2范数。",
                "实验或效果：在多个标准基准上验证有效性，与现有方法兼容并达到新SOTA。"
            ],
            "tags_zh": [
                "OOD检测",
                "背景特征",
                "过自信缓解",
                "卷积局部不变性",
                "伪OOD特征",
                "模型训练优化"
            ],
            "_index": 81
        },
        {
            "title": "Multiplicative Loss for Enhancing Semantic Segmentation in Medical and Cellular Images",
            "authors": [
                "Yuto Yokoi",
                "Kazuhiro Hotta"
            ],
            "arxiv_id": "2510.12258v1",
            "summary": "We propose two novel loss functions, Multiplicative Loss and\nConfidence-Adaptive Multiplicative Loss, for semantic segmentation in medical\nand cellular images. Although Cross Entropy and Dice Loss are widely used,\ntheir additive combination is sensitive to hyperparameters and often performs\nsuboptimally, especially with limited data. Medical images suffer from data\nscarcity due to privacy, ethics, and costly annotations, requiring robust and\nefficient training objectives. Our Multiplicative Loss combines Cross Entropy\nand Dice losses multiplicatively, dynamically modulating gradients based on\nprediction confidence. This reduces penalties for confident correct predictions\nand amplifies gradients for incorrect overconfident ones, stabilizing\noptimization. Building on this, Confidence-Adaptive Multiplicative Loss applies\na confidence-driven exponential scaling inspired by Focal Loss, integrating\npredicted probabilities and Dice coefficients to emphasize difficult samples.\nThis enhances learning under extreme data scarcity by strengthening gradients\nwhen confidence is low. Experiments on cellular and medical segmentation\nbenchmarks show our framework consistently outperforms tuned additive and\nexisting loss functions, offering a simple, effective, and hyperparameter-free\nmechanism for robust segmentation under challenging data limitations.",
            "headline_zh": "提出乘法损失函数以增强医学和细胞图像语义分割的鲁棒性",
            "intro_zh": [
                "医学图像数据稀缺，传统损失函数超参数敏感且性能不佳",
                "乘法损失结合交叉熵和Dice损失，动态调整梯度以稳定优化",
                "实验表明在数据稀缺场景下优于现有损失函数，无需超参数调优"
            ],
            "tags_zh": [
                "语义分割",
                "损失函数",
                "医学图像",
                "细胞图像",
                "数据稀缺",
                "梯度调制"
            ],
            "_index": 82
        },
        {
            "title": "Vectorized Video Representation with Easy Editing via Hierarchical Spatio-Temporally Consistent Proxy Embedding",
            "authors": [
                "Ye Chen",
                "Liming Tan",
                "Yupeng Zhu",
                "Yuanbin Wang",
                "Bingbing Ni"
            ],
            "arxiv_id": "2510.12256v1",
            "summary": "Current video representations heavily rely on unstable and over-grained\npriors for motion and appearance modelling, \\emph{i.e.}, pixel-level matching\nand tracking. A tracking error of just a few pixels would lead to the collapse\nof the visual object representation, not to mention occlusions and large motion\nfrequently occurring in videos. To overcome the above mentioned vulnerability,\nthis work proposes spatio-temporally consistent proxy nodes to represent\ndynamically changing objects/scenes in the video. On the one hand, the\nhierarchical proxy nodes have the ability to stably express the multi-scale\nstructure of visual objects, so they are not affected by accumulated tracking\nerror, long-term motion, occlusion, and viewpoint variation. On the other hand,\nthe dynamic representation update mechanism of the proxy nodes adequately\nleverages spatio-temporal priors of the video to mitigate the impact of\ninaccurate trackers, thereby effectively handling drastic changes in scenes and\nobjects. Additionally, the decoupled encoding manner of the shape and texture\nrepresentations across different visual objects in the video facilitates\ncontrollable and fine-grained appearance editing capability. Extensive\nexperiments demonstrate that the proposed representation achieves high video\nreconstruction accuracy with fewer parameters and supports complex video\nprocessing tasks, including video in-painting and keyframe-based temporally\nconsistent video editing.",
            "headline_zh": "提出分层时空一致代理嵌入以实现稳定视频表示与易编辑",
            "intro_zh": [
                "问题：现有视频表示依赖像素级跟踪，易受跟踪误差、遮挡和大运动影响。",
                "方法：使用分层代理节点稳定表达多尺度对象，并动态更新以利用时空先验。",
                "效果：实验显示高重建精度、少参数，支持视频修复和关键帧编辑任务。"
            ],
            "tags_zh": [
                "视频表示学习",
                "时空一致性",
                "代理节点",
                "视频编辑",
                "分层结构"
            ],
            "_index": 83
        },
        {
            "title": "Ivan-ISTD: Rethinking Cross-domain Heteroscedastic Noise Perturbations in Infrared Small Target Detection",
            "authors": [
                "Yuehui Li",
                "Yahao Lu",
                "Haoyuan Wu",
                "Sen Zhang",
                "Liang Lin",
                "Yukai Shi"
            ],
            "arxiv_id": "2510.12241v1",
            "summary": "In the multimedia domain, Infrared Small Target Detection (ISTD) plays a\nimportant role in drone-based multi-modality sensing. To address the dual\nchallenges of cross-domain shift and heteroscedastic noise perturbations in\nISTD, we propose a doubly wavelet-guided Invariance learning\nframework(Ivan-ISTD). In the first stage, we generate training samples aligned\nwith the target domain using Wavelet-guided Cross-domain Synthesis. This\nwavelet-guided alignment machine accurately separates the target background\nthrough multi-frequency wavelet filtering. In the second stage, we introduce\nReal-domain Noise Invariance Learning, which extracts real noise\ncharacteristics from the target domain to build a dynamic noise library. The\nmodel learns noise invariance through self-supervised loss, thereby overcoming\nthe limitations of distribution bias in traditional artificial noise modeling.\nFinally, we create the Dynamic-ISTD Benchmark, a cross-domain dynamic\ndegradation dataset that simulates the distribution shifts encountered in\nreal-world applications. Additionally, we validate the versatility of our\nmethod using other real-world datasets. Experimental results demonstrate that\nour approach outperforms existing state-of-the-art methods in terms of many\nquantitative metrics. In particular, Ivan-ISTD demonstrates excellent\nrobustness in cross-domain scenarios. The code for this work can be found at:\nhttps://github.com/nanjin1/Ivan-ISTD.",
            "headline_zh": "提出Ivan-ISTD框架以解决红外小目标检测中的跨域偏移和异方差噪声扰动问题",
            "intro_zh": [
                "核心问题：红外小目标检测面临跨域分布偏移和异方差噪声扰动双重挑战",
                "方法要点：采用小波引导的跨域合成和真实噪声不变性学习，构建动态噪声库",
                "实验或效果：在动态ISTD基准上优于现有方法，展现跨域场景下的强鲁棒性"
            ],
            "tags_zh": [
                "红外小目标检测",
                "跨域学习",
                "小波变换",
                "噪声不变性学习",
                "动态数据集"
            ],
            "_index": 84
        },
        {
            "title": "BIGFix: Bidirectional Image Generation with Token Fixing",
            "authors": [
                "Victor Besnier",
                "David Hurych",
                "Andrei Bursuc",
                "Eduardo Valle"
            ],
            "arxiv_id": "2510.12231v1",
            "summary": "Recent advances in image and video generation have raised significant\ninterest from both academia and industry. A key challenge in this field is\nimproving inference efficiency, as model size and the number of inference steps\ndirectly impact the commercial viability of generative models while also posing\nfundamental scientific challenges. A promising direction involves combining\nauto-regressive sequential token modeling with multi-token prediction per step,\nreducing inference time by up to an order of magnitude. However, predicting\nmultiple tokens in parallel can introduce structural inconsistencies due to\ntoken incompatibilities, as capturing complex joint dependencies during\ntraining remains challenging. Traditionally, once tokens are sampled, there is\nno mechanism to backtrack and refine erroneous predictions. We propose a method\nfor self-correcting image generation by iteratively refining sampled tokens. We\nachieve this with a novel training scheme that injects random tokens in the\ncontext, improving robustness and enabling token fixing during sampling. Our\nmethod preserves the efficiency benefits of parallel token prediction while\nsignificantly enhancing generation quality. We evaluate our approach on image\ngeneration using the ImageNet-256 and CIFAR-10 datasets, as well as on video\ngeneration with UCF-101 and NuScenes, demonstrating substantial improvements\nacross both modalities.",
            "headline_zh": "提出BIGFix方法，通过迭代修复令牌解决并行令牌预测中的结构不一致问题。",
            "intro_zh": [
                "核心问题：并行令牌预测导致结构不一致，缺乏回溯机制。",
                "方法要点：使用新颖训练方案注入随机令牌，实现迭代令牌修复。",
                "实验或效果：在图像和视频生成数据集上显著提升生成质量。"
            ],
            "tags_zh": [
                "图像生成",
                "视频生成",
                "并行令牌预测",
                "令牌修复",
                "推理效率"
            ],
            "_index": 85
        },
        {
            "title": "HoneyBee: Data Recipes for Vision-Language Reasoners",
            "authors": [
                "Hritik Bansal",
                "Devandra Singh Sachan",
                "Kai-Wei Chang",
                "Aditya Grover",
                "Gargi Ghosh",
                "Wen-tau Yih",
                "Ramakanth Pasunuru"
            ],
            "arxiv_id": "2510.12225v1",
            "summary": "Recent advances in vision-language models (VLMs) have made them highly\neffective at reasoning tasks. However, the principles underlying the\nconstruction of performant VL reasoning training datasets remain poorly\nunderstood. In this work, we introduce several data curation approaches and\nstudy their impacts on VL reasoning capabilities by carefully controlling\ntraining and evaluation setups. We analyze the effects of context (image and\nquestion pair) sources, implement targeted data interventions, and explore\nscaling up images, questions, and chain-of-thought (CoT) solutions. Our\nfindings reveal that (a) context source strategies significantly affect VLM\nperformance, (b) interventions such as auxiliary signals from image captions\nand the inclusion of text-only reasoning yield substantial gains, and (c)\nscaling all data dimensions (e.g., unique questions per image and unique CoTs\nper image-question pair) consistently improves reasoning capability. Motivated\nby these insights, we introduce HoneyBee, a large-scale, high-quality CoT\nreasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs\ntrained with HoneyBee outperform state-of-the-art models across model sizes.\nFor instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA\nmodel and the base model by 7.8% and 24.8%, respectively, on MathVerse.\nFurthermore, we propose a test-time scaling strategy that reduces decoding cost\nby 73% without sacrificing accuracy. Overall, this work presents improved\nstrategies for VL reasoning dataset curation research.",
            "headline_zh": "提出HoneyBee数据集以提升视觉语言模型的推理能力",
            "intro_zh": [
                "核心问题：视觉语言推理训练数据集构建原则不明确，影响模型性能。",
                "方法要点：通过数据干预和规模扩展，优化图像-问题对和链式思维解决方案。",
                "实验或效果：HoneyBee数据集训练模型在MathVerse等任务上显著超越现有方法。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "推理数据集",
                "链式思维",
                "数据干预",
                "模型缩放"
            ],
            "_index": 86
        },
        {
            "title": "DIANet: A Phase-Aware Dual-Stream Network for Micro-Expression Recognition via Dynamic Images",
            "authors": [
                "Vu Tram Anh Khuong",
                "Luu Tu Nguyen",
                "Thi Bich Phuong Man",
                "Thanh Ha Le",
                "Thi Duyen Ngo"
            ],
            "arxiv_id": "2510.12219v1",
            "summary": "Micro-expressions are brief, involuntary facial movements that typically last\nless than half a second and often reveal genuine emotions. Accurately\nrecognizing these subtle expressions is critical for applications in\npsychology, security, and behavioral analysis. However, micro-expression\nrecognition (MER) remains a challenging task due to the subtle and transient\nnature of facial cues and the limited availability of annotated data. While\ndynamic image (DI) representations have been introduced to summarize temporal\nmotion into a single frame, conventional DI-based methods often overlook the\ndistinct characteristics of different temporal phases within a\nmicro-expression. To address this issue, this paper proposes a novel\ndual-stream framework, DIANet, which leverages phase-aware dynamic images - one\nencoding the onset-to-apex phase and the other capturing the apex-to-offset\nphase. Each stream is processed by a dedicated convolutional neural network,\nand a cross-attention fusion module is employed to adaptively integrate\nfeatures from both streams based on their contextual relevance. Extensive\nexperiments conducted on three benchmark MER datasets (CASME-II, SAMM, and\nMMEW) demonstrate that the proposed method consistently outperforms\nconventional single-phase DI-based approaches. The results highlight the\nimportance of modeling temporal phase information explicitly and suggest a\npromising direction for advancing MER.",
            "headline_zh": "提出DIANet双流网络，通过相位感知动态图像解决微表情识别中忽略不同时间相位的问题。",
            "intro_zh": [
                "核心问题：微表情识别因表情短暂、数据稀缺及传统方法忽略不同时间相位特征而困难。",
                "方法要点：使用双流网络分别处理起始到顶点和顶点到结束相位，并采用交叉注意力融合模块。",
                "实验或效果：在多个基准数据集上优于单相位方法，验证了建模时间相位的重要性。"
            ],
            "tags_zh": [
                "微表情识别",
                "动态图像",
                "双流网络",
                "相位感知",
                "交叉注意力",
                "时间建模"
            ],
            "_index": 87
        },
        {
            "title": "Learning Social Navigation from Positive and Negative Demonstrations and Rule-Based Specifications",
            "authors": [
                "Chanwoo Kim",
                "Jihwan Yoon",
                "Hyeonseong Kim",
                "Taemoon Jeong",
                "Changwoo Yoo",
                "Seungbeen Lee",
                "Soohwan Byeon",
                "Hoon Chung",
                "Matthew Pan",
                "Jean Oh",
                "Kyungjae Lee",
                "Sungjoon Choi"
            ],
            "arxiv_id": "2510.12215v1",
            "summary": "Mobile robot navigation in dynamic human environments requires policies that\nbalance adaptability to diverse behaviors with compliance to safety\nconstraints. We hypothesize that integrating data-driven rewards with\nrule-based objectives enables navigation policies to achieve a more effective\nbalance of adaptability and safety. To this end, we develop a framework that\nlearns a density-based reward from positive and negative demonstrations and\naugments it with rule-based objectives for obstacle avoidance and goal\nreaching. A sampling-based lookahead controller produces supervisory actions\nthat are both safe and adaptive, which are subsequently distilled into a\ncompact student policy suitable for real-time operation with uncertainty\nestimates. Experiments in synthetic and elevator co-boarding simulations show\nconsistent gains in success rate and time efficiency over baselines, and\nreal-world demonstrations with human participants confirm the practicality of\ndeployment. A video illustrating this work can be found on our project page\nhttps://chanwookim971024.github.io/PioneeR/.",
            "headline_zh": "提出结合数据驱动奖励与规则目标的框架，以提升移动机器人在动态人类环境中的导航性能。",
            "intro_zh": [
                "核心问题：移动机器人在动态人类环境中需平衡适应性与安全性。",
                "方法要点：从正负演示学习奖励，结合规则目标，蒸馏为实时策略。",
                "实验效果：在仿真和真实世界测试中，成功率和时间效率优于基线。"
            ],
            "tags_zh": [
                "社会导航",
                "奖励学习",
                "规则集成",
                "策略蒸馏",
                "动态环境",
                "实时控制"
            ],
            "_index": 88
        },
        {
            "title": "The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data",
            "authors": [
                "Muammer Bay",
                "Timo von Marcard",
                "Dren Fazlija"
            ],
            "arxiv_id": "2510.12208v1",
            "summary": "Recent advances in generative AI, particularly in computer vision (CV), offer\nnew opportunities to optimize workflows across industries, including logistics\nand manufacturing. However, many AI applications are limited by a lack of\nexpertise and resources, which forces a reliance on general-purpose models.\nSuccess with these models often requires domain-specific data for fine-tuning,\nwhich can be costly and inefficient. Thus, using synthetic data for fine-tuning\nis a popular, cost-effective alternative to gathering real-world data. This\nwork investigates the impact of synthetic data on the performance of object\ndetection models, compared to models trained on real-world data only,\nspecifically within the domain of warehouse logistics. To this end, we examined\nthe impact of synthetic data generated using the NVIDIA Omniverse Replicator\ntool on the effectiveness of object detection models in real-world scenarios.\nIt comprises experiments focused on pallet detection in a warehouse setting,\nutilizing both real and various synthetic dataset generation strategies. Our\nfindings provide valuable insights into the practical applications of synthetic\nimage data in computer vision, suggesting that a balanced integration of\nsynthetic and real data can lead to robust and efficient object detection\nmodels.",
            "headline_zh": "比较合成与真实数据对仓库物流中托盘检测模型性能的影响",
            "intro_zh": [
                "核心问题：AI应用因缺乏领域数据而依赖通用模型，导致成本高和效率低。",
                "方法要点：使用NVIDIA Omniverse Replicator生成合成数据，用于对象检测模型微调。",
                "实验或效果：在仓库物流场景中，合成与真实数据结合可提升模型鲁棒性和效率。"
            ],
            "tags_zh": [
                "合成数据",
                "对象检测",
                "仓库物流",
                "模型微调",
                "NVIDIA Omniverse"
            ],
            "_index": 89
        },
        {
            "title": "Controllable Collision Scenario Generation via Collision Pattern Prediction",
            "authors": [
                "Pin-Lun Chen",
                "Chi-Hsi Kung",
                "Che-Han Chang",
                "Wei-Chen Chiu",
                "Yi-Ting Chen"
            ],
            "arxiv_id": "2510.12206v1",
            "summary": "Evaluating the safety of autonomous vehicles (AVs) requires diverse,\nsafety-critical scenarios, with collisions being especially important yet rare\nand unsafe to collect in the real world. Therefore, the community has been\nfocusing on generating safety-critical scenarios in simulation. However,\ncontrolling attributes such as collision type and time-to-accident (TTA)\nremains challenging. We introduce a new task called controllable collision\nscenario generation, where the goal is to produce trajectories that realize a\nuser-specified collision type and TTA, to investigate the feasibility of\nautomatically generating desired collision scenarios. To support this task, we\npresent COLLIDE, a large-scale collision scenario dataset constructed by\ntransforming real-world driving logs into diverse collisions, balanced across\nfive representative collision types and different TTA intervals. We propose a\nframework that predicts Collision Pattern, a compact and interpretable\nrepresentation that captures the spatial configuration of the ego and the\nadversarial vehicles at impact, before rolling out full adversarial\ntrajectories. Experiments show that our approach outperforms strong baselines\nin both collision rate and controllability. Furthermore, generated scenarios\nconsistently induce higher planner failure rates, revealing limitations of\nexisting planners. We demonstrate that these scenarios fine-tune planners for\nrobustness improvements, contributing to safer AV deployment in different\ncollision scenarios.",
            "headline_zh": "提出可控碰撞场景生成框架，通过预测碰撞模式实现指定碰撞类型与时间",
            "intro_zh": [
                "核心问题：自动驾驶安全评估需多样碰撞场景，但真实收集困难且模拟中控制属性如碰撞类型和时间具挑战性",
                "方法要点：构建COLLIDE数据集，预测碰撞模式以紧凑表示空间配置，再生成完整对抗轨迹",
                "实验或效果：方法在碰撞率和可控性上优于基线，生成场景提高规划器失败率并增强其鲁棒性"
            ],
            "tags_zh": [
                "自动驾驶安全",
                "碰撞场景生成",
                "可控模拟",
                "碰撞模式预测",
                "规划器评估"
            ],
            "_index": 90
        },
        {
            "title": "Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos",
            "authors": [
                "Shingo Yokoi",
                "Kento Sasaki",
                "Yu Yamaguchi"
            ],
            "arxiv_id": "2510.12190v1",
            "summary": "Recent advances in end-to-end (E2E) autonomous driving have been enabled by\ntraining on diverse large-scale driving datasets, yet autonomous driving models\nstill struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark\ntargets this gap by encouraging hazard understanding beyond closed taxonomies,\nand the 2COOOL challenge extends it to generating human-interpretable incident\nreports. We present a hierarchical reasoning framework for incident report\ngeneration from dashcam videos that integrates frame-level captioning, incident\nframe detection, and fine-grained reasoning within vision-language models\n(VLMs). We further improve factual accuracy and readability through model\nensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL\nopen leaderboard, our method ranks 2nd among 29 teams and achieves the best\nCIDEr-D score, producing accurate and coherent incident narratives. These\nresults indicate that hierarchical reasoning with VLMs is a promising direction\nfor accident analysis and for broader understanding of safety-critical traffic\nevents. The implementation and code are available at\nhttps://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.",
            "headline_zh": "提出分层推理框架，用于从行车记录仪视频生成事故报告。",
            "intro_zh": [
                "核心问题：自动驾驶模型在分布外场景中理解危险事件困难。",
                "方法要点：集成帧级描述、事故帧检测和细粒度推理于视觉语言模型。",
                "实验或效果：在2COOOL挑战中排名第2，CIDEr-D得分最高。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "分层推理",
                "事故报告生成",
                "行车记录仪视频",
                "模型集成"
            ],
            "_index": 91
        },
        {
            "title": "CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs",
            "authors": [
                "Jiwan Kim",
                "Kibum Kim",
                "Sangwoo Seo",
                "Chanyoung Park"
            ],
            "arxiv_id": "2510.12184v1",
            "summary": "Recently, efficient Multimodal Large Language Models (MLLMs) have gained\nsignificant attention as a solution to their high computational complexity,\nmaking them more practical for real-world applications. In this regard, the\nknowledge distillation (KD) approach has emerged as a promising alternative,\nwhich transfers the rich visual and linguistic knowledge from a larger model\n(teacher) to a smaller model (student). However, we observe that existing KD\nmethods struggle to effectively distill the teacher MLLM's rich visual\nperception abilities to the student, a challenge that has been largely\noverlooked in previous studies. Through a systematic analysis, we identify\nvisual attention misalignment between student and teacher as the main cause of\nthis issue. Based on this insight, we propose CompoDistill, a novel KD\nframework that explicitly aligns the student's visual attention with that of\nthe teacher to enhance the student's visual perception abilities. Our extensive\nexperiments show that CompoDistill significantly improves performance on\ncompositional reasoning tasks that require visual perception abilities while\nmaintaining strong performance on visual question answering tasks, as done in\nexisting studies. Furthermore, CompoDistill demonstrates effectiveness with a\nmore advanced backbone, highlighting its generalizability.",
            "headline_zh": "提出CompoDistill框架，通过注意力对齐解决多模态大模型中视觉感知蒸馏问题",
            "intro_zh": [
                "现有知识蒸馏方法在多模态大模型中难以有效传递教师模型的视觉感知能力",
                "通过视觉注意力对齐机制，显式优化学生模型的视觉注意力分布",
                "实验显示在组合推理任务中性能显著提升，并保持视觉问答任务表现"
            ],
            "tags_zh": [
                "多模态大模型",
                "知识蒸馏",
                "视觉注意力对齐",
                "组合推理",
                "视觉感知",
                "模型压缩"
            ],
            "_index": 92
        },
        {
            "title": "BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation",
            "authors": [
                "Youngju Yoo",
                "Seho Kim",
                "Changick Kim"
            ],
            "arxiv_id": "2510.12182v1",
            "summary": "3D instance segmentation is crucial for understanding complex 3D\nenvironments, yet fully supervised methods require dense point-level\nannotations, resulting in substantial annotation costs and labor overhead. To\nmitigate this, box-level annotations have been explored as a weaker but more\nscalable form of supervision. However, box annotations inherently introduce\nambiguity in overlapping regions, making accurate point-to-instance assignment\nchallenging. Recent methods address this ambiguity by generating pseudo-masks\nthrough training a dedicated pseudo-labeler in an additional training stage.\nHowever, such two-stage pipelines often increase overall training time and\ncomplexity, hinder end-to-end optimization. To overcome these challenges, we\npropose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance\nsegmentation. BEEP3D adopts a student-teacher framework, where the teacher\nmodel serves as a pseudo-labeler and is updated by the student model via an\nExponential Moving Average. To better guide the teacher model to generate\nprecise pseudo-masks, we introduce an instance center-based query refinement\nthat enhances position query localization and leverages features near instance\ncenters. Additionally, we design two novel losses-query consistency loss and\nmasked feature consistency loss-to align semantic and geometric signals between\npredictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS\ndatasets demonstrate that BEEP3D achieves competitive or superior performance\ncompared to state-of-the-art weakly supervised methods while remaining\ncomputationally efficient.",
            "headline_zh": "提出BEEP3D以解决3D实例分割中框监督的模糊性问题",
            "intro_zh": [
                "核心问题：框级监督在重叠区域引入模糊性，阻碍点对实例的准确分配",
                "方法要点：采用师生框架，通过查询精炼和一致性损失生成精确伪掩码",
                "实验或效果：在ScanNetV2和S3DIS数据集上性能竞争或优于先进弱监督方法"
            ],
            "tags_zh": [
                "3D实例分割",
                "框监督学习",
                "师生框架",
                "伪掩码生成",
                "弱监督方法"
            ],
            "_index": 93
        },
        {
            "title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering",
            "authors": [
                "Yusen Xie",
                "Zhenmin Huang",
                "Jianhao Jiao",
                "Dimitrios Kanoulas",
                "Jun Ma"
            ],
            "arxiv_id": "2510.12174v1",
            "summary": "In this paper, we propose UniGS, a unified map representation and\ndifferentiable framework for high-fidelity multimodal 3D reconstruction based\non 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated\nrasterization pipeline capable of rendering photo-realistic RGB images,\ngeometrically accurate depth maps, consistent surface normals, and semantic\nlogits simultaneously. We redesign the rasterization to render depth via\ndifferentiable ray-ellipsoid intersection rather than using Gaussian centers,\nenabling effective optimization of rotation and scale attribute through\nanalytic depth gradients. Furthermore, we derive the analytic gradient\nformulation for surface normal rendering, ensuring geometric consistency among\nreconstructed 3D scenes. To improve computational and storage efficiency, we\nintroduce a learnable attribute that enables differentiable pruning of\nGaussians with minimal contribution during training. Quantitative and\nqualitative experiments demonstrate state-of-the-art reconstruction accuracy\nacross all modalities, validating the efficacy of our geometry-aware paradigm.\nSource code and multimodal viewer will be available on GitHub.",
            "headline_zh": "提出UniGS统一框架，实现高保真多模态3D重建，基于几何感知高斯溅射。",
            "intro_zh": [
                "核心问题：多模态3D重建中几何一致性与渲染效率的挑战。",
                "方法要点：重新设计光栅化，使用可微光线-椭球相交优化深度和表面法线。",
                "实验或效果：定量定性实验显示在所有模态上达到最先进重建精度。"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "多模态渲染",
                "几何一致性",
                "可微光栅化",
                "CUDA加速"
            ],
            "_index": 94
        },
        {
            "title": "Hybrid Terrain-Aware Path Planning: Integrating VD--RRT\\(^{*}\\) Exploration and VD--D\\(^{*}\\) Lite Repair",
            "authors": [
                "Akshay Naik",
                "William R. Norris",
                "Dustin Nottage",
                "Ahmet Soylemezoglu"
            ],
            "arxiv_id": "2510.12169v1",
            "summary": "Autonomous ground vehicles operating off-road must plan curvature-feasible\npaths while accounting for spatially varying soil strength and slope hazards in\nreal time. We present a continuous state--cost metric that combines a Bekker\npressure--sinkage model with elevation-derived slope and attitude penalties.\nThe resulting terrain cost field is analytic, bounded, and monotonic in soil\nmodulus and slope, ensuring well-posed discretization and stable updates under\nsensor noise. This metric is evaluated on a lattice with exact steering\nprimitives: Dubins and Reeds--Shepp motions for differential drive and\ntime-parameterized bicycle arcs for Ackermann steering. Global exploration is\nperformed using Vehicle-Dynamics RRT\\(^{*}\\), while local repair is managed by\nVehicle-Dynamics D\\(^{*}\\) Lite, enabling millisecond-scale replanning without\nheuristic smoothing. By separating the terrain--vehicle model from the planner,\nthe framework provides a reusable basis for deterministic, sampling-based, or\nlearning-driven planning in deformable terrain. Hardware trials on an off-road\nplatform demonstrate real-time navigation across soft soil and slope\ntransitions, supporting reliable autonomy in unstructured environments.",
            "headline_zh": "提出混合地形感知路径规划框架，集成VD-RRT*探索与VD-D* Lite修复，用于越野自主车辆实时导航。",
            "intro_zh": [
                "核心问题：越野自主车辆需实时规划曲率可行路径，同时处理土壤强度和坡度变化。",
                "方法要点：结合Bekker压力-沉降模型与坡度惩罚，构建连续状态-成本度量，支持快速重规划。",
                "实验或效果：硬件试验显示在软土和坡度变化中实现实时可靠导航。"
            ],
            "tags_zh": [
                "路径规划",
                "地形感知",
                "自主车辆",
                "实时导航",
                "越野环境",
                "车辆动力学"
            ],
            "_index": 95
        },
        {
            "title": "State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding",
            "authors": [
                "Jiahuan Zhou",
                "Kai Zhu",
                "Zhenyu Cui",
                "Zichen Liu",
                "Xu Zou",
                "Gang Hua"
            ],
            "arxiv_id": "2510.12160v1",
            "summary": "Recently, pre-trained state space models have shown great potential for video\nclassification, which sequentially compresses visual tokens in videos with\nlinear complexity, thereby improving the processing efficiency of video data\nwhile maintaining high performance. To apply powerful pre-trained models to\ndownstream tasks, prompt learning is proposed to achieve efficient downstream\ntask adaptation with only a small number of fine-tuned parameters. However, the\nsequentially compressed visual prompt tokens fail to capture the spatial and\ntemporal contextual information in the video, thus limiting the effective\npropagation of spatial information within a video frame and temporal\ninformation between frames in the state compression model and the extraction of\ndiscriminative information. To tackle the above issue, we proposed a State\nSpace Prompting (SSP) method for video understanding, which combines\nintra-frame and inter-frame prompts to aggregate and propagate key\nspatiotemporal information in the video. Specifically, an Intra-Frame Gathering\n(IFG) module is designed to aggregate spatial key information within each\nframe. Besides, an Inter-Frame Spreading (IFS) module is designed to spread\ndiscriminative spatio-temporal information across different frames. By\nadaptively balancing and compressing key spatio-temporal information within and\nbetween frames, our SSP effectively propagates discriminative information in\nvideos in a complementary manner. Extensive experiments on four video benchmark\ndatasets verify that our SSP significantly outperforms existing SOTA methods by\n2.76% on average while reducing the overhead of fine-tuning parameters.",
            "headline_zh": "提出状态空间提示方法以解决视频理解中时空信息传播不足的问题",
            "intro_zh": [
                "问题：预训练状态空间模型压缩视觉提示时，难以捕获视频的时空上下文信息。",
                "方法：设计帧内聚集和帧间扩散模块，自适应平衡和压缩关键时空信息。",
                "效果：在四个基准数据集上平均提升2.76%，同时减少微调参数开销。"
            ],
            "tags_zh": [
                "视频理解",
                "状态空间模型",
                "提示学习",
                "时空信息传播",
                "参数高效微调"
            ],
            "_index": 96
        },
        {
            "title": "DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation",
            "authors": [
                "Ziyuan Gao",
                "Philippe Morel"
            ],
            "arxiv_id": "2510.12159v1",
            "summary": "One-shot medical image segmentation faces fundamental challenges in prototype\nrepresentation due to limited annotated data and significant anatomical\nvariability across patients. Traditional prototype-based methods rely on\ndeterministic averaging of support features, creating brittle representations\nthat fail to capture intra-class diversity essential for robust generalization.\nThis work introduces Diffusion Prototype Learning (DPL), a novel framework that\nreformulates prototype construction through diffusion-based feature space\nexploration. DPL models one-shot prototypes as learnable probability\ndistributions, enabling controlled generation of diverse yet semantically\ncoherent prototype variants from minimal labeled data. The framework operates\nthrough three core innovations: (1) a diffusion-based prototype enhancement\nmodule that transforms single support prototypes into diverse variant sets via\nforward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism\nthat leverages geometric properties derived from prototype feature statistics,\nand (3) a conservative fusion strategy that preserves prototype fidelity while\nmaximizing representational diversity. DPL ensures training-inference\nconsistency by using the same diffusion enhancement and fusion pipeline in both\nphases. This process generates enhanced prototypes that serve as the final\nrepresentations for similarity calculations, while the diffusion process itself\nacts as a regularizer. Extensive experiments on abdominal MRI and CT datasets\ndemonstrate significant improvements respectively, establishing new\nstate-of-the-art performance in one-shot medical image segmentation.",
            "headline_zh": "提出DPL框架以解决少样本医学图像分割中的原型表示脆弱性问题",
            "intro_zh": [
                "核心问题：少样本医学分割中，传统原型方法因数据有限和患者解剖变异导致表示脆弱。",
                "方法要点：使用扩散模型将原型建模为概率分布，生成多样且语义一致的变体。",
                "实验或效果：在腹部MRI和CT数据集上实现SOTA性能，显著提升分割精度。"
            ],
            "tags_zh": [
                "少样本医学分割",
                "扩散模型",
                "原型学习",
                "空间条件机制",
                "保守融合策略"
            ],
            "_index": 97
        },
        {
            "title": "Class-aware Domain Knowledge Fusion and Fission for Continual Test-Time Adaptation",
            "authors": [
                "Jiahuan Zhou",
                "Chao Zhu",
                "Zhenyu Cui",
                "Zichen Liu",
                "Xu Zou",
                "Gang Hua"
            ],
            "arxiv_id": "2510.12150v1",
            "summary": "Continual Test-Time Adaptation (CTTA) aims to quickly fine-tune the model\nduring the test phase so that it can adapt to multiple unknown downstream\ndomain distributions without pre-acquiring downstream domain data. To this end,\nexisting advanced CTTA methods mainly reduce the catastrophic forgetting of\nhistorical knowledge caused by irregular switching of downstream domain data by\nrestoring the initial model or reusing historical models. However, these\nmethods are usually accompanied by serious insufficient learning of new\nknowledge and interference from potentially harmful historical knowledge,\nresulting in severe performance degradation. To this end, we propose a\nclass-aware domain Knowledge Fusion and Fission method for continual test-time\nadaptation, called KFF, which adaptively expands and merges class-aware domain\nknowledge in old and new domains according to the test-time data from different\ndomains, where discriminative historical knowledge can be dynamically\naccumulated. Specifically, considering the huge domain gap within streaming\ndata, a domain Knowledge FIssion (KFI) module is designed to adaptively\nseparate new domain knowledge from a paired class-aware domain prompt pool,\nalleviating the impact of negative knowledge brought by old domains that are\ndistinct from the current domain. Besides, to avoid the cumulative computation\nand storage overheads from continuously fissioning new knowledge, a domain\nKnowledge FUsion (KFU) module is further designed to merge the fissioned new\nknowledge into the existing knowledge pool with minimal cost, where a greedy\nknowledge dynamic merging strategy is designed to improve the compatibility of\nnew and old knowledge while keeping the computational efficiency. Extensive\nexperiments on the ImageNet-C dataset verify the effectiveness of our proposed\nmethod against other methods.",
            "headline_zh": "提出类感知域知识融合与分裂方法以解决持续测试时适应中的知识遗忘与干扰问题",
            "intro_zh": [
                "核心问题：现有方法在持续测试时适应中易导致新知识学习不足和历史知识干扰，引发性能下降",
                "方法要点：设计知识分裂模块分离新域知识，知识融合模块高效合并新旧知识，动态积累判别性知识",
                "实验或效果：在ImageNet-C数据集上验证方法有效性，优于其他方法"
            ],
            "tags_zh": [
                "持续测试时适应",
                "知识融合",
                "知识分裂",
                "类感知域提示",
                "动态知识管理",
                "ImageNet-C"
            ],
            "_index": 98
        },
        {
            "title": "MAPS: Masked Attribution-based Probing of Strategies- A computational framework to align human and model explanations",
            "authors": [
                "Sabine Muzellec",
                "Yousif Kashef Alghetaa",
                "Simon Kornblith",
                "Kohitij Kar"
            ],
            "arxiv_id": "2510.12141v1",
            "summary": "Human core object recognition depends on the selective use of visual\ninformation, but the strategies guiding these choices are difficult to measure\ndirectly. We present MAPS (Masked Attribution-based Probing of Strategies), a\nbehaviorally validated computational tool that tests whether explanations\nderived from artificial neural networks (ANNs) can also explain human vision.\nMAPS converts attribution maps into explanation-masked images (EMIs) and\ncompares image-by-image human accuracies on these minimal images with limited\npixel budgets with accuracies on the full stimuli. MAPS provides a principled\nway to evaluate and choose among competing ANN interpretability methods. In\nsilico, EMI-based behavioral similarity between models reliably recovers the\nground-truth similarity computed from their attribution maps, establishing\nwhich explanation methods best capture the model's strategy. When applied to\nhumans and macaques, MAPS identifies ANN-explanation combinations whose\nexplanations align most closely with biological vision, achieving the\nbehavioral validity of Bubble masks while requiring far fewer behavioral\ntrials. Because it needs only access to model attributions and a modest set of\nbehavioral data on the original images, MAPS avoids exhaustive psychophysics\nwhile offering a scalable tool for adjudicating explanations and linking human\nbehavior, neural activity, and model decisions under a common standard.",
            "headline_zh": "提出MAPS框架以对齐人类与模型解释，评估神经网络解释方法。",
            "intro_zh": [
                "核心问题：人类视觉策略难以直接测量，需对齐人工神经网络解释。",
                "方法要点：将归因图转换为解释掩码图像，比较人类在有限像素图像上的准确率。",
                "实验或效果：验证MAPS在模型间恢复真实相似性，并识别与生物视觉对齐的解释方法。"
            ],
            "tags_zh": [
                "视觉解释对齐",
                "归因图分析",
                "行为验证",
                "神经网络解释",
                "计算框架"
            ],
            "_index": 99
        },
        {
            "title": "FedHUG: Federated Heterogeneous Unsupervised Generalization for Remote Physiological Measurements",
            "authors": [
                "Xiao Yang",
                "Jiyao Wang"
            ],
            "arxiv_id": "2510.12132v1",
            "summary": "Remote physiological measurement gained wide attention, while it requires\ncollecting users' privacy-sensitive information, and existing contactless\nmeasurements still rely on labeled client data. This presents challenges when\nwe want to further update real-world deployed models with numerous user data\nlacking labels. To resolve these challenges, we instantiate a new protocol\ncalled Federated Unsupervised Domain Generalization (FUDG) in this work.\nSubsequently, the \\textbf{Fed}erated \\textbf{H}eterogeneous\n\\textbf{U}nsupervised \\textbf{G}eneralization (\\textbf{FedHUG}) framework is\nproposed and consists of: (1) Minimal Bias Aggregation module dynamically\nadjusts aggregation weights based on prior-driven bias evaluation to cope with\nheterogeneous non-IID features from multiple domains. (2) The Global\nDistribution-aware Learning Controller parameterizes the label distribution and\ndynamically manipulates client-specific training strategies, thereby mitigating\nthe server-client label distribution skew and long-tail issue. The proposal\nshows superior performance across state-of-the-art techniques in estimation\nwith either RGB video or mmWave radar. The code will be released.",
            "headline_zh": "提出FedHUG框架以解决联邦学习中无监督远程生理测量的异构泛化问题",
            "intro_zh": [
                "核心问题：远程生理测量依赖隐私数据，现有方法需标注数据，难以在无标签用户数据上更新模型",
                "方法要点：引入最小偏差聚合和全局分布感知学习控制器，处理异构特征和标签分布偏斜",
                "实验或效果：在RGB视频和毫米波雷达估计中优于现有技术，代码将开源"
            ],
            "tags_zh": [
                "联邦学习",
                "无监督学习",
                "远程生理测量",
                "异构数据",
                "域泛化",
                "长尾问题"
            ],
            "_index": 100
        },
        {
            "title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites",
            "authors": [
                "Zhenxin Lei",
                "Zhangwei Gao",
                "Changyao Tian",
                "Erfei Cui",
                "Guanzhou Chen",
                "Danni Yang",
                "Yuchen Duan",
                "Zhaokai Wang",
                "Wenhao Li",
                "Weiyun Wang",
                "Xiangyu Zhao",
                "Jiayi Ji",
                "Yu Qiao",
                "Wenhai Wang",
                "Gen Luo"
            ],
            "arxiv_id": "2510.12126v1",
            "summary": "Generalist visual captioning goes beyond a simple appearance description\ntask, but requires integrating a series of visual cues into a caption and\nhandling various visual domains. In this task, current open-source models\npresent a large performance gap with commercial ones, which limits various\napplications such as data synthesis. To bridge the gap, this paper proposes\nCapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for\nthe first time that, by capitalizing on open-source models, it is possible to\nachieve caption quality on par with GPT-4.1 in various domains with an 89.5%\nreduction in costs. By leveraging CapFlow as the data synthesizer, we produce\nhigh-quality visual captions from image and video domains at scale, and obtain\na generalist visual captioner via fine-tuning, namely MetaCaptioner. Through\nextensive experiments, we show that MetaCaptioner not only achieves comparable\ncaptioning capabilities with commercial models but also reaches top-tier\nmultimodal performance in the open-source community. We hope CapFlow and\nMetaCaptioner can benefit future multimodal research by providing a strong and\ncost-effective visual captioning solution.",
            "headline_zh": "提出CapFlow工作流和MetaCaptioner模型，实现低成本高质量通用视觉描述。",
            "intro_zh": [
                "当前开源视觉描述模型与商业模型性能差距大，限制数据合成等应用。",
                "引入CapFlow多智能体协作工作流，利用开源模型降低成本并提升描述质量。",
                "MetaCaptioner在实验中达到与商业模型相当性能，并在开源社区领先。"
            ],
            "tags_zh": [
                "通用视觉描述",
                "多智能体协作",
                "数据合成",
                "成本优化",
                "开源模型"
            ],
            "_index": 101
        },
        {
            "title": "Hardware-aware Coding Function Design for Compressive Single-Photon 3D Cameras",
            "authors": [
                "David Parra",
                "Felipe Gutierrez-Barragan",
                "Trevor Seets",
                "Andreas Velten"
            ],
            "arxiv_id": "2510.12123v1",
            "summary": "Single-photon cameras are becoming increasingly popular in time-of-flight 3D\nimaging because they can time-tag individual photons with extreme resolution.\nHowever, their performance is susceptible to hardware limitations, such as\nsystem bandwidth, maximum laser power, sensor data rates, and in-sensor memory\nand compute resources. Compressive histograms were recently introduced as a\nsolution to the challenge of data rates through an online in-sensor compression\nof photon timestamp data. Although compressive histograms work within limited\nin-sensor memory and computational resources, they underperform when subjected\nto real-world illumination hardware constraints. To address this, we present a\nconstrained optimization approach for designing practical coding functions for\ncompressive single-photon 3D imaging. Using gradient descent, we jointly\noptimize an illumination and coding matrix (i.e., the coding functions) that\nadheres to hardware constraints. We show through extensive simulations that our\ncoding functions consistently outperform traditional coding designs under both\nbandwidth and peak power constraints. This advantage is particularly pronounced\nin systems constrained by peak power. Finally, we show that our approach adapts\nto arbitrary parameterized impulse responses by evaluating it on a real-world\nsystem with a non-ideal impulse response function.",
            "headline_zh": "提出硬件感知编码函数设计以优化压缩单光子3D相机性能",
            "intro_zh": [
                "单光子3D相机受硬件限制影响性能，如带宽和峰值功率",
                "采用约束优化方法联合优化照明和编码矩阵，适应硬件约束",
                "仿真显示在带宽和峰值功率约束下优于传统编码设计"
            ],
            "tags_zh": [
                "单光子3D成像",
                "压缩直方图",
                "硬件约束优化",
                "编码函数设计",
                "梯度下降优化"
            ],
            "_index": 102
        },
        {
            "title": "ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation",
            "authors": [
                "Ziyuan Luo",
                "Yangyi Zhao",
                "Ka Chun Cheung",
                "Simon See",
                "Renjie Wan"
            ],
            "arxiv_id": "2510.12119v1",
            "summary": "The widespread adoption of Retrieval-Augmented Image Generation (RAIG) has\nraised significant concerns about the unauthorized use of private image\ndatasets. While these systems have shown remarkable capabilities in enhancing\ngeneration quality through reference images, protecting visual datasets from\nunauthorized use in such systems remains a challenging problem. Traditional\ndigital watermarking approaches face limitations in RAIG systems, as the\ncomplex feature extraction and recombination processes fail to preserve\nwatermark signals during generation. To address these challenges, we propose\nImageSentinel, a novel framework for protecting visual datasets in RAIG. Our\nframework synthesizes sentinel images that maintain visual consistency with the\noriginal dataset. These sentinels enable protection verification through\nrandomly generated character sequences that serve as retrieval keys. To ensure\nseamless integration, we leverage vision-language models to generate the\nsentinel images. Experimental results demonstrate that ImageSentinel\neffectively detects unauthorized dataset usage while preserving generation\nquality for authorized applications. Code is available at\nhttps://github.com/luo-ziyuan/ImageSentinel.",
            "headline_zh": "提出ImageSentinel框架以保护视觉数据集免遭未经授权的检索增强图像生成使用",
            "intro_zh": [
                "核心问题：检索增强图像生成系统未经授权使用私有图像数据集，传统数字水印方法失效",
                "方法要点：利用视觉语言模型合成哨兵图像，通过随机字符序列作为检索键实现保护验证",
                "实验或效果：实验显示ImageSentinel能有效检测未经授权使用，同时保持授权应用的生成质量"
            ],
            "tags_zh": [
                "图像数据集保护",
                "检索增强图像生成",
                "视觉语言模型",
                "哨兵图像",
                "保护验证"
            ],
            "_index": 103
        },
        {
            "title": "Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration",
            "authors": [
                "Wenjie Li",
                "Xiangyi Wang",
                "Heng Guo",
                "Guangwei Gao",
                "Zhanyu Ma"
            ],
            "arxiv_id": "2510.12114v1",
            "summary": "Old-photo face restoration poses significant challenges due to compounded\ndegradations such as breakage, fading, and severe blur. Existing pre-trained\ndiffusion-guided methods either rely on explicit degradation priors or global\nstatistical guidance, which struggle with localized artifacts or face color. We\npropose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages\npseudo-reference faces generated by a pre-trained diffusion model under weak\nguidance. These pseudo-labels exhibit structurally aligned contours and natural\ncolors, enabling region-specific restoration via staged supervision: structural\nguidance applied throughout the denoising process and color refinement in later\nsteps, aligned with the coarse-to-fine nature of diffusion. By incorporating\nface parsing maps and scratch masks, our method selectively restores breakage\nregions while avoiding identity mismatch. We further construct VintageFace, a\n300-image benchmark of real old face photos with varying degradation levels.\nSSDiff outperforms existing GAN-based and diffusion-based methods in perceptual\nquality, fidelity, and regional controllability. Code link:\nhttps://github.com/PRIS-CV/SSDiff.",
            "headline_zh": "提出自监督选择性引导扩散模型以解决老照片人脸修复中的复合退化问题",
            "intro_zh": [
                "老照片人脸修复面临破损、褪色和严重模糊等复合退化挑战",
                "利用弱引导生成伪参考人脸，通过结构引导和颜色精炼实现区域选择性修复",
                "在自建基准上，SSDiff在感知质量、保真度和区域可控性方面优于现有方法"
            ],
            "tags_zh": [
                "人脸修复",
                "扩散模型",
                "自监督学习",
                "选择性引导",
                "老照片处理",
                "图像增强"
            ],
            "_index": 104
        },
        {
            "title": "DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning",
            "authors": [
                "Jiawei Zhan",
                "Jun Liu",
                "Jinlong Peng",
                "Xiaochen Chen",
                "Bin-Bin Gao",
                "Yong Liu",
                "Chengjie Wang"
            ],
            "arxiv_id": "2510.12107v1",
            "summary": "With the excellent representation capabilities of Pre-Trained Models (PTMs),\nremarkable progress has been made in non-rehearsal Class-Incremental Learning\n(CIL) research. However, it remains an extremely challenging task due to three\nconundrums: increasingly large model complexity, non-smooth representation\nshift during incremental learning and inconsistency between stage-wise\nsub-problem optimization and global inference. In this work, we propose the\nDiscriminative Representation Learning (DRL) framework to specifically address\nthese challenges. To conduct incremental learning effectively and yet\nefficiently, the DRL's network, called Incremental Parallel Adapter (IPA)\nnetwork, is built upon a PTM and increasingly augments the model by learning a\nlightweight adapter with a small amount of parameter learning overhead in each\nincremental stage. The adapter is responsible for adapting the model to new\nclasses, it can inherit and propagate the representation capability from the\ncurrent model through parallel connection between them by a transfer gate. As a\nresult, this design guarantees a smooth representation shift between different\nincremental stages. Furthermore, to alleviate inconsistency and enable\ncomparable feature representations across incremental stages, we design the\nDecoupled Anchor Supervision (DAS). It decouples constraints of positive and\nnegative samples by respectively comparing them with the virtual anchor. This\ndecoupling promotes discriminative representation learning and aligns the\nfeature spaces learned at different stages, thereby narrowing the gap between\nstage-wise local optimization over a subset of data and global inference across\nall classes. Extensive experiments on six benchmarks reveal that our DRL\nconsistently outperforms other state-of-the-art methods throughout the entire\nCIL period while maintaining high efficiency in both training and inference\nphases.",
            "headline_zh": "提出DRL框架，通过并行适配器和解耦锚监督解决类增量学习中的模型复杂度和表示不一致问题。",
            "intro_zh": [
                "核心问题：类增量学习中模型复杂度增加、表示偏移不平滑及阶段优化与全局推断不一致。",
                "方法要点：使用增量并行适配器网络，轻量级适配器通过传输门继承表示能力，确保平滑表示转移。",
                "实验或效果：在六个基准测试中，DRL持续优于现有方法，训练和推断效率高。"
            ],
            "tags_zh": [
                "类增量学习",
                "预训练模型",
                "并行适配器",
                "解耦锚监督",
                "表示学习",
                "轻量级学习"
            ],
            "_index": 105
        },
        {
            "title": "Gaussian Semantic Field for One-shot LiDAR Global Localization",
            "authors": [
                "Pengyu Yin",
                "Shenghai Yuan",
                "Haozhi Cao",
                "Xingyu Ji",
                "Ruofei Bai",
                "Siyu Chen",
                "Lihua Xie"
            ],
            "arxiv_id": "2510.12101v1",
            "summary": "We present a one-shot LiDAR global localization algorithm featuring semantic\ndisambiguation ability based on a lightweight tri-layered scene graph. While\nlandmark semantic registration-based methods have shown promising performance\nimprovements in global localization compared with geometric-only methods,\nlandmarks can be repetitive and misleading for correspondence establishment. We\npropose to mitigate this problem by modeling semantic distributions with\ncontinuous functions learned from a population of Gaussian processes. Compared\nwith discrete semantic labels, the continuous functions capture finer-grained\ngeo-semantic information and also provide more detailed metric information for\ncorrespondence establishment. We insert this continuous function as the middle\nlayer between the object layer and the metric-semantic layer, forming a\ntri-layered 3D scene graph, serving as a light-weight yet performant backend\nfor one-shot localization. We term our global localization pipeline Outram-GSF\n(Gaussian semantic field) and conduct a wide range of experiments on publicly\navailable data sets, validating the superior performance against the current\nstate-of-the-art.",
            "headline_zh": "提出高斯语义场以解决LiDAR全局定位中的语义歧义问题",
            "intro_zh": [
                "核心问题：地标语义重复导致LiDAR全局定位对应关系建立困难",
                "方法要点：使用高斯过程学习连续语义分布，构建三层场景图",
                "实验或效果：在公开数据集上验证优于当前最优方法"
            ],
            "tags_zh": [
                "LiDAR全局定位",
                "高斯语义场",
                "三层场景图",
                "语义分布建模",
                "一次性定位"
            ],
            "_index": 106
        },
        {
            "title": "G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior",
            "authors": [
                "Junfeng Ni",
                "Yixin Chen",
                "Zhifei Yang",
                "Yu Liu",
                "Ruijie Lu",
                "Song-Chun Zhu",
                "Siyuan Huang"
            ],
            "arxiv_id": "2510.12099v1",
            "summary": "Despite recent advances in leveraging generative prior from pre-trained\ndiffusion models for 3D scene reconstruction, existing methods still face two\ncritical limitations. First, due to the lack of reliable geometric supervision,\nthey struggle to produce high-quality reconstructions even in observed regions,\nlet alone in unobserved areas. Second, they lack effective mechanisms to\nmitigate multi-view inconsistencies in the generated images, leading to severe\nshape-appearance ambiguities and degraded scene geometry. In this paper, we\nidentify accurate geometry as the fundamental prerequisite for effectively\nexploiting generative models to enhance 3D scene reconstruction. We first\npropose to leverage the prevalence of planar structures to derive accurate\nmetric-scale depth maps, providing reliable supervision in both observed and\nunobserved regions. Furthermore, we incorporate this geometry guidance\nthroughout the generative pipeline to improve visibility mask estimation, guide\nnovel view selection, and enhance multi-view consistency when inpainting with\nvideo diffusion models, resulting in accurate and consistent scene completion.\nExtensive experiments on Replica, ScanNet++, and DeepBlending show that our\nmethod consistently outperforms existing baselines in both geometry and\nappearance reconstruction, particularly for unobserved regions. Moreover, our\nmethod naturally supports single-view inputs and unposed videos, with strong\ngeneralizability in both indoor and outdoor scenarios with practical real-world\napplicability. The project page is available at\nhttps://dali-jack.github.io/g4splat-web/.",
            "headline_zh": "提出几何引导的高斯泼溅方法，利用生成先验增强3D场景重建",
            "intro_zh": [
                "现有方法缺乏可靠几何监督，导致重建质量差和多视图不一致",
                "利用平面结构获取精确深度图，并整合几何指导以改进可见性掩码和视图选择",
                "在多个数据集上实验，几何和外观重建优于基线，支持单视图和无位姿视频输入"
            ],
            "tags_zh": [
                "3D场景重建",
                "高斯泼溅",
                "生成先验",
                "几何指导",
                "多视图一致性",
                "深度估计"
            ],
            "_index": 107
        },
        {
            "title": "An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring",
            "authors": [
                "Jianping Li",
                "Dongyang Guo",
                "Wenjie Li",
                "Wei Zhao"
            ],
            "arxiv_id": "2510.12098v1",
            "summary": "Unlike general image deblurring that prioritizes perceptual quality, QR code\ndeblurring focuses on ensuring successful decoding. QR codes are characterized\nby highly structured patterns with sharp edges, a robust prior for restoration.\nYet existing deep learning methods rarely exploit these priors explicitly. To\naddress this gap, we propose the Edge-Guided Attention Block (EGAB), which\nembeds explicit edge priors into a Transformer architecture. Based on EGAB, we\ndevelop Edge-Guided Restormer (EG-Restormer), an effective network that\nsignificantly boosts the decoding rate of severely blurred QR codes. For mildly\nblurred inputs, we design the Lightweight and Efficient Network (LENet) for\nfast deblurring. We further integrate these two networks into an Adaptive\nDual-network (ADNet), which dynamically selects the suitable network based on\ninput blur severity, making it ideal for resource-constrained mobile devices.\nExtensive experiments show that our EG-Restormer and ADNet achieve\nstate-of-the-art performance with a competitive speed. Project page:\nhttps://github.com/leejianping/ADNet",
            "headline_zh": "提出自适应双网络框架，结合边缘引导Transformer和轻量网络，优化移动设备上QR码运动去模糊。",
            "intro_zh": [
                "核心问题：现有深度学习方法未充分利用QR码结构化边缘先验，影响解码成功率。",
                "方法要点：设计边缘引导注意力块和自适应双网络，根据模糊程度动态选择模型。",
                "实验或效果：在严重模糊QR码上提升解码率，速度快，适合移动设备部署。"
            ],
            "tags_zh": [
                "QR码去模糊",
                "边缘引导注意力",
                "自适应网络",
                "Transformer架构",
                "移动设备优化"
            ],
            "_index": 108
        },
        {
            "title": "IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation",
            "authors": [
                "Wenxu Zhou",
                "Kaixuan Nie",
                "Hang Du",
                "Dong Yin",
                "Wei Huang",
                "Siqiang Guo",
                "Xiaobo Zhang",
                "Pengbo Hu"
            ],
            "arxiv_id": "2510.12095v1",
            "summary": "In this study, we present IL3D, a large-scale dataset meticulously designed\nfor large language model (LLM)-driven 3D scene generation, addressing the\npressing demand for diverse, high-quality training data in indoor layout\ndesign. Comprising 27,816 indoor layouts across 18 prevalent room types and a\nlibrary of 29,215 high-fidelity 3D object assets, IL3D is enriched with\ninstance-level natural language annotations to support robust multimodal\nlearning for vision-language tasks. We establish rigorous benchmarks to\nevaluate LLM-driven scene generation. Experimental results show that supervised\nfine-tuning (SFT) of LLMs on IL3D significantly improves generalization and\nsurpasses the performance of SFT on other datasets. IL3D offers flexible\nmultimodal data export capabilities, including point clouds, 3D bounding boxes,\nmultiview images, depth maps, normal maps, and semantic masks, enabling\nseamless adaptation to various visual tasks. As a versatile and robust\nresource, IL3D significantly advances research in 3D scene generation and\nembodied intelligence, by providing high-fidelity scene data to support\nenvironment perception tasks of embodied agents.",
            "headline_zh": "提出IL3D数据集以支持LLM驱动的3D室内场景生成",
            "intro_zh": [
                "核心问题：缺乏多样高质量训练数据用于室内布局设计和3D场景生成。",
                "方法要点：构建大规模数据集，包含室内布局、3D对象和自然语言注释。",
                "实验或效果：监督微调LLM在IL3D上提升泛化能力，优于其他数据集。"
            ],
            "tags_zh": [
                "3D场景生成",
                "室内布局数据集",
                "大语言模型",
                "多模态学习",
                "监督微调"
            ],
            "_index": 109
        },
        {
            "title": "Translating Milli/Microrobots with A Value-Centered Readiness Framework",
            "authors": [
                "Hakan Ceylan",
                "Edoardo Sinibaldi",
                "Sanjay Misra",
                "Pankaj J. Pasricha",
                "Dietmar W. Hutmacher"
            ],
            "arxiv_id": "2510.12090v1",
            "summary": "Untethered mobile milli/microrobots hold transformative potential for\ninterventional medicine by enabling more precise and entirely non-invasive\ndiagnosis and therapy. Realizing this promise requires bridging the gap between\ngroundbreaking laboratory demonstrations and successful clinical integration.\nDespite remarkable technical progress over the past two decades, most\nmillirobots and microrobots remain confined to laboratory proof-of-concept\ndemonstrations, with limited real-world feasibility. In this Review, we\nidentify key factors that slow translation from bench to bedside, focusing on\nthe disconnect between technical innovation and real-world application. We\nargue that the long-term impact and sustainability of the field depend on\naligning development with unmet medical needs, ensuring applied feasibility,\nand integrating seamlessly into existing clinical workflows, which are\nessential pillars for delivering meaningful patient outcomes. To support this\nshift, we introduce a strategic milli/microrobot Technology Readiness Level\nframework (mTRL), which maps system development from initial conceptualization\nto clinical adoption through clearly defined milestones and their associated\nstepwise activities. The mTRL model provides a structured gauge of\ntechnological maturity, a common language for cross-disciplinary collaboration\nand actionable guidance to accelerate translational development toward new,\nsafer and more efficient interventions.",
            "headline_zh": "提出毫/微米机器人技术就绪框架以加速临床转化",
            "intro_zh": [
                "核心问题：毫/微米机器人实验室成果难以转化为临床应用，存在技术与现实脱节。",
                "方法要点：引入mTRL框架，定义从概念到临床的里程碑和逐步活动。",
                "实验或效果：未知具体实验，但框架旨在促进跨学科合作和加速技术成熟。"
            ],
            "tags_zh": [
                "毫/微米机器人",
                "技术就绪水平",
                "临床转化",
                "医疗机器人",
                "跨学科合作"
            ],
            "_index": 110
        },
        {
            "title": "Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback",
            "authors": [
                "Xingpei Ma",
                "Shenneng Huang",
                "Jiaran Cai",
                "Yuansheng Guan",
                "Shen Zheng",
                "Hanfeng Zhao",
                "Qiang Zhang",
                "Shunsi Zhang"
            ],
            "arxiv_id": "2510.12089v1",
            "summary": "Recent advances in diffusion models have significantly improved audio-driven\nhuman video generation, surpassing traditional methods in both quality and\ncontrollability. However, existing approaches still face challenges in lip-sync\naccuracy, temporal coherence for long video generation, and multi-character\nanimation. In this work, we propose a diffusion transformer (DiT)-based\nframework for generating lifelike talking videos of arbitrary length, and\nintroduce a training-free method for multi-character audio-driven animation.\nFirst, we employ a LoRA-based training strategy combined with a position shift\ninference approach, which enables efficient long video generation while\npreserving the capabilities of the foundation model. Moreover, we combine\npartial parameter updates with reward feedback to enhance both lip\nsynchronization and natural body motion. Finally, we propose a training-free\napproach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character\nanimation, which requires no specialized datasets or model modifications and\nsupports audio-driven animation for three or more characters. Experimental\nresults demonstrate that our method outperforms existing state-of-the-art\napproaches, achieving high-quality, temporally coherent, and multi-character\naudio-driven video generation in a simple, efficient, and cost-effective\nmanner.",
            "headline_zh": "提出基于扩散变换器的训练免费多角色音频驱动动画方法，以提升唇同步和长视频生成质量。",
            "intro_zh": [
                "核心问题：现有方法在唇同步精度、长视频时序一致性和多角色动画方面存在挑战。",
                "方法要点：采用LoRA训练与位置偏移推理，结合奖励反馈增强唇同步和身体运动自然性。",
                "实验效果：在实验中超越现有方法，实现高质量、时序一致的多角色音频驱动视频生成。"
            ],
            "tags_zh": [
                "扩散变换器",
                "音频驱动动画",
                "多角色生成",
                "训练免费方法",
                "长视频生成",
                "唇同步增强"
            ],
            "_index": 111
        },
        {
            "title": "A Review on Domain Adaption and Generative Adversarial Networks(GANs)",
            "authors": [
                "Aashish Dhawan",
                "Divyanshu Mudgal"
            ],
            "arxiv_id": "2510.12075v1",
            "summary": "The major challenge in today's computer vision scenario is the availability\nof good quality labeled data. In a field of study like image classification,\nwhere data is of utmost importance, we need to find more reliable methods which\ncan overcome the scarcity of data to produce results comparable to previous\nbenchmark results. In most cases, obtaining labeled data is very difficult\nbecause of the high cost of human labor and in some cases impossible. The\npurpose of this paper is to discuss Domain Adaptation and various methods to\nimplement it. The main idea is to use a model trained on a particular dataset\nto predict on data from a different domain of the same kind, for example - a\nmodel trained on paintings of airplanes predicting on real images of airplanes",
            "headline_zh": "综述领域自适应与GAN方法以解决计算机视觉中标注数据稀缺问题",
            "intro_zh": [
                "核心问题：计算机视觉中高质量标注数据稀缺，导致模型泛化能力受限",
                "方法要点：利用源域训练模型迁移至目标域，如从绘画预测真实图像",
                "实验或效果：未知具体实验，但旨在实现与基准结果可比性能"
            ],
            "tags_zh": [
                "领域自适应",
                "生成对抗网络",
                "图像分类",
                "数据稀缺",
                "模型迁移"
            ],
            "_index": 112
        },
        {
            "title": "EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making",
            "authors": [
                "Zixing Lei",
                "Sheng Yin",
                "Yichen Xiong",
                "Yuanzhuo Ding",
                "Wenhao Huang",
                "Yuxi Wei",
                "Qingyao Xu",
                "Yiming Li",
                "Weixin Li",
                "Yunhong Wang",
                "Siheng Chen"
            ],
            "arxiv_id": "2510.12072v1",
            "summary": "Embodied decision-making enables agents to translate high-level goals into\nexecutable actions through continuous interactions within the physical world,\nforming a cornerstone of general-purpose embodied intelligence. Large language\nmodels (LLMs), with their general decision-making capabilities, offer a\npromising path to realize this potential; however, LLMs trained solely on\nlanguage lack exposure to physical environments, limiting their true embodied\nunderstanding. To bridge this gap, we propose the concept of a training ground:\na comprehensive infrastructure that provides task and scene simulation,\nembodied interaction, and feedback signals, offering a one-stop solution for\nLLM acquire genuine embodied decision-making skills. In this work, we present\nEmboMatrix, the first training ground of its kind, providing massive and\ndiverse tasks with efficient simulation and precise rewards. EmboMatrix\nincorporates a series of novel techniques: a multi-agent data engine for\nlarge-scale task and scene generation, a distributed heterogeneous-hardware\nsystem for scalable simulation, and a multi-level reward architecture for\nprecise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM\nwhose embodied decision-making abilities emerge from extensive embodied\ninteractions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1\nbaseline by 9.5\\% on two challenging embodied decision-making benchmarks,\ndemonstrating the power of interactive, environment-grounded learning for\nbuilding truly intelligent embodied agents.",
            "headline_zh": "提出EmboMatrix训练场以提升大语言模型的具身决策能力",
            "intro_zh": [
                "核心问题：大语言模型缺乏物理环境暴露，限制其具身决策理解。",
                "方法要点：构建EmboMatrix训练场，集成任务模拟、交互和反馈信号。",
                "实验效果：EmboBrain-7B在基准测试中超越DeepSeek-R1基线9.5%。"
            ],
            "tags_zh": [
                "具身决策",
                "大语言模型训练",
                "模拟环境",
                "多智能体数据引擎",
                "分布式仿真系统",
                "多级奖励架构"
            ],
            "_index": 113
        },
        {
            "title": "VIDMP3: Video Editing by Representing Motion with Pose and Position Priors",
            "authors": [
                "Sandeep Mishra",
                "Oindrila Saha",
                "Alan C. Bovik"
            ],
            "arxiv_id": "2510.12069v1",
            "summary": "Motion-preserved video editing is crucial for creators, particularly in\nscenarios that demand flexibility in both the structure and semantics of\nswapped objects. Despite its potential, this area remains underexplored.\nExisting diffusion-based editing methods excel in structure-preserving tasks,\nusing dense guidance signals to ensure content integrity. While some recent\nmethods attempt to address structure-variable editing, they often suffer from\nissues such as temporal inconsistency, subject identity drift, and the need for\nhuman intervention. To address these challenges, we introduce VidMP3, a novel\napproach that leverages pose and position priors to learn a generalized motion\nrepresentation from source videos. Our method enables the generation of new\nvideos that maintain the original motion while allowing for structural and\nsemantic flexibility. Both qualitative and quantitative evaluations demonstrate\nthe superiority of our approach over existing methods. The code will be made\npublicly available at https://github.com/sandeep-sm/VidMP3.",
            "headline_zh": "提出VidMP3方法，利用姿态和位置先验解决运动保持视频编辑中的结构灵活性问题。",
            "intro_zh": [
                "核心问题：现有视频编辑方法在结构可变编辑中存在时间不一致、主体身份漂移等问题。",
                "方法要点：基于姿态和位置先验学习广义运动表示，实现运动保持与结构语义灵活编辑。",
                "实验或效果：定性和定量评估显示优于现有方法，代码将开源。"
            ],
            "tags_zh": [
                "视频编辑",
                "运动表示",
                "姿态先验",
                "位置先验",
                "扩散模型",
                "时间一致性"
            ],
            "_index": 114
        },
        {
            "title": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier",
            "authors": [
                "Yi-Chung Chen",
                "David I. Inouye",
                "Jing Gao"
            ],
            "arxiv_id": "2510.12060v1",
            "summary": "Generative classifiers, which leverage conditional generative models for\nclassification, have recently demonstrated desirable properties such as\nrobustness to distribution shifts. However, recent progress in this area has\nbeen largely driven by diffusion-based models, whose substantial computational\ncost severely limits scalability. This exclusive focus on diffusion-based\nmethods has also constrained our understanding of generative classifiers. In\nthis work, we propose a novel generative classifier built on recent advances in\nvisual autoregressive (VAR) modeling, which offers a new perspective for\nstudying generative classifiers. To further enhance its performance, we\nintroduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a\nsuperior trade-off between accuracy and inference speed, thereby significantly\nimproving practical applicability. Moreover, we show that the VAR-based method\nexhibits fundamentally different properties from diffusion-based methods. In\nparticular, due to its tractable likelihood, the VAR-based classifier enables\nvisual explainability via token-wise mutual information and demonstrates\ninherent resistance to catastrophic forgetting in class-incremental learning\ntasks.",
            "headline_zh": "提出基于视觉自回归模型的高效可解释生成分类器，提升分类准确性与推理速度。",
            "intro_zh": [
                "扩散模型生成分类器计算成本高，限制可扩展性。",
                "利用视觉自回归模型构建新生成分类器，实现高效推理。",
                "实验显示高准确率、可解释性和抗灾难性遗忘能力。"
            ],
            "tags_zh": [
                "生成分类器",
                "视觉自回归模型",
                "可解释性",
                "类增量学习",
                "推理效率"
            ],
            "_index": 115
        },
        {
            "title": "APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection",
            "authors": [
                "Xinxin Huang",
                "Han Sun",
                "Junmin Cai",
                "Ningzhong Liu",
                "Huiyu Zhou"
            ],
            "arxiv_id": "2510.12056v1",
            "summary": "Detecting camouflaged objects in underwater environments is crucial for\nmarine ecological research and resource exploration. However, existing methods\nface two key challenges: underwater image degradation, including low contrast\nand color distortion, and the natural camouflage of marine organisms.\nTraditional image enhancement techniques struggle to restore critical features\nin degraded images, while camouflaged object detection (COD) methods developed\nfor terrestrial scenes often fail to adapt to underwater environments due to\nthe lack of consideration for underwater optical characteristics.\n  To address these issues, we propose APGNet, an Adaptive Prior-Guided Network,\nwhich integrates a Siamese architecture with a novel prior-guided mechanism to\nenhance robustness and detection accuracy. First, we employ the Multi-Scale\nRetinex with Color Restoration (MSRCR) algorithm for data augmentation,\ngenerating illumination-invariant images to mitigate degradation effects.\nSecond, we design an Extended Receptive Field (ERF) module combined with a\nMulti-Scale Progressive Decoder (MPD) to capture multi-scale contextual\ninformation and refine feature representations. Furthermore, we propose an\nadaptive prior-guided mechanism that hierarchically fuses position and boundary\npriors by embedding spatial attention in high-level features for coarse\nlocalization and using deformable convolution to refine contours in low-level\nfeatures.\n  Extensive experimental results on two public MAS datasets demonstrate that\nour proposed method APGNet outperforms 15 state-of-art methods under widely\nused evaluation metrics.",
            "headline_zh": "提出APGNet自适应先验引导网络以解决水下伪装物体检测问题",
            "intro_zh": [
                "核心问题：水下图像退化与海洋生物伪装导致检测困难",
                "方法要点：集成Siamese架构与先验引导机制，增强特征鲁棒性",
                "实验或效果：在MAS数据集上优于15种先进方法，验证有效性"
            ],
            "tags_zh": [
                "水下伪装物体检测",
                "自适应先验引导",
                "Siamese架构",
                "多尺度特征提取",
                "图像增强"
            ],
            "_index": 116
        }
    ]
}