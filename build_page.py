#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse, json, os, re, sys, datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple

# =============== åŸºç¡€å·¥å…· ===============

def slugify(text: str, maxlen: int = 80) -> str:
    text = re.sub(r"[^\w\s-]", "", text, flags=re.U).strip().lower()
    text = re.sub(r"[\s_-]+", "-", text)
    return text[:maxlen] if maxlen else text

def read_json(p: Path) -> Dict[str, Any]:
    with p.open("r", encoding="utf-8") as f:
        return json.load(f)

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def md_escape(s: str) -> str:
    return s.replace("|", r"\|")

def to_authors(auth_list) -> str:
    if isinstance(auth_list, list):
        return ", ".join(auth_list)
    return str(auth_list)

# =============== æ¸²æŸ“ ===============

def render_paper_md(p: Dict[str, Any]) -> str:
    title = p.get("title", "").strip()
    authors = to_authors(p.get("authors", []))
    arxiv_id = p.get("arxiv_id", "")
    headline = p.get("headline_zh", "").strip()
    intro = p.get("intro_zh", [])
    tags = p.get("tags_zh", [])
    summary = p.get("summary", "").strip()

    lines = []
    lines.append(f"# {md_escape(title)}")
    if arxiv_id:
        base_id = arxiv_id.split('v')[0]
        abs_url = f"https://arxiv.org/abs/{base_id}"
        pdf_url = f"https://arxiv.org/pdf/{base_id}.pdf"
        lines.append(f"**arXiv**ï¼š[{arxiv_id}]({abs_url}) Â· [PDF]({pdf_url})  ")
    lines.append(f"**ä½œè€…**ï¼š{md_escape(authors)}  ")
    if headline:
        lines.append(f"\n**ä¸€å¥è¯è¦ç‚¹**ï¼š{md_escape(headline)}")
    if tags:
        lines.append(f"\n**å…³é”®è¯**ï¼š{', '.join(tags)}")
    lines.append("")
    if intro and isinstance(intro, list):
        lines.append("## 3 ç‚¹ç®€è¿°")
        for it in intro:
            lines.append(f"- {md_escape(str(it))}")
        lines.append("")
    lines.append("## æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰")
    lines.append(f"\n> " + "\n> ".join(md_escape(summary).splitlines()))
    lines.append("")
    return "\n".join(lines)

def build_date_index_md(date_label: str, papers: List[Dict[str, Any]], site_title: str) -> str:
    lines = []
    lines.append(f"---\nlayout: default\ntitle: {site_title} - {date_label}\n---\n")
    lines.append(f"# {site_title}ï¼ˆ{date_label}ï¼‰\n")
    lines.append("| # | é¢˜ç›® | ä¸€å¥è¯è¦ç‚¹ | å…³é”®è¯ |")
    lines.append("|---:|---|---|---|")
    for i, p in enumerate(papers, 1):
        title = p.get("title", "").strip()
        slug = slugify(f"{p.get('arxiv_id','')}-{title}") or f"paper-{i}"
        headline = md_escape(p.get("headline_zh", ""))
        tags = p.get("tags_zh", [])
        tag_str = ", ".join(tags) if tags else ""
        lines.append(f"| {i} | [{md_escape(title)}](./papers/{slug}.html) | {headline} | {md_escape(tag_str)} |")
    lines.append("")
    lines.append("[è¿”å›é¦–é¡µ](../index.html)")
    return "\n".join(lines)

def build_home_md(dates: List[str], latest_date: str, site_title: str) -> str:
    """
    é¦–é¡µï¼šåŒ…å«ä¸‹æ‹‰é€‰æ‹©å™¨ + è‡ªåŠ¨è·³è½¬åˆ°æ‰€é€‰æ—¥æœŸï¼›é»˜è®¤å‘ˆç°æœ€æ–°æ—¥æœŸçš„ç›®å½•è¡¨çš„å…¥å£æŒ‰é’®ã€‚
    """
    # ç”Ÿæˆä¸‹æ‹‰ options
    options_html = "\n".join(
        [f'<option value="dates/{d}/index.html" {"selected" if d == latest_date else ""}>{d}</option>'
         for d in dates]
    )
    # å†…åµŒä¸€ç‚¹ JS åšè·³è½¬
    html_block = f"""
<div class="date-switcher">
  <label for="date-select"><strong>é€‰æ‹©æ—¥æœŸï¼š</strong></label>
  <select id="date-select" onchange="location.href=this.value;">
    {options_html}
  </select>
  <a class="btn" href="dates/{latest_date}/index.html">å‰å¾€æœ€æ–°ï¼ˆ{latest_date}ï¼‰</a>
</div>
"""
    lines = []
    lines.append(f"---\nlayout: default\ntitle: {site_title}\n---\n")
    lines.append(f"# {site_title}\n")
    lines.append("> æ”¯æŒå¤šæ—¥æœŸåˆ‡æ¢ï¼šä»ä¸‹æ‹‰èœå•é€‰æ‹©æŸä¸€å¤©è¿›å…¥è¯¥æ—¥ç›®å½•é¡µï¼ˆå«æ‰€æœ‰è®ºæ–‡æ¡ç›®ä¸è¯¦æƒ…é¡µï¼‰ã€‚\n")
    lines.append(html_block)
    lines.append("\n---\n")
    lines.append("å¦‚æœä½ æƒ³æŠŠé¦–é¡µç›´æ¥å±•ç¤ºæœ€æ–°æ—¥æœŸçš„å®Œæ•´ç›®å½•ï¼Œä¹Ÿå¯ä»¥æŠŠè¯¥æ—¥çš„ `index.md` å†…å®¹å¤åˆ¶åˆ°è¿™é‡Œã€‚")
    return "\n".join(lines)

def write_site_scaffold(docs_dir: Path):
    ensure_dir(docs_dir)
    ensure_dir(docs_dir / "assets")
    (docs_dir / "_config.yml").write_text(
        "theme: jekyll-theme-cayman\nmarkdown: kramdown\nkramdown:\n  input: GFM\n",
        encoding="utf-8"
    )
    (docs_dir / "_layouts").mkdir(exist_ok=True)
    (docs_dir / "_layouts" / "default.html").write_text(
        """<!doctype html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<title>{{ page.title }}</title>
<link rel="stylesheet" href="{{ '/assets/style.css' | relative_url }}">
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>
<main class="container">
  {{ content }}
</main>
<footer class="footer">
  <p>Powered by GitHub Pages Â· Generated on {{ site.time | date: "%Y-%m-%d" }}</p>
</footer>
</body>
</html>
""",
        encoding="utf-8"
    )
    (docs_dir / "assets" / "style.css").write_text(
        """.container{max-width:960px;margin:2rem auto;padding:0 1rem;font:16px/1.6 -apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial;}
h1,h2,h3{line-height:1.25}
table{border-collapse:collapse;width:100%;margin:1rem 0}
th,td{border:1px solid #ddd;padding:.5rem;vertical-align:top}
code,pre{background:#f6f8fa;padding:.2rem .4rem;border-radius:6px}
.footer{margin:2rem 0;color:#666;font-size:.9rem;text-align:center}
a{color:#0366d6}
.date-switcher{display:flex;gap:.5rem;align-items:center;margin:1rem 0}
.date-switcher .btn{padding:.4rem .8rem;border:1px solid #ccc;border-radius:6px;text-decoration:none}
""",
        encoding="utf-8"
    )

# =============== ç«™ç‚¹ç”Ÿæˆ ===============

def collect_dates(data_root: Path) -> List[Tuple[str, Path]]:
    """
    æ‰«æ data/*/ai_summary*.jsonï¼Œè¿”å› [(date_label, json_path), ...]ï¼ŒæŒ‰æ—¥æœŸå‡åºã€‚
    """
    items = []
    for p in sorted(data_root.glob("*/ai_summary*.json")):
        # ä»è·¯å¾„æˆ–æ–‡ä»¶åä¸­æŠ½æ—¥æœŸ
        m = re.search(r"(\d{4}-\d{2}-\d{2})", str(p))
        if not m:
            # å…¼å®¹æ²¡æœ‰æ—¥æœŸçš„è·¯å¾„ï¼Œç”¨çˆ¶ç›®å½•åå…œåº•
            m = re.search(r"(\d{4}-\d{2}-\d{2})", p.parent.name)
        if m:
            date_label = m.group(1)
        else:
            # å®åœ¨å–ä¸åˆ°ï¼Œç”¨æ–‡ä»¶ä¿®æ”¹æ—¥æœŸ
            ts = datetime.date.fromtimestamp(p.stat().st_mtime).isoformat()
            date_label = ts
        items.append((date_label, p))
    # å»é‡ & æ’åº
    items = sorted(list({(d, str(p)): (d, Path(p)) for d, p in items}.values()), key=lambda x: x[0])
    return items

def save_date_site(docs_dir: Path, date_label: str, papers: List[Dict[str, Any]], site_title: str):
    """ç”ŸæˆæŸä¸€å¤©çš„ç›®å½•é¡µ + è¯¦æƒ…é¡µ"""
    day_dir = docs_dir / "dates" / date_label
    ensure_dir(day_dir)
    ensure_dir(day_dir / "papers")

    # date index
    date_md = build_date_index_md(date_label, papers, site_title)
    (day_dir / "index.md").write_text(date_md, encoding="utf-8")

    # per-paper pages
    for i, p in enumerate(papers, 1):
        title = p.get("title", "").strip()
        slug = slugify(f"{p.get('arxiv_id','')}-{title}") or f"paper-{i}"
        body_md = render_paper_md(p)
        md = f"---\nlayout: default\ntitle: {title}\n---\n\n{body_md}\n"
        (day_dir / "papers" / f"{slug}.md").write_text(md, encoding="utf-8")

def main():
    ap = argparse.ArgumentParser(description="Build multi-date GitHub Pages with date switcher.")
    ap.add_argument("--data", default="data", help="æ•°æ®æ ¹ç›®å½•ï¼ˆé»˜è®¤ data/ï¼‰")
    ap.add_argument("--outdir", default="docs", help="è¾“å‡ºç«™ç‚¹ç›®å½•ï¼ˆé»˜è®¤ docs/ï¼‰")
    ap.add_argument("--title", default="arXivÂ·cs.CV ä¸­æ–‡è¦ç‚¹æ±‡æ€»ï¼ˆwith DeepSeekï¼‰", help="ç«™ç‚¹æ ‡é¢˜")
    args = ap.parse_args()

    data_root = Path(args.data)
    docs_dir = Path(args.outdir)
    site_title = args.title

    pairs = collect_dates(data_root)
    if not pairs:
        print("[ERR] æœªæ‰¾åˆ° data/*/ai_summary*.json", file=sys.stderr)
        sys.exit(2)

    write_site_scaffold(docs_dir)

    # ç”Ÿæˆæ¯ä¸ªæ—¥æœŸçš„é¡µé¢
    all_dates = []
    for date_label, json_path in pairs:
        data = read_json(json_path)
        papers = data.get("papers", [])
        if not isinstance(papers, list) or not papers:
            print(f"[WARN] {json_path} ä¸­ papers ä¸ºç©ºï¼Œè·³è¿‡ {date_label}")
            continue
        all_dates.append(date_label)
        save_date_site(docs_dir, date_label, papers, site_title)

    if not all_dates:
        print("[ERR] æ²¡æœ‰å¯ç”¨çš„æ—¥æœŸé¡µé¢ç”Ÿæˆ", file=sys.stderr)
        sys.exit(3)

    latest_date = sorted(all_dates)[-1]
    # é¦–é¡µï¼šæ—¥æœŸä¸‹æ‹‰ + æŒ‰é’®è·³åˆ°æœ€æ–°
    home_md = build_home_md(sorted(all_dates), latest_date, site_title)
    (docs_dir / "index.md").write_text(home_md, encoding="utf-8")

    print(f"[OK] ç”Ÿæˆå®Œæˆã€‚å…± {len(all_dates)} ä¸ªæ—¥æœŸã€‚é¦–é¡µï¼š{docs_dir}/index.md")
    print("ğŸ‘‰ æ‰“å¼€ GitHub â†’ Settings â†’ Pagesï¼ŒSource é€‰ Branchï¼Œç›®å½•é€‰ docs/ï¼Œä¿å­˜å³å¯ã€‚")

if __name__ == "__main__":
    main()
