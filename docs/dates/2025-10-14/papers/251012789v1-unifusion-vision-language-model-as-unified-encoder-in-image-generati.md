---
layout: default
title: UniFusion: Vision-Language Model as Unified Encoder in Image Generation
---

# UniFusion: Vision-Language Model as Unified Encoder in Image Generation
**arXiv**：[2510.12789v1](https://arxiv.org/abs/2510.12789) · [PDF](https://arxiv.org/pdf/2510.12789.pdf)  
**作者**：Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale  

**一句话要点**：提出UniFusion，利用冻结VLM作为统一编码器，提升扩散模型的跨模态生成与编辑能力。

**关键词**：视觉语言模型, 扩散模型, 跨模态生成, 图像编辑, 统一编码器, 零样本泛化

## 3 点简述
- 现有扩散模型依赖独立图像和文本编码器，限制跨模态推理和知识迁移。
- UniFusion引入Layerwise Attention Pooling机制，从冻结VLM提取多层级语义和细节，条件化扩散生成。
- 实验显示，LAP在文本-图像对齐和视觉信息迁移上优于浅层融合，并零样本泛化到多图像编辑任务。

## 摘要（原文）

> Although recent advances in visual generation have been remarkable, most
> existing architectures still depend on distinct encoders for images and text.
> This separation constrains diffusion models' ability to perform cross-modal
> reasoning and knowledge transfer. Prior attempts to bridge this gap often use
> the last layer information from VLM, employ multiple visual encoders, or train
> large unified models jointly for text and image generation, which demands
> substantial computational resources and large-scale data, limiting its
> accessibility.We present UniFusion, a diffusion-based generative model
> conditioned on a frozen large vision-language model (VLM) that serves as a
> unified multimodal encoder. At the core of UniFusion is the Layerwise Attention
> Pooling (LAP) mechanism that extracts both high level semantics and low level
> details from text and visual tokens of a frozen VLM to condition a diffusion
> generative model. We demonstrate that LAP outperforms other shallow fusion
> architectures on text-image alignment for generation and faithful transfer of
> visual information from VLM to the diffusion model which is key for editing. We
> propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),
> which conditions a diffusion transformer (DiT) only on the text tokens
> generated by the VLM during in-model prompt rewriting. VERIFI combines the
> alignment of the conditioning distribution with the VLM's reasoning
> capabilities for increased capabilities and flexibility at inference. In
> addition, finetuning on editing task not only improves text-image alignment for
> generation, indicative of cross-modality knowledge transfer, but also exhibits
> tremendous generalization capabilities. Our model when trained on single image
> editing, zero-shot generalizes to multiple image references further motivating
> the unified encoder design of UniFusion.

