---
layout: default
title: Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark
---

# Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark
**arXiv**：[2510.13759v1](https://arxiv.org/abs/2510.13759) · [PDF](https://arxiv.org/pdf/2510.13759.pdf)  
**作者**：Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, Ziwei Liu  

**一句话要点**：提出Uni-MMMU基准以评估多领域多模态统一模型的生成与理解双向协同

**关键词**：多模态统一模型, 基准评估, 视觉生成, 视觉理解, 跨模态推理, 多领域任务

## 3 点简述
- 现有基准孤立评估视觉生成与理解，忽视其内在耦合任务
- 构建涵盖科学、编码等八领域的双向耦合任务，强调推理步骤与可验证输出
- 评估显示统一模型性能差距，揭示跨模态依赖与能力强化机制

## 摘要（原文）

> Unified multimodal models aim to jointly enable visual understanding and
> generation, yet current benchmarks rarely examine their true integration.
> Existing evaluations either treat the two abilities in isolation or overlook
> tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
> comprehensive and discipline-aware benchmark that systematically unfolds the
> bidirectional synergy between generation and understanding across eight
> reasoning-centric domains, including science, coding, mathematics, and puzzles.
> Each task is bidirectionally coupled, demanding models to (i) leverage
> conceptual understanding to guide precise visual synthesis, or (ii) utilize
> generation as a cognitive scaffold for analytical reasoning. Uni-MMMU
> incorporates verifiable intermediate reasoning steps, unique ground truths, and
> a reproducible scoring protocol for both textual and visual outputs. Through
> extensive evaluation of state-of-the-art unified, generation-only, and
> understanding-only models, we reveal substantial performance disparities and
> cross-modal dependencies, offering new insights into when and how these
> abilities reinforce one another, and establishing a reliable foundation for
> advancing unified models.

