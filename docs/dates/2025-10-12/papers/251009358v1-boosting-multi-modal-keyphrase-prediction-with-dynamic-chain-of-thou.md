---
layout: default
title: Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models
---

# Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models
**arXiv**：[2510.09358v1](https://arxiv.org/abs/2510.09358) · [PDF](https://arxiv.org/pdf/2510.09358.pdf)  
**作者**：Qihang Ma, Shengyu Li, Jie Tang, Dingkang Yang, Shaodong Chen, Yingyi Zhang, Chao Feng, Jiao Ran  
**一句话要点**：提出动态思维链策略以提升视觉语言模型在多模态关键词预测中的推理能力

**关键词**：多模态关键词预测, 视觉语言模型, 思维链推理, 动态训练策略, 模型微调

## 3 点简述
- 核心问题：传统多模态方法在缺失和未见场景中表现不佳，且基准测试高估模型能力
- 方法要点：采用零样本和监督微调评估基线，并引入动态思维链策略优化推理过程
- 实验或效果：在多个数据集上验证了方法的有效性，代码已开源

## 摘要（原文）

> Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only
> methods by incorporating multiple modalities of input information to produce a
> set of conclusive phrases. Traditional multi-modal approaches have been proven
> to have significant limitations in handling the challenging absence and unseen
> scenarios. Additionally, we identify shortcomings in existing benchmarks that
> overestimate model capability due to significant overlap in training tests. In
> this work, we propose leveraging vision-language models (VLMs) for the MMKP
> task. Firstly, we use two widely-used strategies, e.g., zero-shot and
> supervised fine-tuning (SFT) to assess the lower bound performance of VLMs.
> Next, to improve the complex reasoning capabilities of VLMs, we adopt
> Fine-tune-CoT, which leverages high-quality CoT reasoning data generated by a
> teacher model to finetune smaller models. Finally, to address the
> "overthinking" phenomenon, we propose a dynamic CoT strategy which adaptively
> injects CoT data during training, allowing the model to flexibly leverage its
> reasoning capabilities during the inference stage. We evaluate the proposed
> strategies on various datasets and the experimental results demonstrate the
> effectiveness of the proposed approaches. The code is available at
> https://github.com/bytedance/DynamicCoT.

